%\documentclass[11pt]{article} 
%\documentclass[9pt,twocolumn,twoside]{pnas-new}
\documentclass[numsec,webpdf,modern,medium,namedate]{oup-authoring-template}
%\documentclass[9pt,twocolumn,twoside]{pnas-new}
% Use the lineno option to display guide line numbers if required.
\onecolumn

\usepackage{booktabs}
\newcommand{\description}{}
\usepackage{enumitem}



%\newcommand\arxiv[2]{#1} % 1st argument for arxiv format
\newcommand\arxiv[2]{#2} % 2nd argument for PNAS format
% also, swich to \documentclass[11pt]{article} for arxiv format

\input{macros}

\usepackage{tabto}


\arxiv{
  \usepackage{fullpage}
  %% some packages loaded by pnas-new.cls which we need for
  %% the arxiv version
  \usepackage{algorithm}
  \usepackage[noend]{algpseudocode}
  \usepackage{xcolor}
  \usepackage{tikz}
  \usepackage{enumitem}
  \bibliographystyle{ieeetr}
}{
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\theoremstyle{thmstyletwo}%w
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}
\usepackage[doublespacing]{setspace}
\usepackage[fontsize=12pt]{fontsize}}



\AtEndDocument{\refstepcounter{thm}\label{finalthm}}
\AtEndDocument{\refstepcounter{lem}\label{finallem}}
\AtEndDocument{\refstepcounter{defn}\label{finaldefn}}
\AtEndDocument{\refstepcounter{prop}\label{finalprop}}
\AtEndDocument{\refstepcounter{aspt}\label{finalaspt}}
\AtEndDocument{\refstepcounter{figure}\label{finalfig}}


\begin{document}

\arxiv{\title{Automatic Differentiation Accelerates Inference for Partially Observed Markov Processes}

\author{K. Tan$^{1}$, G. Hooker$^{1}$ and E. L. Ionides$^{2}$
  \vspace{3ex}\\
  \small{$^{1}$Department of Statistics, University of Pennsylvania}\\
  \small{$^{2}$Department of Statistics, University of Michigan}}
  
}{
  
\journaltitle{Journals of the Royal Statistical Society}
\DOI{DOI HERE}
\copyrightyear{XXXX}
\pubyear{XXXX}
\access{Advance Access Publication Date: Day Month Year}
\appnotes{Original article}

\firstpage{1}

%\subtitle{Subject Section}
\title[AD for POMPs]{Automatic Differentiation Accelerates Inference for Partially Observed Markov Processes}

\author[1,$\ast$]{Kevin Tan}
\author[1]{Giles Hooker}
\author[2]{Edward L. Ionides}
%\author[4]{Fifth Author\ORCID{0000-0000-0000-0000}}
\authormark{Tan et al.}

\address[1]{\orgdiv{Department of Statistics and Data Science}, \orgname{University of Pennsylvania}, \orgaddress{\street{265 South 37th Street, 3rd \& 4th Floors}, \postcode{19104}, \state{Pennsylvania}, \country{United States of America}}}
\address[2]{\orgdiv{Department of Statistics}, \orgname{University of Michigan}, \orgaddress{\street{323 West Hall, 1085 S University Ave}, \postcode{48109}, \state{Michigan}, \country{United States of America}}}

\corresp[$\ast$]{Address for correspondence. Kevin Tan, University of Pennsylvania, Philadelphia, 19104, USA. \href{Email:kevtan@wharton.upenn.edu}{kevtan@wharton.upenn.edu}}

\received{Date}{0}{Year}
\revised{Date}{0}{Year}
\accepted{Date}{0}{Year}

}

\arxiv{
\begin{abstract}
  Automatic differentiation (AD) has driven recent advances in machine learning, including deep neural networks and Hamiltonian Markov Chain Monte Carlo methods. Partially observed nonlinear stochastic dynamical systems have proved resistant to AD techniques because widely used particle filter algorithms yield an estimated likelihood function that is discontinuous as a function of the model parameters. To resolve this, we create a theoretical framework that embeds two existing AD particle filter methods within a new class of algorithms. This new class permits a bias-variance tradeoff and hence a mean squared error substantially lower than the existing algorithms. This allows us to develop likelihood maximization algorithms suited to the Monte Carlo properties of the AD gradient estimate. Our algorithms require only a differentiable simulator for the latent dynamic system; by contrast, most previous approaches to AD likelihood maximization for particle filters require access to the system's transition probabilities. Numerical results indicate that a hybrid algorithm that uses AD to refine a coarse solution from an iterated filtering algorithm shows substantial improvement on current state-of-the-art methods on a challenging scientific benchmark problem.
\end{abstract}
}{

\abstract{Although automatic differentiation (AD) has driven many recent advances in machine learning, partially observed nonlinear stochastic dynamical systems have proved resistant to AD techniques because widely used particle filter algorithms yield an estimated likelihood that is discontinuous in the model parameters. To resolve this, we create a theoretical framework that embeds two existing AD particle filter methods within a new class of algorithms. This new class permits a bias-variance tradeoff and a mean squared error substantially lower than the existing algorithms. This allows us to develop likelihood maximization algorithms suited to the Monte Carlo properties of the AD gradient estimate. Our algorithms require only a differentiable simulator for the latent dynamic system while most previous approaches require access to the system's transition probabilities. Numerical results show that using AD to refine a coarse solution from an iterated filtering algorithm shows substantial improvement on current state-of-the-art methods on a challenging scientific benchmark problem.}
\keywords{Sequential Monte Carlo, Automatic Differentiation, Particle Filter, Markov Process, Likelihood}
}

\maketitle

\section{Introduction}

Many scientific models involve highly nonlinear stochastic dynamic systems possessing significant random variation in both the process dynamics and the measurements.
Commonly, the latent system is modeled as a Markov process, giving rise to a partially observed Markov process (POMP) model, also known as a hidden Markov models or a state space model.
POMP models arise in fields as diverse as optimal control \citep{singh22}, epidemiology \citep{he10, stocks17}, ecology \citep{knape12} and finance \citep{kim08, breto14}.
Despite their ubiquity, estimation and inference for this broad class of models remains a challenging problem.
To tackle this, we use automatic differentiation (AD) to construct improved algorithms for complex POMP models.

The particle filter, also known as sequential Monte Carlo, serves as the foundation for various inference algorithms for POMP models.
It provides an unbiased estimate of the likelihood function  \citep{delMoral04}, enabling Bayesian inference \citep{andrieu10,chopin13} and likelihood-based inference \citep{ionides06-pnas,ionides15}.
Likelihood-based inference is statistically efficient \citep{pawitan01} and robust to a moderate amount of Monte Carlo error \citep{ionides17,ning21}.
Furthermore, basic particle filters, also known as boostrap filters, do not need to evaluate the transition density of the latent Markov process, enabling an arbitrary model simulator to be plugged into the algorithm.

This \textit{plug-and-play} property is useful for scientific applications \citep{he10}.
Some plug-and-play methods depend on the construction of low-dimensional summary statistics \citep{wood10,toni09}, sacrificing statistical efficiency for computational convenience.
Plug-and-play methods are often called likelihood-free \citep{owen15}, and it may be counter-intuitive that likelihood-based inference is possible using likelihood-free methods. 
However, iterated filtering algorithms have shown that this is practical in various scientific investigations \citep{king08,blake14,pons-salort18,subramanian21,fox22,drake23}.
Nevertheless, the Monte Carlo variability arising in iterated filtering applications is increasingly problematic as the size of the data and the complexity of the model increase.
Algorithmic advances are needed to make plug-and-play likelihood maximization for POMPs more numerically efficient.


We examine plug-and-play AD as a tool to do so by providing a likelihood gradient. Recent advances in AD for particle filters \citep{naesseth18, jonschkowski18, corenflos21, scibior21, singh22} have drawn attention to AD as a tool for inference in POMP models.
However, existing approaches are either asymptotically biased \citep{naesseth18, jonschkowski18}, have high variance \citep{poyiadjis11, scibior21}, are computationally expensive \citep{corenflos21, chen24}, or require access to transition densities \citep{poyiadjis11, scibior21, singh22, chen24}. 

\cite{scibior21} showed that the estimators derived by \cite{poyiadjis11} can be attained with standard AD software using an algorithmic procedure, called a {\it stop-gradient}, which allows selected expressions to be evaluated but not differentiated.
However, the algorithm of \cite{scibior21} that can be modified to have the plug-and-play property recovers an estimator with high Monte Carlo variability, which led the authors to reject this algorithm in favor of an approach without the plug-and-play property and with inferior scalability. \textit{Our approach is the most significant methodological advance since \cite{ionides15}, in nearly a decade, for plug-and-play statistical inference on POMP models.}

We start by presenting a new algorithm which (for reasons which will be discussed later) we call a {\it Measurement Off-Parameter} (MOP) particle filter. 
In summary, \textit{instead of directly differentiating through a basic particle filter, MOP uses AD to differentiate through a series of measurement density ratios and a differentiable simulator}. 
Our MOP algorithm differs from previous smooth particle filters \citep{svensson18,malik11} by possessing the plug-and-play property.
Critically, we also avoid the high variance of \cite{svensson18} by generalizing the MOP particle filter to add a discount factor, $\alpha \in [0,1]$.
Remarkably, we show that the derivative of our algorithm coincides with the biased gradient estimator of \cite{naesseth18}, when $\alpha=0$, and the high-variance gradient estimate from \citep{poyiadjis11, scibior21}, when $\alpha=1$ (Theorem \ref{thm:mop-functional-forms}).
The bias-variance tradeoff induced by $\alpha$ suggests, both in theory (Theorem \ref{thm:mop-biasvar}) and in practice (Figure \ref{fig:biasvar}), a choice of $\alpha$ strictly in $(0,1)$.
Choosing $\alpha$ close to one, we obtain a low-bias, low-variance, plug-and-play gradient estimate with computational scaling linear in the number of particles.

MOP-$\alpha$ can provide strongly consistent estimates of the filtering distribution and the log-likelihood (Theorem \ref{thm:mop-targeting}).
When $\alpha=1$, MOP-$\alpha$ also provides consistent score estimates (Theorem \ref{thm:mop-grad-consistency}). This lets us derive a linear convergence rate for gradient descent with MOP-$\alpha$ in the strongly convex and smooth setting (Theorem \ref{thm:mop-convergence}).

In practice, gradient descent with MOP-$\alpha$ converges quickly within a neighborhood of the maximum but struggles to reach this neighborhood in the presence of local minima and saddle points.
That behavior is complementary to that of iterated filtering algorithms \citep{ionides06-pnas,ionides15} which provide a relatively fast and stable way to identify this neighborhood.
We therefore build a simple but effective hybrid algorithm called {\it Iterated Filtering with Automatic Differentiation} (IFAD) that warm-starts first-order or second-order gradient methods with a preliminary solution obtained from a few rounds of iterated filtering.
Promising numerical results indicate that IFAD beats IF2 (and by the numerical results of \cite{ionides15}, also IF1 \citep{ionides06-pnas,ionides11} and the Liu-West filter \citep{liuwest01}) on a challenging problem in epidemiology, the Dhaka cholera model of \cite{king08}.

These improvements also extend to Bayesian inference. In Section \ref{sec:bayes}, we use the MOP-$\alpha$ gradient estimates within a No-U-Turn Sampler (NUTS) \citep{homan14} in conjunction with an empirical Bayes-type prior estimated with IF2 to reduce the burn-in period of particle MCMC \citep{andrieu10} on the Dhaka cholera model of \cite{king08} from the $700,000$ iterations in \cite{fasiolo16} to just $500$. To the best of our knowledge, attaining rapid mixing on this challenging problem has not previously been achieved. 


\section{Problem Setup}


% By a similar decomposition to that in \cite{doucet2009tutorial}, we find that the joint density of $X_{0:N}, Y_{1:N}$ can be factored as
% \begin{align*}
%     &f_{X_{0: N}, Y_{1: N}}\left(x_{0: N}, y_{1: N} ; \theta\right)\\
%     &=f_{X_0}\left(x_0 ; \theta\right) \prod_{n=1}^N f_{X_n \mid X_{n-1}}\left(x_n \mid x_{n-1} ; \theta\right) f_{Y_n \mid X_n}\left(y_n \mid x_n ; \theta\right).
% \end{align*}



Consider an unobserved Markov process $\{X(t),t  \geq t_0\}$, with discrete-time observations $Y_1,...,Y_N$ realized at values $y_1^*,...,y_N^*$ at times $t_1,..., t_N$.
The process is parameterized by an unknown parameter $\theta \in \Theta \subseteq \R^p$, where the state $X(t)$ takes values in the state space $\gX \subseteq \R^d$, the observations $Y_n$ take values in $\gY,$ and we write $X_n := X(t_n)$. We suppose that the discrete-time latent process model has a density $f_{X_n|X_{n-1}}\left(x_{n} \mid x_{n-1}; \theta\right)$.
The existence of this density is necessary to define the likelihood function, but plug-and-play algorithms do not explicitly evaluate this density.
Instead, they have access to a corresponding simulator, which we, write as $\process_n\left(x_n\mid x_{n-1}; \theta\right)$.
We set $f_{Y_n|X_n}\left(y_n \mid x_n; \theta\right)$ to be the measurement density, which  can be directly evaluated. We write $y_n^*$ for the actual values of the observations that were observed.
We call $f_{X_{1:n}|Y_{1:n}}(x_{1:n}|y_{1:n}^*; \theta)$ the posterior distribution of states, and $f_{X_{n}|Y_{1:n}}(x_n|y_{1:n}^*; \theta)$ the filtering distribution at time $t_n$.
$x_{n,j}^A$ denotes the ancestral trajectory of particle $j$ at time $n$,  and $k_j$ the resampling indices for particle $j$. 
%$a(\cdot)$ is the ancestor function that maps a particle to its parent, $j \mapsto a(j)$,

The above densities are defined on a probability space $(\Omega, \Sigma, \prob)$ which is assumed to enable construction of independent replicates and all random variables defined in our algorithms.
For the algorithmic interpretation of our theory, we identify the random number seed with an element of the sample space $\omega \in \Omega$. The random seed determines the sequence of pseudo-random numbers generated by a computer, just as $\omega\in\Omega$ generates the sequence of random variables.

%When two algorithms share the same seed, this corresponds to the method of common random numbers. 


\section{Off-Parameter Particle Filters}

%Directly differentiating through a particle filter, for a fixed value of $\omega$, leads to a biased gradient estimator due to the discreteness of the resampling operation \cite{corenflos21}.
Our approach is as follows. \textit{Instead of directly differentiating through a basic particle filter, we instead use AD to differentiate through a series of measurement density ratios and a differentiable simulator}. 
This is done through the construction of a novel particle filtering algorithm called {\it Measurement Off-Parameter with discount factor} $\alpha$ (MOP-$\alpha$), defined by the pseudocode in Algorithm~\ref{alg:mop}.
Note that the plug-and-play property forbids access to transition densities but permits access to the density of the measurements conditional on the value of the latent process.
The simulator can be differentiated using AD as long as the underlying computer code for the simulator is differentiable, and so MOP-$\alpha$ is applicable to a broad class of POMP models.
To that end, we suppose that the simulator is a differentiable function of $\theta$ for every fixed $\omega$, a condition that requires the latent process to be a continuous random variable. 
This condition is also known as the {\it reparameterization trick} \citep{corenflos21}.
The discrete resampling step is more delicate to address. 
Our goal involves estimating the derivative of the expectation of the particle filter likelihood, but the discontinuity of the resampling (as a function of $\theta$, for fixed $\omega$) in the standard particle filter means that this is not consistently estimated by the expectation of the derivative. 
MOP-$\alpha$ is constructed to avoid this discontinuity, and its derivative (available via AD) provides consistent estimation of the score (for $\alpha=1)$.

MOP-$\alpha$ resamples the particles according to a resampling rule at a baseline parameter value while correcting their weights to target the filtering distribution at another parameter value, yielding the name {\it measurement off-parameter}.
This is intended as an analogy to {\it off-policy} learning in reinforcement learning.
Specifically, MOP-$\alpha$ evaluates the likelihood at some $\theta \in \Theta$, but instead resamples the particles according to the indices generated by a particle filter run at a {\it baseline parameter}, $\phi \in \Theta$.
This ensures that the resampling indices are invariant in $\theta$ when $\omega$ and $\phi$ are fixed, bypassing the issue of Monte Carlo resampling.

One can think of this as reconstructing local smooth approximations of the discontinuous likelihood estimates produced by a particle filter for a fixed number of particles $J$ and fixed seed $\omega$. These approximate the likelihood at $\theta$ in a neighborhood around $\phi$, with the intent that differentiating these smooth log-likelihood reconstructions yields a score estimate. This is illustrated in the left panel of Figure \ref{fig:biasvar} for various choices of $\alpha$. 


\begin{figure}[htbp!]
  \centering
    \includegraphics[width=\arxiv{10cm}{\textwidth/\real{2.2}}]{imgs/095/mop.png}
    \includegraphics[width=\arxiv{10cm}{\textwidth/\real{2.2}}]{imgs/095/biasvar.png}
    \arxiv{}{\vspace*{-3ex}}
    \caption{\textbf{A:} Non‐differentiable likelihood estimates and our smooth off‐parameter approximations that hold locally around the baseline parameter $\phi$. \textbf{B:} Illustration of the bias-variance tradeoff induced by the discounting hyperparameter $\alpha$, on the Dhaka cholera model of \cite{king08}. We display the MSE of score estimates for the trend in transmission at the MLE. Our novel contribution of $\alpha \in (0,1)$ significantly improves the quality of the likelihood approximation, and therefore the gradient estimates. }
    \label{fig:biasvar}
    \arxiv{}{\vspace*{-4ex}}
\end{figure}


\begin{algorithm}[htbp!]
	\caption{MOP-$\alpha$}
    \label{alg:mop}
	     \textbf{Input:} Number of particles $J$, timesteps $N$, measurement model $f_{Y_n|X_n}(y_n^*|x_n, \theta)$, simulator $\process_n(\cdot|x_n; \theta)$, evaluation parameter $\theta$, baseline parameter $\phi$, seed $\omega$.
      
        \textbf{First pass:} Set $\theta=\phi$ and fix $\omega$, yielding $X_{n,j}^{P,\phi}$, $X_{n,j}^{F,\phi}$, $g^{\phi}_{n,j}$.
            
        \textbf{Second pass:}
        Fix $\omega$, and filter at $\theta\neq \phi$:
            
		\textbf{Initialize } particles ${X}_{0,j}^{F,\theta}\sim {f}_{{X}_{0}}\left(\cdot\giventh{\theta}\right)$, weights $w^{F,\theta}_{0,j}= 1$. \newline
		\textbf{For} $n=1,...,N$: \newline
            \hspace*{4mm} Accumulate discounted weights, $w_{n,j}^{P,\theta} = \big(w_{n-1,j}^{F,\theta}\big)^\alpha$.\newline
            \hspace*{4mm} Simulate process model,
            ${X}_{n,j}^{P,\theta}\sim \process_n\big(\cdot|{X}_{n-1,j}^{F, \theta};{\theta}\big)$. \newline
            \hspace*{4mm} Measurement density,
            $g^{\theta}_{n,j}={f}_{{Y}_{n}|{X}_{n}}(y_{n}^{*}|{X}_{n,j}^{P,\theta}\giventh{\theta})$. \newline
            \hspace*{4mm} Compute $L_n^{B,\theta,\alpha} ={\sum_{j=1}^Jg^\theta_{n,j} \, w^{P,\theta}_{n,j}}\, \big/\, {\sum_{j=1}^J  w^{P,\theta}_{n,j}}$. \newline
            \hspace*{4mm} Conditional likelihood under $\phi$,
            $L_n^{\phi} = \frac{1}{J}\sum_{m=1}^{J}g^{\phi}_{n,m}$.\newline
            \hspace*{4mm} Select resampling indices $k_{1:J}$ with $\prob\big(k_{j}=m\big) \propto g^{\phi}_{n,m}$. \newline
            \hspace*{4mm} Obtain resampled particles ${X}_{n,j}^{F,\theta}={X}_{n,k_{j}}^{P,\theta}$. \newline
            \hspace*{4mm} Calculate corrected weights
            $w_{n,j}^{F,\theta}= w^{P,\theta}_{n,k_j} \, g^{\theta}_{n,k_j} \, \big/ \, { g^{\phi}_{n,k_j}}$.\newline
            \hspace*{4mm} Compute $ L_n^{A,\theta,\alpha} = L_n^\phi\cdot {\sum_{j=1}^J w^{F,\theta}_{n,j}} \, \big/ \, {\sum_{j=1}^J  w^{P,\theta}_{n,j}}$.\newline
		\textbf{Return:} likelihood estimate $\hat{\lik}(\theta) = \prod_{n=1}^N L_n^{A,\theta,\alpha}$ or $\hat{\lik}(\theta) = \prod_{n=1}^N L_n^{B,\theta,\alpha}$, filtering distributions $\{(X_{n,j}^{F, \theta}, w^{F,\theta}_{n,j})\}_{n,j=1}^{N,J}.$
\end{algorithm}

Our approach has similarities with \cite{svensson18}, in which particles from a previous run at $\phi$ are reweighted to evaluate the likelihood at any sufficiently nearby parameter $\theta$ for $0$-th order optimization.
However, their method is not plug-and-play because they use transition densities to reweight particles, and the variance of their log-likelihood estimate is $O(||\theta - \phi||_2^{2N})$ with an exponential dependence in the horizon. Our approach bypasses these issues -- the first with the \textit{reparameterization trick} to consider particle paths dependent only on $\theta$, and the second as using AD allows us to only evaluate the likelihood and score estimates when $\theta=\phi$. 
%The use of AD bypasses the issue of exponentially increasing variance, but nevertheless can result in problematic polynomial variance.
% Our introduction of $\alpha$ avoids that difficulty.
%also roughly $O(||\theta - \theta_{k-1}||_2^{2N})$, where $N$ is

% Theorem~\ref{thm:mop-functional-forms} shows that MOP-$\alpha$ is constructed so this direct derivative obtains the score estimator of  \cite{poyiadjis11, scibior21} when $\alpha=1$ and that of \cite{naesseth18} when $\alpha=0$.
% Setting $\alpha<1$ adds bias but reduces variance, producing opportunities for a favorable tradeoff as illustrated in theory (Theorem \ref{thm:mop-biasvar}) and in practice (Figure \ref{fig:biasvar}).




\subsection{Algorithm Outline} 

Algorithm~\ref{alg:mop} constructs two coupled sets of particles, one under $\phi \in \Theta$, and another with the process model at $\theta \in \Theta$ but with the resampling indices constructed from the first pass.
The resampling indices, which we write as $k_j \sim \text{Categorical}(g^{\phi}_{n,1},...,g^{\phi}_{n,J})$, are a function of $\phi$ for any value of $\theta$.
For the categorical distribution, we use systematic resampling \citep{arulampalam02,king16} which outperforms multinomial resampling.
In practice, we only ever evaluate MOP-$\alpha$ in the case where $\theta$ and $\phi$ coincide. We then only need to run one particle filter at $\theta=\phi$, otherwise one needs two runs at the same seed $\omega \in \Omega$.

In Algorithm~\ref{alg:mop}, the conditional likelihoods are reweighted by a correction factor accumulated over time to account for the resampling under $\phi$. 
Writing $g^{\theta}_{n,j}={f}_{{Y}_{n}|{X}_{n}}(y_{n}^{*}|{X}_{n,j}^{P,\theta}\giventh{\theta})$ for the measurement density and $L_n^{\phi} = \frac{1}{J}\sum_{m=1}^{J}g^{\phi}_{n,m}$ for the conditional likelihood estimate under $\phi$, we obtain two estimates of the conditional likelihood under $\theta$,
\arxiv{}{\vspace*{-1.5mm}}
\begin{equation}
     \label{eq:mop-conditional-likelihood}
     L_n^{B,\theta,\alpha} = \frac{\sum_{j=1}^Jg^\theta_{n,j}\, w^{P,\theta}_{n,j}}{\sum_{j=1}^J  w^{P,\theta}_{n,j}}, \arxiv{\hspace{15mm}}{\;\;}
     L_n^{A,\theta,\alpha} = L_n^\phi\cdot \frac{\sum_{j=1}^J w^{F,\theta}_{n,j}}{\sum_{j=1}^J  w^{P,\theta}_{n,j}},
     \arxiv{}{\vspace*{-1.5mm}}
\end{equation}
where we recursively correct the weights for each particle for the cumulative error incurred by the off-parameter resampling:
\arxiv{}{\vspace*{-3ex}}
\begin{equation}
    \label{eq:weighting-scheme}
    w_{n,j}^{P,\theta} = (w_{n-1,j}^{F,\theta})^\alpha, 
    \arxiv{\hspace{5mm}}{\;}
    w^{F,\theta}_{n,j} = w^{P,\theta}_{n,k_j} \, g^{\theta}_{n,k_j} \, \big/ g^{\phi}_{n,k_j}, 
    \arxiv{\hspace{5mm}}{\;}
    w^{F,\theta}_{0,j}= 1.
    \arxiv{}{\vspace*{-3ex}}
\end{equation}
The before-resampling estimate $L_n^{B,\theta,\alpha}$ is preferable in practice, as it has slightly lower variance than the after-resampling estimate $L_n^{A,\theta,\alpha}$, but the latter is useful in deriving properties of the MOP-$\alpha$ gradient estimate such as that in Theorem \ref{thm:mop-functional-forms}.

%%\ed{This yields a suitably reweighted particle filter that targets the filtering distribution under $\theta$ and produces a strongly consistent likelihood estimate for the likelihood at $\theta$. As the resampling indices are invariant to $\theta$ when $\omega$ and $\phi$ are fixed, we successfully bypass the issue of Monte Carlo resampling when the simulator and and measurement model are differentiable. Taking the gradient with respect to $\theta$ of the log-likelihood estimate for $\theta$ then yields an estimate for the score at $\theta$. I THINK THIS MATERIAL IS COVERED ELSEWHERE, E.G. EARLIER IN THIS SECTION}



\subsection{Discounting Weights for a Bias-Variance Tradeoff}

Heuristically, $\alpha$ controls the exponentially decaying memory that the filter at time $t_n$, and its resulting gradient estimate, has over the ancestral trajectories prior to $t_n$. If $\alpha=1$ in Algorithm~\ref{alg:mop}, the gradient of the weights, $w^{P,\theta}_{n,j}$ and $w^{F,\theta}_{n,j}$, accumulates as $n$ increases.
This leads to numerical instability and high variance unless $N$ is small.
For $\alpha<1$, the weights from previous timesteps are discounted as in Equation~(\ref{eq:weighting-scheme}), allowing for lower variance. Algorithm~\ref{alg:mop} continues to target the filtering distribution and likelihood if $\theta=\phi$, but is asymptotically biased for the score.
MOP-$1$ and MOP-$0$ correspond to the estimators of \cite{poyiadjis11} and \cite{naesseth18} (Theorem \ref{thm:mop-functional-forms}), while the novel possibility of $0<\alpha<1$ optimizes a bias-variance tradeoff for the score in both theory (Theorem \ref{thm:mop-biasvar}) and practice (Figure \ref{fig:biasvar}). 



\subsection{MOP-$\alpha$ Encompasses the Estimators of \citep{poyiadjis11, naesseth18}}

When $\alpha=1$, MOP-$1$ maintains complete memory of each particle's ancestral trajectory. Theorem~\ref{thm:mop-functional-forms} shows that it recovers the consistent but high-variance score estimate of \cite{poyiadjis11}. 
When $\alpha=0$, MOP-$0$ considers only single-step transition dynamics, recovering the low-variance but asymptotically biased score estimate of \cite{naesseth18} that \cite{scibior21} show is the gradient of a vanilla particle filter when resampling terms are dropped. 

\begin{thm}[MOP-$0$ and MOP-$1$ Functional Forms]
    \label{thm:mop-functional-forms}
    Writing $\nabla_\theta \hat\ell^\alpha(\theta)$ for the gradient estimate yielded by MOP-$\alpha$ when $\theta=\phi$ and using the after-resampling conditional likelihood estimate so that $\hat\lik(\theta) = \prod_{n=1}^N L_n^{A, \theta, \alpha}$, when $\alpha=0$,
    \vspace*{-3ex}
    \begin{equation} \nonumber
        \nabla_\theta \hat\ell^0(\theta) 
        = \frac{1}{J} \sum_{n=1}^N \sum_{j=1}^J \nabla_\theta \log f_{Y_n|X_{n}}(y_n^*|x_{n,j}^{F, \theta}; \theta),
        \vspace*{-3ex}
    \end{equation}
    yielding \cite{naesseth18}'s estimator with the bootstrap filter. When $\alpha=1$,
    \vspace*{-3ex}
    \begin{equation} \nonumber
        \nabla_\theta \hat{\ell}^1(\theta) 
        = \frac{1}{J}\sum_{j=1}^J \nabla_\theta \log f_{Y_{1:N}|X_{1:N}}\left(y_{1:N}^* | x_{1:n,j}^{A, F,\theta}\right),
    \vspace*{-3ex}
    \end{equation}
    yielding the estimator of \cite{poyiadjis11} for the bootstrap filter.
\end{thm}

We defer the proof to \arxiv{Appendix~\ref{appendix:functional}}{the supplementary material}. 
The argument relies on a useful decomposition of the after-resampling estimate $L_n^{A,\theta,\alpha}$. 
Repeated applications of the log-derivative identity that $\nabla_x \log(f(x)) = (\nabla_x f(x))/f(x)$, and noting $\theta=\phi$ implies that $w_{n,j}^{P,\theta}$ evaluates to $1$. 

The result in Theorem \ref{thm:mop-functional-forms} provides mathematical intuition for why \cite{scibior21}'s use of the stop-gradient procedure in the particle filter recovers the estimate of \cite{poyiadjis11}. When $\theta=\phi$, the correction $g_{n,k_j}^\theta/g_{n,k_j}^\phi$ is equivalent to $g_{n,k_j}^\theta / \text{stop\_gradient}(g_{n,k_j}^\theta)$, and is implemented in this way in practice. As such, the use of the stop-gradient in \cite{scibior21} can be seen as the MOP-$\alpha$ weight correction for $\alpha=1, \theta=\phi$.

% This further illustrates how $\alpha$ dictates the memory of the gradient estimate. 
% As \cite{scibior21} remarks, the MOP-$0$ estimator of \cite{naesseth18} only considers single-step quantities, and is ``memoryless'' beyond a single step. 
% In contrast, the MOP-$1$ estimate of \cite{poyiadjis11, scibior21} only considers the surviving particles at time $N$ and so fully tracks dependencies over time. 



% \arxiv{}{\vspace*{-3ex}}
% \subsection{Summary of Theoretical Guarantees}

% The construction of MOP-$\alpha$ bypasses the issue of differentiating through a Monte Carlo algorithm with discontinuous resampling by turning it into a problem of differentiating through a simulator and a series of measurement density ratios.
% We defer the theoretical analysis of MOP-$\alpha$ to Section \ref{sec:thms}, but first we highlight a few key results.
% MOP-$\alpha$ estimates the likelihood and conditional distributions of latent variables, as one expects of a particle filter (Theorem \ref{thm:mop-targeting}).
% When differentiated, we obtain the estimators of \cite{poyiadjis11, scibior21, naesseth18} as special cases (Theorem \ref{thm:mop-functional-forms}).
% In particular, MOP-$1$ is consistent for the score (Theorem \ref{thm:mop-grad-consistency}), has rates for its bias and variance under different choices of $\alpha$ that illustrate the desirable bias-variance tradeoff observed empirically in Figure \ref{fig:biasvar}  (Theorem \ref{thm:mop-biasvar}), and enjoys a linear rate of convergence for gradient descent with the resulting gradient estimate (Theorem \ref{thm:mop-convergence}).  



\section{Practical Maximum-Likelihood Estimation}

%One can think of MOP-$\alpha$ as taking a likelihood estimate at $\phi$, reweighting it to instead target the likelihood at $\theta$, and then taking the derivative with respect to $\theta$ of the above estimate. 

If $\theta$ is evaluated at $\phi$, the particles at $\theta$ and $\phi$ coincide. One then only needs to run one particle filter at $\theta=\phi$, setting the particles at $\phi$ to be copies of the particles at $\theta$ where gradients don't propagate. This is done algorithmically through the \texttt{stop\_gradient()} function in \texttt{JAX}, coinciding with and justifying the use of the stop-gradient operation in \cite{scibior21}.


\begin{algorithm}[htbp!]
	\caption{IFAD}
    \label{alg:ifad}
	    \textbf{Input:} Number of particles $J$, timesteps $N$, IF2 and MOP-$\alpha$ cooling schedules $\eta_m$, MOP-$\alpha$ discounting parameter $\alpha$, $\theta_0$, $m=0.$\newline
        Run initial IF2 search under cooling schedule $\eta_m$ to obtain $\{\Theta_j, j=1,...,J\}$, set $\theta_m := \frac{1}{J}\sum_{j=1}^J \Theta_j.$\newline
		\textbf{While} procedure not converged: \newline
		\hspace*{4mm} Run Algorithm \ref{alg:mop} to obtain $\hat\loglik(\theta_m).$ \newline
		\hspace*{4mm} Set $g(\theta_m) := \nabla_{\theta}(-\hat\loglik(\theta_m))$, and consider any $H(\theta_m)$ s.t. $\lambda_{\min}(H(\theta_m)) \geq c$. \newline
		\hspace*{4mm} Update $\theta_{m+1} := \theta_m - \eta_m (H(\theta_m))^{-1} g(\theta_m)$, $m:=m+1.$ \newline
		\textbf{Return} $\hat{\theta} := \theta_m.$
\end{algorithm}


\subsection{Optimization}

To design an effective procedure for likelihood maximization with MOP-$\alpha$, we propose a simple algorithm we call Iterated Filtering followed by Automatic Differentiation (IFAD) in Algorithm \ref{alg:ifad}. IFAD runs a few iterations of IF2 to warm-start an iterative first or second-order method that uses the MOP-$\alpha$ gradient estimate.
We write the algorithm in a fairly general manner, requiring only a Hessian estimate $H(\theta)$ with minimum eigenvalue no smaller than $c$, but in our experiments we use $H=I_p$, corresponding to gradient descent.
To see the importance of this innovation, we briefly summarize the empirical results in Section~\ref{sec:cholera}.

The convergence of IF2 (and so the first stage of IFAD) to a neighborhood of the MLE happens rapidly in practice. 
In the case of the Dhaka cholera model of \cite{king08}, when an aggressive geometric cooling multiplier of 0.95 and initial random walk standard deviation of 0.02 is used, we require no more than 40 iterations of IF2 to obtain an acceptable warm-start. 
However, even when applying much larger amounts of Monte Carlo effort, IF2 fails to get within seven log-likelihood units of the maximum on this problem (Figure~\ref{fig:boxplot}).
These observations are consistent with theoretical results for iterated filtering, showing slow convergence close to the maximum \citep{doucet15-if}.
IF2, without assistance from AD, requires additional computationally intensive techniques, such as repeated pruning and profiling, to reach the inferentially critical region within one log-likelihood unit of the maximum.

Gradient methods can struggle with saddle points, local minima, and nonlinear ridges.
Without initialization using IF2, Section~\ref{sec:cholera} shows that AD alone can perform poorly.
Conversely, switching to gradient ascent with MOP-$\alpha$ score estimates lets one bypass the difficulty that IF2 has with optimizing the last few units of log-likelihood. 
In combination, these two methods avoid their respective weaknesses and provide, for the first time, straightforward and effective likelihood maximization for our benchmark problem.


\subsection{Linear Convergence Rates}

The AD stage confers a linear convergence rate for IFAD, up to a small region near the MLE shrinking in $J$, under the usual smoothness and strong convexity assumptions in convex optimization theory, as we show below in Theorem \ref{thm:mop-convergence}.

\begin{thm}[Linear Convergence of IFAD] \label{thm:linear}
    
Consider the second stage of IFAD (Algorithm \ref{alg:ifad}), stopping if $\|\nabla_\theta \hat\ell^\alpha(\theta_m)\| \leq (1+\sigma) \epsilon$, where $\sigma \geq \frac{4 \Gamma}{(1-\beta)}$ for some $\beta \in (0,1)$. Assume strongly convex and smooth $-\ell$, $\gamma I \preceq \nabla_\theta^2 (-\ell) \preceq \Gamma I$, and assumptions (A1--A5) in Section~\ref{sec:thms}. 
Choose the learning rate $\eta$ so $\eta \leq \frac{c(1-\beta)}{2\Gamma}$. 
Then, for $\alpha$ sufficiently close to 1 and $J$ sufficiently large (depending on $\epsilon$ and $\delta$), we have that
$\|\nabla_\theta \hat\ell^\alpha(\theta_m) - \nabla_\theta\ell^\alpha(\theta_m)\| \leq \epsilon$ and $H(\theta_m) \succ cI_p \succ 0$ for all $m$, and that
\arxiv{}{\vspace*{-3ex}}
$$
\ell(\theta^*) - \ell(\theta_{m+1}) \leq \Big(1-\eta\beta\, \frac{8\gamma}{9c}\Big)\big(\ell(\theta^*)-\ell(\theta_m)\big) \text{ with probability at least } 1-\delta.
$$
\label{thm:mop-convergence}
\end{thm}
\arxiv{}{\vspace*{-10ex}}
The proof of Theorem~\ref{thm:linear} uses results from randomized numerical linear algebra, building on Theorem~6 of \cite{mahoney16}. 
Details are deferred to \arxiv{Appendix~\ref{appendix:convergence}}{the supplementary information}. 
This result shows that particle estimates for the gradient can enjoy the same linear convergence rate on well-conditioned problems as gradient descent with access to the score. 

We therefore see that the second stage of IFAD converges linearly to the MLE if (1) the log-likelihood surface is well-conditioned in a neighborhood of the MLE and (2) the first stage of IFAD successfully reaches a (high-probability) basin of attraction of the MLE.
Within a neighborhood of the MLE that grows as more data become available \citep{ning21}, local quadratic approximations such as local asymptotic normality apply under general conditions \citep{lecam00}.
This suggests that the linear convergence may occur often in practice.

%% This happens fairly often in practice, for example, when local asymptotic normality of the MLE holds. We conjecture that this applies to the entirety of IFAD, as IF2 converges very quickly to a neighborhood of the MLE, and will explore this in future work.

%IF2 posesses some SGD-like qualities, including what looks like linear convergence to a ball around the MLE with width determined by the random walk standard deviation. We will explore this conjecture in future work.

%While this theoretical guarantee currently only holds for IFAD-1, as it forms a particle approximation of the score as in \cite{poyiadjis11}, one can also extend this for a similar guarantee if the bias for the gradient estimate given by MOP-$\alpha$ for $\alpha < 1$ is small enough. For general $\alpha$, including MOP-0, the analysis then becomes more complicated, as one needs to handle the case of biased gradient descent. However, if future measurements $y_{n+1:N}^*$ do not provide a large amount of information on the identification of the current state $x_n$ given past and current measurements $y_{0:n}^*$, as mentioned in \cite{corenflos21}, the bias may be small enough for it to not present too much of a problem.

%The warm start allows gradient methods to shine, as regularity conditions ensure that the likelihood surface is well-behaved enough in this neighborhood that the aforementioned issues with saddle points and local minima are alleviated. 



\section{Application to a Cholera Transmission Model}
\label{sec:cholera}

The cholera transmission model that \cite{king08} developed for Dhaka, Bangladesh, has been used to benchmark the performance of various POMP inference methods \citep{ionides15, fasiolo16, wycoff24}, and we employ it here for the same purpose.
This model categorizes individuals in a population as susceptible, $S(t)$, infected, $I(t)$, and recovered, $R(t)$ and so is called an SIRS compartmental model.
In this case, the compartment $R(t)$ is further subdivided into 
%. where the population at time $t$, $H_t$, is divided into the susceptible compartment $S_t$, infected compartment $I_t$, and
three recovered compartments $R^1(t)$, $R^2(t)$, $R^3(t)$ denoting varying degrees of cholera immunity.
We write $P(t)$ for the total population, and $M_n$ for the cholera deaths in each month.
As in \citep{king08, ionides15}, the transition dynamics follow a series of stochastic differential equations:
\vspace*{-1mm}
\begin{align*}
    dS&=\big(k \epsilon R^k+\delta(P-S)-\lambda(t) S\big)\, dt+d P-({\sigma S I}/{P})\, dB, \\
    dI&=\big(\lambda(t) S-(m+\delta+\gamma) I\big)\, dt+({\sigma S I}/{P})\, dB, \\
    dR^1&=\big(\gamma I-(k \epsilon+\delta) R^1\big)\, dt, \hspace{3ex} \dots \\
    dR^k&=\big(k \epsilon R^{k-1} -(k \epsilon+\delta) R^k\big)\, dt,
    \vspace*{-3ex}
\end{align*}
with Brownian motion $B(t)$, cholera death rate $m$, recovery rate $\gamma$, mean immunity duration $1/\epsilon$, standard deviation of the force of infection $\sigma$, and population death rate $\delta=0.02$. The force of infection, $\lambda_t$, is modeled by splines $(s_j)_{j=1}^6$
\vspace*{-3ex}
\begin{equation*}    \lambda_t=\exp\hspace{-1mm}\left(\hspace{-.5mm}\beta_{\text{trend}}(t-t_0)+\hspace{-1mm}\sum_{j=1}^{6} \beta_j s_j(t)\hspace{-1mm}\right)\hspace{-1mm}\frac{I}{P} + \exp\hspace{-1mm} \left(\sum_{j=1}^{6} \omega_j s_j(t)\hspace{-1mm}\right)\hspace{-1mm},
    \vspace*{-3ex}
\end{equation*}
where the coefficients $(\beta_j)_{j=1}^6$ model seasonality in the force of infection, $\beta_{\text{trend}}$ models the trend in the force of infection, and the $\omega_j$ represent seasonality of a non-human environmental reservoir of disease.
The measurement model for observed monthly cholera deaths is
    $Y_n \sim \gN(M_n, \tau^2M_n^2)$,
where $M_n=\gamma\int_{t_{n-1}}^{t_n}I(s)\, ds$ is the modeled cholera deaths in that month.


\begin{figure}
    \centering
    \includegraphics[width=\arxiv{14cm}{\textwidth}]{imgs/095/tikzcholera.png}
    \vspace*{-7mm}
    \caption{A compartment flow diagram for the Dhaka cholera model from \cite{king08}.}
    \label{fig:tikz-cholera}
    \arxiv{}{\vspace*{-3mm}}
\end{figure}


\subsection{Results}

We tested IFAD against IF2 on a global search problem for the Dhaka cholera model.
We re-implemented IF2 to do so, but we also compare our results with the results of \cite{ionides15} (labeled ``IF2 2015'').
Our re-implementation reaches a better log-likelihood than that of \cite{ionides15} due to a better choice of algorithmic parameters.
For each method, we performed 100 searches, initialized with 100 starting parameter vectors drawn uniformly from the same wide bounding box used in \cite{ionides15}. We summarize our findings below. 

% \kevin{Dynamically save to a tex file and load from there}
  
\arxiv{}{\vspace*{-1mm}}
\begin{table}[htbp!]
\centering
\input{imgs/095/table}
\caption{Maximum log-likelihood found by IF2, IFAD, and MOP alone. IFAD performs the best among all methods.}
\arxiv{}{\vspace*{-7mm}}
\label{table:mle}
\end{table}

\subsection{IFAD Successfully Finds the MLE:} Previously, an MLE at a log-likelihood of $-3748.6$ was reported by \cite{king16}.
This MLE was obtained with much computational effort, using many global and local IF2 searches and with the assistance of likelihood profiling, 8 years after the model was first proposed by \cite{king08}.
Meanwhile, \cite{ionides15} only achieve a maximum log-likelihood of $-3768.6$, and \cite{king08} only $-3793.4$.
Despite being initialized for a global search, IFAD manages to get much closer to the MLE over the $100$ searches than \cite{ionides15} and finds it up to $2.5$ standard deviations of Monte Carlo log-likelihood error, as seen in Table~\ref{table:mle}.
On this problem, the sequence of local searches, refinement, and likelihood profiling that was previously required for finding the MLE is not necessary with IFAD. 
% In other words, the improvement in numerical efficiency of IFAD over IF2 is so large that routine application of IFAD (consisting of a collection of Monte Carlo replications from random starting points) generates results outside the reach of a routine application of IF2. 
When interpreting Table~\ref{table:mle}, note that by Wilks' Theorem, any difference of over $1.92$ log units has statistical relevance when testing one parameter at the $0.05$ significance level, and therefore may have scientific value.

\subsection{IFAD Outperforms Both IF2 and MOP Alone:} While IF2 quickly approaches a neighborhood of the MLE within only 40 iterations, IF2 alone ultimately fails to achieve the last few log-likelihood units, as no IF2 search comes within 7 log-likelihood units of the MLE (as seen in Figure \ref{fig:scatter}). Conversely, we encountered many failed searches with gradient descent alone. This is a difficult, nonconvex, and noisy problem with $18$ parameters, and gradient descent without a warm-start gets stuck in local minima and saddle points.

IFAD, in comparison, approaches the MLE quickly due to the IF2 warm-start (as seen in Figure \ref{fig:optim}) and also succeeds at refining the coarse solution found by the warm-start with MOP gradient steps to find the MLE (as seen in Figures~\ref{fig:scatter} and \ref{fig:boxplot}). IFAD therefore successfully combines the best qualities of IF2 and MOP, outperforming either of them alone.
Monte~Carlo replication is appropriate for IFAD, or any Monte Carlo algorithm used to solve a challenging numerical problem, but Figure~\ref{fig:boxplot} shows that IFAD can find higher likelihood values, more quickly and more reliably than the previous state-of-the-art. 
In particular, Figure~\ref{fig:boxplot} shows that IFAD with $\alpha=0.97$ can maximize the challenging likelihood of \cite{king08} using a modest number of iterations and Monte Carlo replications, whereas previously it was necessary to carry out an extensive customized search or to live with an incompletely maximized likelihood.

For the results presented here, we did not employ the many common heuristics used in the machine learning and optimization literature in the gradient descent stage.
We used constant learning rates of $0.01, 0.05$, and $0.2$ for IFAD-$1,0,$ and $0.97$ respectively, and a constant cooling rate of $0.95$ for our IF2 implementation.
While techniques such as annealing learning rates, gradient normalization, and momentum could further improve the performance of IFAD in other simulations we performed, we chose to report the results of the simplest implementation to serve as a baseline for the method's performance.
The results for this basic implementation already showcase the merits of the approach.

\begin{figure}[htbp!]
    \centering
    \vspace{1mm}
    \includegraphics[width=\arxiv{8cm}{\textwidth/\real{2.2}}]{imgs/095/pairs.png}
    \includegraphics[width=\arxiv{8cm}{\textwidth/\real{2.2}}]{imgs/095/qq.png}
    %\arxiv{}{\vspace*{-1mm}}
    \caption{Scatterplots depicting the performance of IFAD against that of IF2. \textbf{A:} Paired searches from the same starting point. Tuning $\alpha$ allows IFAD-$0.97$ to strictly improve on IF2, \cite{poyiadjis11}, and \cite{naesseth18}, on almost every initialization. \textbf{B:} Q-Q plot of ranked IFAD searches against ranked IF2 searches. IFAD performs best on average, and approaches the MLE while no IF2 search successfully gets within 7 log-likelihood units of it. The dotted red line shows the true maximized log-likelihood. }
    \label{fig:scatter}
    \arxiv{}{\vspace*{-4mm}}
\end{figure}

%\kevin{Include plots of IFAD vs Poyiadjis and Naesseth only in the supplemental information}

%% \ed{Labeling A and B would be more conventional than Left and Right, and also easier to refer to in the text.}
  
\begin{figure}[ht]
    \centering
    \includegraphics[width=\arxiv{8cm}{\textwidth/\real{2.2}}]{imgs/095/boxplot.png}
    \includegraphics[width=\arxiv{8cm}{\textwidth/\real{2.2}}]{imgs/095/boxplot_all.png}
    %\arxiv{}{\vspace*{-1mm}}
    \caption{\textbf{A:} Raincloud plot depicting the performance of IFAD and IF2 where we plot the results of the best run out of every ten runs, representing the common procedure of running a few searches and choosing the best one. \textbf{B:} Raincloud plot of all searches. IFAD outperforms all other methods.
    The dotted red line shows the true maximized log-likelihood.}
    \label{fig:boxplot}
    \arxiv{}{\vspace*{-5mm}}
\end{figure}

\begin{figure}[ht]
    \centering
    \vspace{1mm}
    \includegraphics[width=\arxiv{10cm}{\textwidth/2}]{imgs/095/optim.png}
    \arxiv{}{\vspace*{-1mm}}
    \caption{Optimization progress of IFAD and IF2. The dashed orange line depicts the median warm-start given by running 40 IF2 iterations. Running 60 more iterations of IF2 ultimately underperforms IFAD. We see that IFAD has better tail control and successfully reaches the MLE. 
    We use a dotted red line to display the MLE.}
    \label{fig:optim}
    \arxiv{}{\vspace*{-3ex}}
\end{figure}


\section{Application to Bayesian Inference}
\label{sec:bayes}

Particle MCMC \citep{andrieu10} is arguably the most popular method for full-information plug-and-play Bayesian inference for POMP models.
However, particle Metropolis-Hastings often mixes slowly, requiring long burn-in periods.
For instance, \cite{fasiolo16} reported needing $700000$ burn-in iterations for effective sampling in the \cite{king08} model, even after coarsening each Euler timestep to reduce the computational burden. 

To alleviate this, we employ a No U-Turn Sampler (NUTS) \citep{homan14} powered by MOP-$\alpha$, with a nonparametric empirical prior constructed from density estimation of the parameter swarm from the IF2 warm-start. 
%This prior has a standard deviation of $6.3\%$ of the MLE. 
%\sqrt{(0.02\cdot0.95^{40})^2 \cdot 600}\approx 6.3\%
Using NUTS with MOP-$\alpha$ and this prior significantly lowers the mixing time of the Markov chain to as little as $500$ iterations (Figure \ref{fig:nuts-eb}). 

\begin{figure}[t!]
    \centering
    \vspace{1mm}
    \includegraphics[width=\arxiv{\textwidth/\real{1.25}}{\textwidth}/\real{1.25}]{imgs/pmcmc/nuts_eb.png}
    \caption{Histogram and density estimate of the posterior of $\beta_{\text{trend}}$, as well as convergence diagnostics for NUTS with $4$ chains. NUTS mixes quickly when regularized by an empirical prior with a standard deviation of $6.3\%$ of the MLE, and the posterior estimates from each chain share roughly the same posterior mode.}
    \label{fig:nuts-eb}
    \arxiv{}{\vspace*{-4mm}}
\end{figure}

Without the IF2 prior, NUTS with the MOP-$\alpha$ estimate alone tended to diverge.
In contrast, particle Metropolis-Hastings, with or without said prior, fails to effectively explore the parameter space.
We display these results \arxiv{in Figures \ref{fig:mh} and \ref{fig:nuts} in Appendix~\ref{appendix:bayes}}{within the supplementary information}. 
This speedup in the mixing time of particle MCMC by over three orders of magnitude provides hope that particle Bayesian inference might be increasingly employed for the complex scientific models commonly encountered in areas like epidemiology.


\section{Theoretical Analysis of MOP-$\alpha$}
\label{sec:thms}

Here, we will show that MOP-$\alpha$ targets the filtering distribution and likelihood under $\theta$, is consistent when $\alpha=1$, and characterize rates for its bias and variance under different choices of $\alpha$. To do so, we require the following assumptions:

\arxiv{}{\vspace*{-3ex}}
\begin{enumerate}[label=(A\arabic*),itemsep=-1ex, , font=\small] 
    \item \textbf{Continuity of the Likelihood.} $\ell(\theta)$ has more than two continuous derivatives in a neighborhood $\left\{\theta: \ell(\theta)>\lambda_1\right\}$ for some $\lambda_1<\sup _{\varphi} \ell(\varphi)$.\label{assump:conti-lik}
    \item \textbf{Bounded Process Model.} There exist $\underbar{M}, \bar{M}$ such that $0 < \underbar{M} \leq f_{X_n|X_{n-1}}(x_n | x_{n-1};\theta) \leq \bar{M} < \infty$.\label{assump:bounded-process}
    \item \textbf{Bounded Measurement Model.} There exist $\underbar{G}, \bar{G}$ such that $0<\underbar{G} \leq f_{Y_n \mid X_n}\left(y_n^* \mid x_n; \theta\right) \leq \bar{G}<\infty$, and $G'(\theta)$ with $\|\nabla_\theta \log f_{Y_n \mid X_n}\left(y_n^* \mid x_n; \theta\right)\|_\infty \leq G'(\theta)< \infty$.\label{assump:bounded-measurement}
    \item \textbf{Bounded Gradient Estimates.} There are functions $G(\theta), H(\theta): \Theta \to [0,\infty)$ uniformly bounded by $G^*, H^*<\infty$, so the MOP-$\alpha$ gradient and Hessian estimates at $\theta=\phi$ are almost surely bounded by $G(\theta)$ and $H(\theta)$ for all $\alpha$.\label{assump:local-bounded-derivative}
    \item \textbf{Differentiable Density Ratios and Simulator.} The measurement density, \arxiv{\\}{}$f_{Y_n|X_n}(y_n^*|x_n; \theta)$, and simulator have more than two continuous derivatives in $\theta$.\label{assump:diff-meas-and-sim}
\end{enumerate}
\arxiv{}{\vspace*{-3ex}}

In particular, assumptions \ref{assump:bounded-process} and \ref{assump:bounded-measurement} imply that the POMP model satisfies a strong mixing condition, and as such are fairly strong but commonplace in the literature \citep{karjalainen23, delMoral04}. The fact that the likelihood estimate yielded by MOP-$\alpha$ has more than two continuous derivatives in $\theta$ follows from the construction of the likelihood estimate in equations \ref{eq:mop-conditional-likelihood} and \ref{eq:weighting-scheme}, as well as Assumptions \ref{assump:bounded-measurement} and \ref{assump:diff-meas-and-sim}. 


\subsection{MOP-$\alpha$ Targets the Filtering Distribution}


We show here that MOP-$\alpha$ targets the filtering distribution under $\theta$ and is strongly consistent for the likelihood under $\theta$ when $\alpha=1$ or $\theta=\phi$.
Employing $\alpha<1$ lead to inconsistency when $\theta$ deviates from $\phi$, but this bias disappears as $\theta$ approaches $\phi$.

While the result is presented here as specific to MOP-$\alpha$, we prove a more general result in \arxiv{Appendix~\ref{appendix:targeting}}{the supplementary material}.
This is a strong law of large numbers for triangular arrays of particles resampled arbitrarily, with resampling probabilities not proportional to the targeted distribution of interest, but reweighted to correct for the cumulative discrepancy between the resampling and the target distribution.

%\kevin{Should we just show the general result? That is, for an arbitrary set of indices $k_{n,j}$ generated in proportion to some arbitrary weights $\pi_{n,j}$ that do not necessarily sum to $1$ as the estimate is self-normalized, reweighting the particles by $w_{n,a(j)}g_{n,a(j)}/\pi_{n,a(j)}$ targets the posterior. $\pi_{n,j}$ can depend on $\{(x_{n,1:J},g_{n,1:J})\}$ as long as the resampling is carried out independently of $\{(x_{n,1:J},g_{n,1:J})\}$, conditional on $\pi_{n,1:J}$}

\begin{thm}[MOP-$\alpha$ Targets the Filtering Distribution and Likelihood]
    \label{thm:mop-targeting}
    When $\alpha=1$ or $\theta=\phi$, MOP-$\alpha$ targets the filtering distribution under $\theta$ and is strongly consistent for the likelihood under $\theta$. That is, for $\pi_n(\theta)=f_{X_{n}|Y_{1:n}}(x_n|y_{1:n}^* ; \theta)$ and any measurable and bounded functional $h$ and for $\hat\lik(\theta) = \prod_{n=1}^N L_n^{A,\theta,\alpha}$ or $\prod_{n=1}^N L_n^{B,\theta,\alpha}$, it holds that
    \arxiv{}{\vspace*{-1.5mm}}
    \begin{equation} \nonumber
        \frac{\sum_{j=1}^J h(x_{n,j}^{F, \theta}) \, w_{n,j}^{F,\theta}}{\sum_{j=1}^J w_{n,j}^{F,\theta}} \stackrel{a.s.}{\to} E_{\pi_n(\theta)} \big[h(X_n)\big], \hspace{5mm} \hat\lik(\theta)  \stackrel{a.s.}{\to} \lik(\theta).
    \arxiv{}{\vspace*{-1.5mm}}
    \end{equation}
\end{thm}

%\kevin{Write the supplement formally with Del Moral-lite language, going into full detail with the proofs and calculations.}

\begin{proof}
    We provide a proof sketch here, deferring the details to \arxiv{Appendix~\ref{appendix:targeting}}{the supplementary information}. 
    When $\theta=\phi$, the ratio ${g_{n,j}^\theta}/{g_{n,j}^\phi}=1$ regardless of $\alpha$, reducing to the vanilla particle filter.
    When $\alpha=1$ and $\theta\neq\phi,$ suppose inductively that $\{(X^{F,\theta}_{n-1,j},w^{F,\theta}_{n-1,j})\}_{j=1}^J$ targets $f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}|y^*_{1:n-1};\theta)$.
    It can then be shown that $\{(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j})\}_{j=1}^J$ targets $f_{X_{n}|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)$, that $\{(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j} \, g^\theta_{n,j} )\}_{j=1}^J$ targets  $f_{X_{n}|Y_{1:n}}(x_{n}|y^*_{1:n};\theta)$, and that weighting the particles by $(X^{F,\theta}_{n,j},w^{F,\theta}_{n,j}) = (X^{P,\theta}_{n,k_j}, w^{P,\theta}_{n,k_j} \, g^\theta_{n,k_j} \big/ g^\phi_{n,k_j})$,
    resampling the $k_j$ with probabilities proportional to $g^\phi_{n,j}$, also targets $f_{X_{n}|Y_{1:n}}(x_{n}|y^*_{1:n};\theta)$.
    Estimating the likelihood with the before-resampling estimator $\hat\lik(\theta) = \prod_{n=1}^N L_n^{B,\theta,\alpha}$, we see that the strong consistency is a direct consequence of our result that $\{ \big(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j}\big) \}$ targets $f_{X_{n}|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)$. 
\end{proof}


\subsection{MOP-$1$ Score Estimate Is Consistent}

Although we have shown the MOP-$1$ gradient estimate yields the estimate of \citep{poyiadjis11, scibior21} when applied to the bootstrap filter, \cite{poyiadjis11, scibior21} estimate the Fisher score by 
$\frac{1}{J}\sum_{j=1}^J \nabla_\theta \log f_{X_{0:N}, Y_{1:N}}(x_{0:n,j}^{A, F,\theta}, y_{1:N}^* ; \theta)$ and not
$\frac{1}{J}\sum_{j=1}^J \nabla_\theta \log f_{Y_{1:N}| X_{1:N}}(y_{1:N}^* | x_{1:n,j}^{A, F,\theta}; \theta).$
It is therefore not immediately apparent that these two converge to the same thing. As such we directly show the consistency of the MOP-$1$ gradient estimate below. 

\begin{thm}[Consistency of MOP-$1$ Gradient Estimate]
    The gradient estimate of MOP-$\alpha$ when $\alpha=1$, $\theta=\phi$ is strongly consistent for the score: $\nabla_\theta \hat\ell_J^1(\theta) \stackrel{a.s.}{\to} \nabla_\theta \ell(\theta)$ as $J \to \infty$.
    \label{thm:mop-grad-consistency}
\end{thm}


We present a proof outline here, postponing the full argument to \arxiv{Appendix~\ref{appendix:consistency}}{the supplement}
Assumption \ref{assump:local-bounded-derivative} lets us show uniform equicontinuity of the sequence for almost every $\omega \in \Omega$, allowing us to apply Arzela-Ascoli and Theorem \ref{thm:mop-targeting} to establish uniform convergence for the derivatives as a sequence $(\nabla_\theta \hat\lik_J^1(\theta)(\omega))_{J \in \mathbb{N}}$. This is a sufficient condition for swapping the limit and derivative to show that for almost every $\omega \in \Omega$, $\lim_{J \to \infty} \nabla_\theta \hat\lik_J^1(\theta)(\omega) = \nabla_\theta \lim_{J \to \infty} \hat\lik_J^1(\theta)(\omega) = \nabla_\theta \lik(\theta).$

% \begin{proof}
%     Fix $\omega \in \Omega$, $\phi = \theta$. The sequence $(\nabla_\theta \hat\lik_J^1(\theta)(\omega))_{J \in \mathbb{N}}$ is uniformly bounded over all $J$ by Assumption \ref{assump:local-bounded-derivative}. Again by \ref{assump:local-bounded-derivative}, the second derivative of $\hat\lik_J^1(\theta)(\omega)|_{\theta=\theta'}$ is also uniformly bounded over all $J$ by $H^*$ for almost every $\omega\in \Omega$ and every $\theta'\in \Theta$. So $(\nabla_\theta \hat\lik_J^1(\theta)( \omega))_{J \in \mathbb{N}}$ is uniformly Lipschitz, and therefore uniformly equicontinuous for almost every $\omega \in \Omega$.

%     By Arzela-Ascoli, there is a uniformly convergent subsequence. But there is only one subsequential limit, as we can treat the gradient estimate at $\theta$ as a bounded functional of the particles by \ref{assump:local-bounded-derivative}, allowing us to apply Theorem \ref{thm:mop-targeting} to see that the sequence $(\nabla_\theta \hat\lik_J^1(\theta)(\omega))_{J \in \mathbb{N}}$ converges pointwise for $\theta=\phi$ and almost every $\omega \in \Omega$. So the whole sequence must converge uniformly to $\lim_{J \to \infty} \nabla_\theta \hat\lik_J^1(\theta)(\omega).$ 
    
%     With uniform convergence for the derivatives established, we can swap the limit and derivative and obtain, with the strong consistency $\hat{\lik}_J^1(\theta)(\omega) \stackrel{a.s.}{\to} \lik(\theta)$ in Theorem \ref{thm:mop-targeting}, that for almost every $\omega \in \Omega$, 
%     $\lim_{J \to \infty} \nabla_\theta \hat\lik_J^1(\theta)(\omega) = \nabla_\theta \lim_{J \to \infty} \hat\lik_J^1(\theta)(\omega) = \nabla_\theta \lik(\theta).$
%     The result follows by the continuous mapping theorem. 
% \end{proof}


\subsection{MOP-$\alpha$ Error, Bias and Variance}

We now provide a result showing that MOP-$\alpha$ has a desirable bias-variance tradeoff when $0<\alpha<1$.
This is achieved because it combines favorable properties from the low-variance but asymptotically biased estimate of \cite{naesseth18} and the high-variance but consistent estimate of \cite{poyiadjis11}. 
As the bias itself is difficult to analyze, we instead analyze the MSE and variance. 
The below result applies for any $\alpha \in [0,1)$, but not $\alpha=1$, as the gradient estimate when $\alpha=1$ has no forgetting properties. 

\begin{thm}[MSE and Variance of MOP-$\alpha$]
    \label{thm:mop-biasvar}
    When $\alpha\in (0,1)$ and $\theta=\phi$, define $\psi_k(\alpha)=(\alpha^k  + \alpha^{k+1} - \alpha)/(1-\alpha)$. 
    There exists an $\epsilon>0$ depending on $\bar{M}, \underbar{M}, \bar{G}, \underbar{G}$ as in \cite{karjalainen23} such that the MSE and variance of MOP-$\alpha$ are:
    \vspace*{-1ex}
    \arxiv{\begin{eqnarray}
        \E\big\|\nabla_\theta\ell(\theta) - \nabla_\theta \hat\ell^\alpha(\theta)\big\|_2^2 
        &\lesssim& \min_{k \leq N} Np \, G'(\theta)^2\left(k^2J^{-1}+(1-\epsilon)^{\floor{k/(c\log(J))}}+k+\psi_k(\alpha)\right), \label{eq:mop-mse}
        \\
        \Var\big(\nabla_\theta \hat\ell^{\alpha}(\theta)\big) &\lesssim& \min_{k\leq N} Np \, G'(\theta)^2\left(\frac{k^2}{(1-\alpha)^2J} + \frac{\alpha^{k}}{1-\alpha}N\right). \label{eq:mop-variance}
        \end{eqnarray}}{\begin{align}
        &\E\big\|\nabla_\theta\ell(\theta) - \nabla_\theta \hat\ell^\alpha(\theta)\big\|_2^2 \label{eq:mop-mse}\\
        &\lesssim \min_{k \leq N} Np \, G'(\theta)^2\left(k^2J^{-1}+(1-\epsilon)^{\floor{k/(c\log(J))}}+k+\psi_k(\alpha)\right), \nonumber 
        \\
        &\Var\big(\nabla_\theta \hat\ell^{\alpha}(\theta)\big) \lesssim \min_{k\leq N} Np  G'(\theta)^2\left(\frac{k^2}{(1-\alpha)^2J} + \frac{\alpha^{k}}{1-\alpha}\right). \label{eq:mop-variance}
        \end{align}}
\end{thm}
\arxiv{}{\vspace*{-1ex}}

We defer the proof to \arxiv{Appendix~\ref{appendix:biasvar}}{the supplementary material}, but provide a brief outline here. 
The variance bound can be reduced to the approximation error between MOP-$\alpha$ and a variant called MOP-$(\alpha,k)$ where the discounted weights are truncated at lag $k$. 
The discount factor, $\alpha$, ensures strong mixing.
The covariance of the derivative of MOP-$(\alpha,k)$ can be controlled with Davydov's inequality and a standard $L^p$ error bound for the particle filter. 
The MSE bound considers the error between the score and MOP-$(1,k)$, and the error between MOP-$(1,k)$ and MOP-$\alpha$. 
The latter can be shown to be $\tilde{O}\big(Np \, G'(\theta)^2(k+\psi_k(\alpha))\big)$, and the former can be controlled with a result on the the forgetting of the particle filter from \cite{karjalainen23} and the very same $L^p$ error bound mentioned above. 


\subsection{Interpretation:} Theorem \ref{thm:mop-biasvar} provides theoretical understanding of the empirically favorable bias-variance tradeoff enjoyed by MOP-$\alpha$ in Figure \ref{fig:biasvar}.
The first term in the MSE bound in Equation \ref{eq:mop-mse} corresponds to particle approximation error, the second mixing error, and the third and fourth the error between the MOP-$(1,k)$ and MOP-$\alpha$ estimates. 
As $\alpha$ goes to $1$, choosing $k$ appropriately, the first, third and fourth term (corresponding to the variance) increases, while the second term (corresponding to the bias) decreases. 
Likewise, the first term in the variance bound in Equation \ref{eq:mop-variance} corresponds to particle approximation error, while the second corresponds to mixing error. 
The variance increases as $\alpha$ goes to $1$. 


\subsection{Bias-Variance Tradeoff:} We show in \arxiv{Appendix~\ref{appendix:biasvar}}{the supplementary material} that the variance of MOP-$0$ is $\tilde{O}\big( Np \, G'(\theta)^2\big/J \big)$.
Previously, \cite{poyiadjis11} established that the variance of MOP-$1$ is $\tilde{O}(N^4/J)$, ignoring factors of $p$ and $G'$. 
Combining these results with Theorem \ref{thm:mop-biasvar} shows that MOP-$\alpha$ interpolates between the variances of MOP-$0$ and MOP-$1$, as $N \leq Nk^2 \leq N^4$, with a phase transition as soon as $\alpha<1$. 
The phase transition arises because even though the particle filter has forgetting properties \citep{karjalainen23}, the resulting derivative estimate of \cite{poyiadjis11} does not, and we require both for the variance reduction. 

MOP-$\alpha$ achieves a lower MSE than MOP-$1$, as $\alpha, k$ can be as large as desired to balance the impact of the last three terms of Equation \ref{eq:mop-mse}. We also show that MOP-$0$ achieves a MSE of $\tilde{O}\big(NpG'(\theta)^2(J^{-1}+(1-\epsilon)^{\lfloor1/c\log(J)\rfloor})\big)$, where the second term corresponds to uncontrolled mixing error. Unlike MOP-$\alpha$, MOP-$0$ has no opportunity to tune $\alpha$ (or $k$) to reduce said mixing error, leading to uncontrolled asymptotic bias. 



\section{Computational Efficiency}

MOP-$\alpha$ and IFAD are fast algorithms, both in theory and practice. 
In line with the cheap gradient principle of \cite{kakade2019provably}, getting a gradient estimate from MOP-$\alpha$ takes no more than 6 times that of the runtime of the particle filter. In our simulations, MOP-$\alpha$ took 3.75 times the runtime of the particle filter. 
MOP-$\alpha$ and IFAD therefore share the same $O(NJ)$ time complexity as the particle filter, unlike the $O(NJ^2)$ complexity of \cite{corenflos21} and Algorithm 2 in \citep{poyiadjis11, scibior21}.

%% As IF2 amounts to one iteration of the particle filter, with $O(NJ)$ time complexity. 
%%In practice,


Our implementation of the particle filter, simulator, and MOP-$\alpha$ in \texttt{JAX} \cite{jax} enabled us to take advantage of just-in-time compilation and GPU acceleration, even with a simulator written in Python. This led to a 16x speedup (379ms vs 6.29s on a Intel i9-13900K CPU and NVIDIA RTX3090 GPU) over the CPU-only implementation of the particle filter (with a simulator written in C++) in the \texttt{pomp} package of \cite{king16}. This has since spurred the development of a new open source Python library named \texttt{pypomp}\footnote{The package can be found at \href{https://pypi.org/project/pypomp/}{https://pypi.org/project/pypomp/}. The source code for this paper and original implementation can be found at \href{https://github.com/hetankevin/diffPomp}{https://github.com/hetankevin/diffPomp}.}, which incorporates the improvements in methodology and computation described within this paper.

%\kevin{Write about new open source library that brings pomp to python with AD. There are implementations in tfp under the jax substrate, but these are not plug and play -- they require you to supply the transition densities. }




\section{Discussion}

Practical likelihood-based data analysis involves many likelihood optimizations \citep{king08,blake14,pons-salort18,subramanian21,fox22,drake23}.
This occurs with profile likelihood confidence intervals and model selection. A careful scientist should consider many model variations, to see whether the conclusions of the study are sensitive to alternative model specifications.
Coding model variations is relatively simple when using plug-and-play inference methodology.
However, assessing the scientific potential of these variations requires likelihood optimization.
Speeding up the process of model fitting has practical consequences for the rate at which scientists can evaluate hypotheses.
We have demonstrated, for the first time, a plug-and-play maximum likelihood approach which succeeds in finding the MLE from a coarse global search in the complex benchmark model of \cite{king08}.
We anticipate that this will promote scientific advances in all domains where complex POMP models arise.


\subsection{Further Work}

If the simulator is discontinuous in $\theta$, MOP-$\alpha$ no longer applies.
We are working on a variation of MOP-$\alpha$ applicable to this case.
Specifically, differentiable transition density ratios can be used instead of a differentiable simulator.
The off-parameter treatment of the measurement model is extended to an off-parameter simulation for the dynamic process.
In that case, one requires access to the transition densities, or at least their ratios. 
This algorithm may be better suited to large discrete state spaces than either MOP-$\alpha$ or the Baum-Welch algorithm.

Our gradient estimate can be used in variational inference for the posterior over latent states \citep{naesseth18} and parameters. 

Discounting the weights by some $\alpha \in [0,1]$ is not the only way to interpolate between the estimators of \cite{naesseth18} and \cite{poyiadjis11}. 
As the proof of Theorem \ref{thm:mop-biasvar} implies, we can also truncate the weights at a fixed lag, corresponding to the MOP-$(1,k)$ estimate mentioned. 
The analysis is similar, with comparable rates but with a slightly less convenient implementation. 


\section*{Acknowledgments}
This research was supported by National Science Foundation grants DMS-1761603 and DEB-1933497. 
We thank Nicolas Chopin for helpful communications regarding the strong law of large numbers for off-parameter resampled particle filters. We also thank Jun Chen and Arnaud Doucet for helpful discussions regarding the manuscript.
% }{
% \showmatmethods{} % Display the Materials and Methods section
% \acknow{
% This research was supported by National Science Foundation grants DMS-1761603 and DEB-1933497. 
% We thank Nicolas Chopin for helpful communications regarding the strong law of large numbers for off-parameter resampled particle filters. We also thank Jun Chen and Arnaud Doucet for helpful discussions regarding the manuscript.}
% \showacknow{} % Display the acknowledgments section
% }


%\bibsplit[2]
%Use \bibsplit to split the references from the body of the text. Value "[2]" represents the number of reference in the left column (Note: Please avoid single column figures & tables on this page.)

\bibliographystyle{abbrvnat}
% Bibliography
\bibliography{bib-ifad}


\arxiv{
\appendix

\section{MOP-$\alpha$ Functional Forms}
\label{appendix:functional}
\input{appendix/functional}


\section{Optimization Convergence Analysis}
\label{appendix:convergence}
\input{appendix/convergence}

\section{Feynman-Kac Models and Monte Carlo Approximations}
\label{appendix:feynman}
\input{appendix/feynman}

\section{A Strong Law of Large Numbers for Triangular Arrays of Particles With Off-Parameter Resampling}
\label{appendix:targeting}
\input{appendix/targeting}


\section{Consistency of Off-Parameter Resampled Gradient Estimates}
\label{appendix:consistency}
\input{appendix/consistency}

\section{Bias-Variance Analysis}
\label{appendix:biasvar}
\input{appendix/biasvar}


\section{Figures for Bayesian Inference}
\label{appendix:bayes}
\input{appendix/bayes}


}{
}


\end{document}
