\documentclass[12p]{article}
\usepackage{fullpage}

%% for algorithm pseudocode
\usepackage{enumerate,alltt,xstring}
\usepackage[ruled,noline,linesnumbered]{algorithm2e}
\SetKwFor{For}{for}{do}{end}
\SetKwFor{While}{while}{do}{end}
\SetKwInput{KwIn}{input}
\SetKwInput{KwOut}{output}
\SetKwInput{KwCplx}{complexity}
\SetKwInput{KwIndices}{note}
\SetKwBlock{Begin}{procedure}{}
\DontPrintSemicolon
\newcommand\argequals{{\,=\,}}
\newcommand\data[1]{#1^*}
\newcommand\giventh{{\hspace{0.5mm};\hspace{0.5mm}}}
\newcommand\seq[2]{{#1}\!:\!{#2}}
\newcommand\mydot{{\,\cdot\,}}
\newcommand\given{{\,\vert\,}}
\newcommand\normal{{\mathrm{Normal}}}
\newcommand\vmeasure{V}
\newcommand\emeasure{e}

\newcommand\Time{N}
\newcommand\ttime{n}
\newcommand\Np{J}
\newcommand\np{j}
\newcommand\altNp{q}
\newcommand\prob{\mathrm{Prob}}

\begin{document}

%%%%  PFILTER PSEUDOCODE
\begin{algorithm}[t!]
  \caption{\textbf{Measurement off policy (MOP) SMC}:
    \label{alg:mop-smc}
  }
  %% \KwIn{
  %%   Simulator for $f_{X_n|X_{n-1}}(x_n\given x_{n-1}\giventh\theta)$;
  %%   evaluator for $f_{Y_n|X_n}(y_n\given x_{n}\giventh\theta)$;
  %%   simulator for $f_{X_0}(x_0\giventh\theta)$;
  %%   parameter, $\theta$;
  %%   data, $y^*_{1:N}$;
  %%   number of particles, $J$.
  %% }
  %% \BlankLine
  Initialize filter particles:
  simulate ${X}_{0,j}^{F,\theta}\sim {f}_{{X}_{0}}\left(\mydot\giventh{\theta}\right)$ for $j$ in $\seq{1}{J}$\;
  Initialize relative weights: $w^{F,\theta}_{0,j}= 1$ for $j$ in $\seq{1}{J}$
  \;
  \For{$n$ in $\seq{1}{N}$}{
    Simulate for prediction:
    ${X}_{n,j}^{P,\theta}\sim {f}_{{X}_{n}|{X}_{n-1}}\big(\mydot|{X}_{n-1,j}^{F};{\theta}\big)$ for $j\ \mathrm{in}\ \seq{1}{J}$ \nllabel{alg:pfilter:step1}\;
    Evaluate measurement density:
    $g^{\theta}_{n,j}={f}_{{Y}_{n}|{X}_{n}}(y_{n}^{*}|{X}_{n,j}^{P,\theta}\giventh{\theta})$ for $j$ in $\seq{1}{J}$\;
    Update relative weights to compensate for resampling:
    $\displaystyle w^{P,\theta}_{n,j}= \frac{w^{F,\theta}_{n-1,j} \, g^{\theta}_{n,j}}{ g^{\phi}_{n,j}}$ for $j\ \mathrm{in}\ \seq{1}{J}$ \nllabel{alg:pfilter:weight:update}
    \;
    Conditional likelihood under $\phi$: 
    $L_n^{\phi} = \frac{1}{J}\sum_{m=1}^{J}g^{\phi}_{n,m}$
    \;
    Normalize weights:
    $\displaystyle \tilde{g}^{\phi}_{n,j}= \frac{g^{\phi}_{n,j}}{JL_n^{\phi}}$
    for $j\ \mathrm{in}\ \seq{1}{J}$
    \;
    Apply systematic resampling to select indices $k_{1:J}$ with $\prob\big(k_{j}=m\big) =\tilde{g}^{\phi}_{n,m}$ \nllabel{alg:pfilter:systematic}\;
    Resample:
    set ${X}_{n,j}^{F,\theta}={X}_{n,k_{j}}^{P,\theta}$ and
    $w_{n,j}^{F,\theta}= {w}_{n,k_{j}}^{P,\theta}$
    for $j$ in $\seq{1}{J}$ \nllabel{alg:pfilter:step2}
    \;
  }
  %% \KwOut{
  %%   Log likelihood estimate, $\loglikMC(\theta)=\sum_{n=1}^N\loglikMC_{n|1:n-1}$;
  %%   filter sample, $X^F_{n,1:J}$, for $n$ in $\seq{1}{N}$.
  %% }
  %% \KwCplx{$\bigO{J}$}
\end{algorithm}

\begin{itemize}
\item MOP-SMC requires that the algorithm is first run at $\phi$, for which it is a vanilla particle filter.  $g^{\phi}_{n,j}$ and $\tilde g^{\phi}_{n,j}$ are computed at this first pass. Then it is run at $\theta$, with the seed kept fixed.

\item Here, the resampling rule for particle $j$ depends on $j$ (and therefore $\phi$ and $X^{P,\phi}_{n,j}$) but not on $\theta$ or  $X^{P,\theta}_{n,j}$. This does not have the standard form for a weighted particle filter, for which we can have general reweighting rules but they are usually a funciton of the location of the particle and the model parameter, $\theta$. That may be why it is harder to see that this is a properly weighted filter. Nevertheless, inspection of the algorithm shows that each particle is properly reweighted to account for its resampling probability, in step~\ref{alg:pfilter:weight:update}, so it should be properly weighted.

\item The final estimate of the likelihood is either
  \begin{equation} \label{mop:lik1}
    L(\theta) = \left(\frac{1}{J}\sum_{j=1}^J w^{F,\theta}_{N,j} \right)
    \prod_{n=1}^N L_n^\phi
  \end{equation}
  or
 \begin{equation}
   L^\prime(\theta) = \left(\frac{1}{J}\sum_{j=1}^J w^{P,\theta}_{N,j} \, g^{\theta}_{N,j}\right)
   \prod_{n=1}^{N-1} L_n^\phi
  \end{equation}
 with $L^\prime(\theta)$ presumably having slightly lower variance.

\item Weighted samples representing the filter distribution,
  $f_{X_n|Y_{1:n}}(x_n \given y^*_{1:n} \giventh \theta)$
  are either
  $\big\{\big(X^{F,\theta}_{n,j},w^{F,\theta}_{n,j}\big),  j\ \mathrm{in}\ \seq{1}{J}\big\}$
  or
  $\big\{\big(X^{P,\theta}_{n,j}, g^\theta_{n,j} \, w^{P,\theta}_{n,j}\big),  j\ \mathrm{in}\ \seq{1}{J}\big\}$,
  meaning that an expectation over $f_{X_n|Y_{1:n}}(x_n \given y^*_{1:n} \giventh \theta)$ is consistently estimated by a corresponding weighted average of the filter or prediction particles.

  \item As long as \texttt{rprocess} is a continuously differentiable function of $\theta$ for fixed seed, and \texttt{dmeasure} is a continuously differentiable function of $\theta$, and $g^{\phi}_{n,j}\neq 0$, we see that MOP is a continuously differentiable function of $\theta$ for fixed seed. Since it also provided an unbiased estimate of the likelihood, this justifies exchanging the order of differentiation and integration to ensure that its derivative is an unbiased estimate of the deriative of the likelihood.

  \item Taking the derivative with respect to $\theta$ at $\theta=\phi$, step~\ref{alg:pfilter:weight:update} looks very much like the stop gradient approach.

  \end{itemize}

Here's an outline of an argument explaining why MOP is properly weighted.

\noindent {\bf Lemma 1}. {\it
Suppose that $\{(X_j,u_j),j=1,\dots,J\}$ is properly weighted for $f_X$, meaning that, if $X\sim f_X$,
\[
E[h(X)] \approx \sum_{j=1}^J \frac{w_j}{\sum_{k=1}^J u_k} h(X_j),
\]
for some appropriate formalization of $\approx$. Now, let $\{(Y_j,v_j),j=1,\dots,J\}$ be a sample drawn from $\{(X_j,u_j)\}$ where $(X_j,u_j)$ is represented, on average, $\pi_j J$ times. This could amount to binomial resampling having $J$ draws each with probability $p_j$, or systematic resampling. Suppose
\[
(Y_j,v_j) = \big(X_{a(j)},u_{a(j)}/\pi_{a(j)}\big),
\]
where $a$ is called the ancestor function. Then, $\{(Y_j,v_j),j=1,\dots,J\}$ is als properly weighted for $f_X$.
}

Notably, Lemma~1 permits $\pi_{1:J}$ to depend on $\{(X_j,u_j)\}$ as long as the resampling is carried out independently of $\{(X_j,u_j)\}$ conditional on $\pi_{1:J}$.

\noindent {\bf Lemma 2}. {\it
  Suppose that $Z_j \sim f_{Z|X}(\cdot | X_j)$ where $f_{Z|X}$ is a conditional probability density function corresponding to a joint density $f_{X,Z}$ with marginal densities $f_X$ and $f_Z$. Then, $\{(Z_j,u_j)\}$ is properly weighted for $f_Z$.
}

\noindent {\bf Lemma 3}. {\it
Suppose that $(X^\prime_j,u^\prime_j) = \big(X_j,u_j\, f_{Z|X}(z^*|X_j)\big)$. Then, $\{(X^\prime_j,u^\prime_j)\}$ is properly weighted for $f_{X|Z}(\cdot | z^*)$.
}

Recursively applying Lemmas~1, 2 and~3, we obtain that 
%to step~\ref{alg:pfilter:step1}, Lemma~2 step~ {alg:pfilter:weight:update} and Lemma~3 to step~\ref{alg:pfilter:step2} we obtain that
the MOP filter is properly weighted.
Specifically, suppose inductively that $\big\{\big(X^{F,\theta}_{n-1,j},w^{F,\theta}_{n-1,j}\big)\big\}$ is properly weighted for $f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}|y^*_{1:n-1};\theta)$.
Then, Lemma~2 tells us that $\big\{\big(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j}\big)\big\}$ is properly weighted for $f_{X_{n}|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)$.
Lemma~3 tells us that $\big\{\big(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j} g^\theta_{n,j} \big)\big\}$ is therefore properly weighted for  $f_{X_{n}|Y_{1:n}}(x_{n}|y^*_{1:n};\theta)$.
Lemma~1 guarantees that the resampling rule, given by 
\[
\big(X^{F,\theta}_{n,j},w^{F,\theta}_{n,j}\big) = \big(X^{P,\theta}_{n,a(j)}, w^{P,\theta}_{n,j} g^\theta_{n,j}\big/ g^\phi_{n,j}\big),
\]
with resampling weights proportional to $g^\phi_{n,j}$, is therefore also properly weighted for $f_{X_{n}|Y_{1:n}}(x_{n}|y^*_{1:n};\theta)$.

This has addressed filtering, but not quite yet the likelihood evaluation.

A properly weighted conditional likelihood estimate from MOP (give the argument above) is
\[
f_{Y_n|Y_{1:n-1}}(y_n^*|y_{1_n-1}^*;\theta) \approx \frac{\sum_{j=1}^Jg^\theta_{n,j} w^{P,\theta}_{n,j}}{\sum_{j=1}^J  w^{P,\theta}_{n,j}}.
\]
We write the numerator as
\[
L^\phi_n \sum_{j=1}^J \left[ \frac{g^\theta_{n,j}}{g^\phi_{n,j}} w^{P,\theta}_{n,j}\right] \frac{g^\phi_{n,j}}{L_n^\phi}
\]
Using Lemma~1, we see this is properly estimated by
\[
L^\phi_n \sum_{j=1}^J w^{F,\theta}_{n,j}.
\]
This corresponds to
\[
L^\theta = \prod_{n=1}^N L^\phi_n \frac{\sum_{j=1}^J w^{F,\theta}_{n,j}}{\sum_{j=1}^J w^{P,\theta}_{n,j}}.
\]
Since $w^{F,\theta}_{n,j}=w^{P,\theta}_{n+1,j}$, this is a telescoping product. The remaining terms are
$\sum_{j=1}^J w^{P,\theta}_{0,j} = J$ on the denominator and $\sum_{j=1}^J w^{F,\theta}_{N,j}$ on the numerator.
This derives the MOP estimate in (\ref{mop:lik1}).



\end{document}

