

In this section, we prove various rates on the bias, variance, and MSE of MOP-$\alpha$. 
First, we note the following relation between the Dobrushin contraction coefficient and the alpha-mixing coefficients in our context below.
\begin{lem}
    \label{lem:dobrushin-implies-alpha-mixing}
    Setting $X$ to be the particle collection at time $m$, and $Y$ at time $n$, we have that the alpha mixing coefficients,
    $$\int | f_{XY} - f_X f_Y | < \alpha,$$
    are bounded by the Dobrushin coefficient, i.e we have
    $$\int | f_{Y|X}(y|x_1) - f_{Y|X}(y|x_2) |dy < \alpha$$ for all $x_1$, $x_2$.
\end{lem}

\begin{proof}
    We rewrite the alpha-mixing assertion as 
    $$\int { \int |f_Y|X(y|x) - f_Y(y)| dy } f_X dx.$$
    We claim that the Dobrushin coefficient implies $$\int | f_Y|X(y|x_1) - f_Y(y) |dy < \alpha$$ for all $x_1$. This is shown as follows:
    \begin{align*}
        \int | f_{Y|X}(y|x) - f_Y(y) |dy 
        &= \int \left|  \int [f_{Y|X}(y|x_1) - f_{Y|X}(y|x)] f_X(x)dx \right|dy \\
        &< \int   \int  | f_{Y|X}(y|x_1) - f_{Y|X}(y|x)] | dy f_X(x)dx \\
        &< \int \alpha f_X(x)dx \\
        &=\alpha
    \end{align*}
    We then have the desired result.
\end{proof}

\subsection{Warm Up: MOP-$0$ Variance Bound}
We now prove the variance bound, presented below. 
\begin{thm}
    
\end{thm}
We made Assumptions \ref{assump:bounded-process} and \ref{assump:bounded-measurement} to leverage the results from \cite{karjalainen23} on the forgetting of the particle filter. This is required to show error bounds on the gradient estimates that we provide -- namely that the error of the MOP-$1$ estimator that corresponds to that of \cite{poyiadjis11} is $O(N^2)$, and that the variance of MOP-$\alpha$ for any $\alpha<1$ is $O(N)$. 

We can decompose the MOP-$\alpha$ estimator as follows. When $\theta=\phi$,
$$\hat{\mathcal{L}}(\theta):=\prod_{n=1}^N L_n^{A, \theta, \alpha}=\prod_{n=1}^N L_n^\phi \cdot \frac{\sum_{j=1}^J w_{n, j}^{F, \theta}}{\sum_{j=1}^J w_{n, j}^{P, \theta}}=\prod_{n=1}^N L_n^{A, \theta, \alpha}=\prod_{n=1}^N L_n^\phi \cdot \frac{\sum_{j=1}^J w_{n, j}^{F, \theta}}{\sum_{j=1}^J (w_{n-1, j}^{F, \theta})^\alpha}.$$
Now, to illustrate the proof strategy for the general case in an easier context, we first analyze the special case when $\alpha=0.$ 

\begin{proof}
When $\alpha=0$, we have:
\begin{equation}
    \hat{\lik}(\theta) := \prod_{n=1}^N L_n^{A, \theta, \alpha} = \prod_{n=1}^N L_n^\phi \cdot \frac{\sum_{j=1}^J w_{n,j}^{F,\theta}}{\sum_{j=1}^J w_{n,j}^{P,\theta}} = \prod_{n=1}^N L_n^\phi \cdot \frac{1}{J}\sum_{j=1}^J s_{n,j} = \prod_{n=1}^N L_n^\phi \cdot \frac{1}{J}\sum_{j=1}^J \frac{f_{Y_n|X_n}(y_n^*|x_{n,j}^{P, \theta})}{f_{Y_n|X_n}(y_n^*|x_{n,j}^{P, \phi})}.
\end{equation}
Similarly to the proof of Lemma \ref{lem:mop-0-formula}, its gradient when $\theta=\phi$ is therefore 
\begin{align*}
    \nabla_\theta \hat{\ell}(\theta) &:= \sum_{n=1}^N \nabla_\theta \log\left(L_n^\phi \frac{1}{J} \sum_{j=1}^J s_{n,j}\right) \\
    &= \sum_{n=1}^N \frac{\nabla_\theta \left(L_n^\phi \frac{1}{J} \sum_{j=1}^J s_{n,j}\right)}{\left(L_n^\phi \frac{1}{J} \sum_{j=1}^J s_{n,j}\right)} \\
    &= \sum_{n=1}^N \frac{\sum_{j=1}^J \nabla_\theta s_{n,j}}{\sum_{j=1}^J s_{n,j}} \\
    &= \sum_{n=1}^N \frac{1}{J} \sum_{j=1}^J \frac{\nabla_\theta f_{Y_n|X_{n}}(y_n^*|x_{n,j}^{F, \theta}; \theta)}{f_{Y_n|X_{n}}(y_n^*|x_{n,j}^{F, \phi}; \phi)} \\
    &= \frac{1}{J} \sum_{n=1}^N \sum_{j=1}^J \nabla_\theta \log\left(f_{Y_n|X_{n}}(y_n^*|x_{n,j}^{F, \theta}; \theta)\right),
\end{align*}
where we use the log-derivative trick, observe that $\sum_{j=1}^J s_{n,j} = J$ when $\theta=\phi$, and use the log-derivative trick where $\theta=\phi$ again. The point here was that we establish that the gradient of the log-likelihood estimate is given by the sum of terms over all $N$ and $J$:
$$\nabla_\theta \hat{\ell}(\theta) = \frac{1}{J} \sum_{n=1}^N \sum_{j=1}^J \nabla_\theta \log\left(f_{Y_n|X_{n}}(y_n^*|x_{n,j}^{F, \theta}; \theta)\right).$$
Therefore,
\begin{align*}
    \Var(\nabla_\theta \hat\ell(\theta)) &= \frac{1}{J^2}\Var\left(\sum_{n=1}^N\sum_{j=1}^{J}\nabla_\theta \log\left(f_{Y_n|X_{n}}(y_n^*|x_{n,j}^{F, \theta}; \theta)\right)\right) \\
    &= \frac{1}{J^2}\sum_{n=1}^N\Var\left(\sum_{j=1}^{J}\nabla_\theta \log\left(f_{Y_n|X_{n}}(y_n^*|x_{n,j}^{F, \theta}; \theta)\right)\right) \\
    &+ 2\sum_{m<n}\Cov\left(\frac{1}{J}\sum_{j=1}^{J}\nabla_\theta \log\left(f_{Y_m|X_{m}}(y_m^*|x_{m,j}^{F, \theta}; \theta)\right), \frac{1}{J}\sum_{j=1}^{J}\nabla_\theta \log\left(f_{Y_n|X_{n}}(y_n^*|x_{n,j}^{F, \theta}; \theta)\right)\right) \\
    &= \sum_{n=1}^N \Var(\nabla_\theta\hat\ell_n(\theta)) + 2\sum_{m<n} \Cov(\nabla_\theta\hat\ell_m(\theta),\nabla_\theta\hat\ell_n(\theta)).
\end{align*}
Here, we use Assumptions \ref{assump:bounded-process} and \ref{assump:bounded-measurement} that ensure strong mixing. We know from Theorem 3 of \cite{karjalainen23} that when $\textbf{M}_{n,n+k}$ is the $k$-step Markov operator from timestep $n$ and $\beta_{\text{TV}}(M) = \sup _{x, y \in E}\|M(x, \cdot)-M(y, \cdot)\|_{\mathrm{TV}}=\sup _{\mu, \nu \in \mathcal{P}, \mu \neq \nu} \frac{\|\mu M-\nu M\|_{\mathrm{TV}}}{\|\mu-\nu\|_{\mathrm{TV}}}$ is the Dobrushin contraction coefficient of a Markov operator, 
$$\beta_{\text{TV}}(\textbf{M}_{n,n+k}) \leq (1-\epsilon)^{\floor{k/(c\log(J))}},$$
i.e. the mixing time of the particle filter is $O(\log(J))$, where $\epsilon$ and $c$ depend on $\bar{M}, \underbar{M}, \bar{G}, \underbar{G}$ in \ref{assump:bounded-process} and \ref{assump:bounded-measurement}. 

%\kevin{How does the dobrushin contraction coefficient relate to the alpha-mixing coefficients? If you have a bound on the mixing time $b$, then the alpha mixing coefficients should just be $\alpha(t) \leq e^{-t/b}$.}

By Lemma \ref{lem:dobrushin-implies-alpha-mixing}, the particle filter itself is strong mixing, with $\alpha$-mixing coefficients $\alpha(k) \leq (1-\epsilon)^{\floor{k/(c\log(J))}}$. Therefore, functions of particles are strongly mixing as well, with $\alpha$-mixing coefficients bounded by the original (to see this, observe that the $\sigma$-algebra of the functionals is contained within the original $\sigma$-algebra). Therefore, by Davydov's inequality, noting that $\nabla_\theta\hat\ell_n(\theta)\leq G'(\theta)$ by Assumption \ref{assump:bounded-measurement}, and WLOG assuming $\E[(\nabla_\theta\hat\ell_m(\theta))^4]^{1/4}\leq\E[(\nabla_\theta\hat\ell_n(\theta))^4]^{1/4}$, we see that
\begin{align*}
    \Cov(\nabla_\theta\hat\ell_m(\theta), \nabla_\theta\hat\ell_n(\theta)) 
    &\leq \alpha(n-m)^{1/2}\E[(\nabla_\theta\hat\ell_m(\theta))^4]^{1/4}\E[(\nabla_\theta\hat\ell_n(\theta))^4]^{1/4}\\
    &\leq \alpha(n-m)^{1/2}\E[(\nabla_\theta\hat\ell_n(\theta))^4]^{1/2}.
\end{align*}
To bound this, we use the fact that $$\E[(\nabla_\theta\hat\ell_n(\theta))^4] = \E[(\nabla_\theta\hat\ell_n(\theta)-\E[\nabla_\theta\hat\ell_n(\theta)])^4]+\E[(\nabla_\theta\hat\ell_n(\theta))^2]^2$$
alongside Lemma 2 of \cite{karjalainen23}, which shows that
$$\E[(\nabla_\theta\hat\ell_n(\theta)-\E[\nabla_\theta\hat\ell_n(\theta)])^4] \lesssim \frac{G'(\theta)^4}{J^2}, \;\; \E[(\nabla_\theta\hat\ell_n(\theta)-\E[\nabla_\theta\hat\ell_n(\theta)])^2] \lesssim \frac{G'(\theta)^2}{J}.$$
It follows that 
$$\E[(\nabla_\theta\hat\ell_n(\theta))^4]^{1/2} \lesssim  \sqrt{\frac{G'(\theta)^4}{J^2}+\left(\frac{G'(\theta)^2}{J}\right)^2} = \frac{G'(\theta)^2}{J},$$
and we conclude that 
$$\Cov(\nabla_\theta\hat\ell_m(\theta), \nabla_\theta\hat\ell_n(\theta)) \leq (1-\epsilon)^{\frac{1}{2}\floor{\frac{|n-m|}{c\log(J)}}}\frac{G'(\theta)^2}{J}.$$
Putting it all together, we see that
\begin{align*}
    \Var(\nabla_\theta \hat\ell(\theta)) &= \sum_{n=1}^N \Var(\nabla_\theta\hat\ell_n(\theta)) + 2\sum_{m<n} \Cov(\nabla_\theta\hat\ell_m(\theta),\nabla_\theta\hat\ell_n(\theta))\\
    &\leq \frac{NG'(\theta)^2}{J} + 2\sum_{n=1}^{N-1} (N-n)(1-\epsilon)^{\frac{1}{2}\floor{\frac{n}{c\log(J)}}}\frac{G'(\theta)^2}{J} \\
    &\leq \frac{G'(\theta)^2}{J} \left(N + 2\sum_{n=1}^{N-1} (N-n) (1-\epsilon)^{\frac{1}{2}\floor{\frac{n}{c\log(J)}}}\right) \\
    &\lesssim \frac{G'(\theta)^2N}{J}.
\end{align*}
\end{proof}


\paragraph{Remark:} Note that the factor of $N$ that pops up here is due to the use of the unnormalized gradient. If one divides the gradient estimate by $\sqrt{N}$ before usage, the variance does not depend on the horizon. If one divides by $N$, the error is $O(1/\sqrt{NJ})$.