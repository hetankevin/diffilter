\documentclass[11pt]{article} 
%\documentclass[9pt,twocolumn,twoside]{pnas-new}
%\documentclass[9pt,twocolumn,pnasresearcharticle]{pnas-new}
%\documentclass[9pt,twocolumn,twoside]{pnas-new}
% Use the lineno option to display guide line numbers if required.

\usepackage{booktabs}

\newcommand\arxiv[2]{#1} % 1st argument for arxiv format
% \newcommand\arxiv[2]{#2} % 2nd argument for PNAS format
% also, swich to \documentclass[11pt]{article} for arxiv format

\input{macros}

\usepackage{tabto}

\arxiv{
  \usepackage{fullpage}
  %% some packages loaded by pnas-new.cls which we need for
  %% the arxiv version
  \usepackage{algorithm}
  \usepackage[noend]{algpseudocode}
  \usepackage{xcolor}
  \usepackage{tikz}
  \usepackage{enumitem}
  \bibliographystyle{ieeetr}
}{
\templatetype{pnasresearcharticle} % Choose template
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission
}

\begin{document}

\title{Accelerated Inference for Partially Observed Stochastic Processes using Automatic Differentiation}

%\arxiv{ \author{K. Tan, G. Hooker and E. L. Ionides\\
%  \small{Department of Statistics, University of Pennsylvania}\\
%  \small{Department of Statistics, University of Michigan}}}{}

\arxiv{ \author{K. Tan$^{1}$, G. Hooker$^{1}$ and E. L. Ionides$^{2}$
  \vspace{2mm}\\
  \small{$^{1}$Department of Statistics, University of Pennsylvania}\\
  \small{$^{2}$Department of Statistics, University of Michigan}}}{}

\arxiv{}{
% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a]{Kevin Tan}
\author[a]{Giles Hooker}
\author[b,1]{Edward L. Ionides}
%\author[a]{Author Three}
\affil[a]{University of Pennsylvania}
\affil[b]{University of Michigan}
%\affil[c]{Affiliation Three}
% Please give the surname of the lead author for the running footer
\leadauthor{Tan}
% Please add a significance statement to explain the relevance of your work
\significancestatement{Many scientific models involve highly nonlinear stochastic dynamical systems which can be observed only via noisy and incomplete measurements. Prior to this work, iterated filtering algorithms were the only class of algorithms for maximum likelihood estimation that did not require access to the system's transition probabilities, instead needing only a simulator of the system dynamics. We leverage recent advances in automatic differentiation to propose a hybrid algorithm that requires only a differentiable simulator for maximum likelihood estimation. Our new method outperforms previous approaches on a challenging problem in epidemiology.}
\authorcontributions{K.T. and E.L.I. designed research; K.T. analyzed data; K.T., G.H. and E.L.I. performed research, K.T., G.H., and E.L.I. wrote the manuscript.}
\authordeclaration{The authors declare no competing interests.}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: kevtan@wharton.upenn.edu}
% At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol.
\keywords{Sequential Monte Carlo $|$ Automatic Differentiation $|$ Particle Filter $|$ Markov Process $|$ Maximum Likelihood}
}

\arxiv{\date{Draft compiled on \today}}{
\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}
}

\maketitle

\begin{abstract}
  Automatic differentiation (AD) has driven recent advances in machine learning, including deep neural networks and Hamiltonian Markov Chain Monte Carlo methods. Partially observed nonlinear stochastic dynamical systems have proved resistant to AD techniques because widely used particle filter algorithms yield an estimated likelihood function that is discontinuous as a function of the model parameters.  We show how to embed two existing AD particle filter methods in a theoretical framework that provides an extension to a new class of algorithms.  This new class permits a bias/variance tradeoff and hence a mean squared error substantially lower than the existing algorithms. We develop likelihood maximization algorithms suited to the Monte Carlo properties of the AD gradient estimate. Our algorithms require only a differentiable simulator for the latent dynamic system; by contrast, most previous approaches to AD likelihood maximization for particle filters require access to the system's transition probabilities. Numerical results indicate that a hybrid algorithm that uses AD to refine a coarse solution from an iterated filtering algorithm show substantial improvement on current state-of-the-art methods for a challenging scientific benchmark problem.
\end{abstract}

\arxiv{}{
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}
\firstpage{4}
% Use \firstpage to indicate which paragraph and line will start the second page and subsequent formatting. In this example, there are a total of 11 paragraphs on the first page, counting the first level heading as a paragraph. The value {12} represents the number of the paragraph starting the second page. If a paragraph runs over onto the second page, include a bracket with the paragraph line number starting the second page, followed by the paragraph number in curly brackets, e.g. "\firstpage[4]{11}".
}

\arxiv{Many}{\dropcap{M}any}
scientific models involve highly nonlinear stochastic dynamic systems possessing significant random variation in both the process dynamics and the measurements.
Commonly, the latent system is modeled as a Markov process, giving rise to a partially observed Markov process (POMP) model, also known as a hidden Markov models or a state space model.
POMP models arise in fields as diverse as automated control \cite{singh22}, epidemiology \cite{he10, stocks17}, ecology \cite{knape12} and finance \cite{kim08, breto14}.
Despite their ubiquity, estimation and inference within this broad class of models remains a challenging problem.
This article concerns the use automatic differentiation (AD) to construct improved algorithms for inference on complex POMP models.

The particle filter, also known as sequential Monte Carlo, serves as the foundation for various inference algorithms for POMP models.
It provides an unbiased estimate of the likelihood function  \cite{delMoral04}, enabling Bayesian inference \cite{andrieu10,chopin13} and likelihood-based inference \cite{ionides06-pnas,ionides15}.
Likelihood-based inference is statistically efficient \cite{pawitan01} and is robust to a moderate amount of Monte Carlo error \cite{ionides17,ning21}.
An attractive feature of basic particle filter algorithms, also known as boostrap filters, is that they do not require evaluation of the transition density of the latent Markov process, enabling an arbitrary model simulator to be plugged into the algorithm.
This plug-and-play property is useful for scientific applications \cite{he10}.
Some plug-and-play methods depend on the construction of low-dimensional summary statistics \cite{wood10,toni09}, sacrificing statistical efficiency for computational convenience.
Plug-and-play methods are often called likelihood-free \cite{owen15}, and it may be counter-intuitive that likelihood-based inference is possible using likelihood-free methods. 
However, iterated filtering algorithms have shown that this is practical in various scientific investigations \cite{king08,blake14,pons-salort18,subramanian21,fox22,drake23}.
Nevertheless, the Monte Carlo variability arising in iterated filtering applications becomes increasingly problematic as the size of the data and the complexity of the model increases.
Algorithmic advances are needed to make plug-and-play likelihood maximization for POMP models more numerically efficient.
We investigate the use of automatic differentiation (AD) for this purpose.

%\subsection{Automatic Differentiation for Particle Filters}

Recent advances in AD for particle filters \cite{naesseth18, jonschkowski18, corenflos21, scibior21, singh22} have drawn attention to AD as a tool for inference in partially observed stochastic processes.
However, existing approaches are either asymptotically biased \cite{naesseth18, jonschkowski18}, have high variance \cite{poyiadjis11, scibior21}, high computational expense \cite{corenflos21, chen24}, or need access to transition densities \cite{poyiadjis11, scibior21, singh22, chen24}.

Scibior and Wood \cite{scibior21} showed that the estimators derived by Poyiadjis et al. \cite{poyiadjis11} can be attained with standard AD software, using an algorithmic procedure called a ``stop-gradient'' which allows selected exressions to be evaluated but not differentiated.
Unfortunately, \cite{scibior21} used their estimator within an algorithm which does not have the plug-and-play property and which has high Monte Carlo variability.
We use \cite{scibior21} and \cite{poyiadjis11} as our starting point, while developing a new approach which remedies the weaknesses of these papers.

We start by presenting a new construction of the gradient estimator of \cite{scibior21} and \cite{poyiadjis11}.
Specifically, we show how this estimator can be derived as the direct derivative of a suitably weighted particle filter, which (for reasons which will be discussed later) we call a Measurement Off-Parameter (MOP) particle filter.
We proceed to generalize the MOP particle filter by adding a discounting factor, $\alpha \in [0,1]$, which interpolates between the  biased gradient estimator of \cite{naesseth18} when $\alpha=0$, and the high-variance gradient estimate from \cite{poyiadjis11, scibior21} when $\alpha=1$.
The bias-variance tradeoff induced by $\alpha$ suggests, both in theory and in practice, the use of $\alpha$ values strictly between 0 and 1.

MOP-$\alpha$ avoids the discontinuity that is problematic for the standard particle filter.
This makes MOP-$\alpha$ convenient for the application of AD.
Yet, MOP-$\alpha$  can nevertheless provide a consistent log-likelihood estimate.

We derive a linear convergence rate for stochastic gradient descent (SGD) using MOP-$\alpha$ in the  presence of strong convexity.
Critically, the estimator is effective within a neighborhood of the maximum but is relatively slow at reaching this neighborhood.
That behavior is complementary to that of the iterated filtering algorithm \cite{ionides06-pnas,ionides15} which provides a relatively fast and stable way to identify this neighborhood.
We therefore build a hybrid algorithm called {\it Iterated Filtering with Automatic Differentiation} (IFAD) that warm-starts first-order or second-order gradient methods with a preliminary solution obtained from a few rounds of iterated filtering.
Promising numerical results indicate that IFAD beats IF2 (and by the numerical results of \cite{ionides15}, also IF1 and the Liu-West filter \cite{liuwest01}) on a challenging problem in epidemiology, the Dhaka cholera model of \cite{king08}.

These improvements also extend to Bayesian inference, as we show in the supplement that we can use the MOP-$\alpha$ gradient estimates within a No-U-Turn Sampler (NUTS) \cite{homan14} in conjunction with a nonparametric empirical Bayes-style prior estimated with IF2 to reduce the burn-in period of particle MCMC \cite{andrieu10} from the $1.4 \times 10^6$ iterations in \cite{fasiolo16} to just $500$. 

\section{Problem Setup}

Consider an unobserved Markov process $\{X(t),t  \geq t_0\}$, with discrete-time observations $Y_1,...,Y_N$ realized at values $y_1^*,...,y_N^*$ at times $t_1,..., t_N$.
The process is parameterized by an unknown parameter $\theta \in \Theta \subseteq \R^p$, where the state $X(t)$ take values in the state space $\gX \subseteq \R^d$, the observations $Y_n$ take values in $\gY,$ and we write $X_n := X(t_n)$. 

% By a similar decomposition to that in \cite{doucet2009tutorial}, we find that the joint density of $X_{0:N}, Y_{1:N}$ can be factored as
% \begin{align*}
%     &f_{X_{0: N}, Y_{1: N}}\left(x_{0: N}, y_{1: N} ; \theta\right)\\
%     &=f_{X_0}\left(x_0 ; \theta\right) \prod_{n=1}^N f_{X_n \mid X_{n-1}}\left(x_n \mid x_{n-1} ; \theta\right) f_{Y_n \mid X_n}\left(y_n \mid x_n ; \theta\right).
% \end{align*}

We suppose that the discete-time latent process model has a density $f_{X_n|X_{n-1}}\left(x_{n} \mid x_{n-1}; \theta\right)$ and we write $\process_n\left(x_n\mid x_{n-1}; \theta\right)$ for its corresponding simulator.
We set $f_{Y_n|X_n}\left(y_n \mid x_n; \theta\right)$ to be the measurement density, and $y_n^*$ for the actual values of the observations that were observed.
We call $f_{X_{1:n}|Y_{1:n}}(x_{1:n}|y_{1:n}^*; \theta)$ the posterior distribution of states, and $f_{X_{n}|Y_{1:n}}(x_n|y_{1:n}^*; \theta)$ the filtering distribution at time $t_n$.
Superscripts $x_{n,j}^A$ denote the ancestral trajectory of particle $j$ at time $n$, and $a(\cdot)$ is the ancestor function that maps a particle to its parent, $j \mapsto a(j)$.

The above densities are defined on a probability space $(\Omega, \Sigma, \prob)$ which is also assumed to enable construction of independent replicates and all other random variables defined in our algorithms.
For the algorithmic interpretation of our theory, we identify the random number seed with an element of the sample space $\omega \in \Omega$. The random seed determines the sequence of pseudo-random numbers as generated by a computer, just as the outcome $\omega\in\Omega$ generates the sequence of random variables.

%When two algorithms share the same seed, this corresponds to the method of common random numbers. 


\section{Off-Parameter Particle Filters}

Directly differentiating through a particle filter, for a fixed value of $\omega$, leads to a biased gradient estimator due to the discreteness of the resampling operation \cite{corenflos21}.
We avoid this issue by constructing a particle filtering algorithm called {\it Measurement Off-Parameter} with discount factor $\alpha$ (MOP-$\alpha$), defined by the pseudocode in Algorithm~\ref{alg:mop}.
For MOP-$\alpha$, the resampling indices are invariant to $\theta$ when $\omega$ and $\phi$ are fixed.
We assume that the measurement model is differentiable in $\theta$.
Further, we assume that the simulator is a differentiable function of $\theta$ for every fixed $\omega$, a condition that requires the latent process to be a continuous random variable.
\ed{WE COULD CALL THIS THE REPARAMETERIZATION TRICK, BUT HERE FOR AD IT ARISES THROUGH THE STRUCTURE OF THE CODE SO WE DO NOT HAVE TO EXPLICITLY THINK OF THE EXISTENCE OF A REPARAMETERIZATION.}
Under these assumptions, direct differentiation of MOP-$\alpha$ is available via AD.
As we explain subsequently, MOP-$\alpha$ is constructed so this direct derivative obtains the score estimator of  \cite{poyiadjis11, scibior21} when $\alpha=1$ and that of \cite{naesseth18} when $\alpha=0$.
Setting $\alpha<1$ adds bias but reduces variance, raising opportunities for a favorable tradeoff.

\begin{algorithm}[H]
	\caption{MOP-$\alpha$}
    \label{alg:mop}
	     \textbf{Input:} Number of particles $J$, timesteps $N$, measurement model $f_{Y_n|X_n}(y_n^*|x_n, \theta)$, simulator $\process_n(x_{n+1}|x_n; \theta)$, evaluation parameter $\theta$, behavior parameter $\phi$, seed $\omega$.
      
        \textbf{First pass:} Set $\theta=\phi$ and fix $\omega$, yielding $X_{n,j}^{P,\phi}$, $X_{n,j}^{F,\phi}$, $g^{\phi}_{n,j}$.
            
        \textbf{Second pass:}
        Fix $\omega$, and filter at $\theta\neq \phi$:
            
		\textbf{Initialize } particles ${X}_{0,j}^{F,\theta}\sim {f}_{{X}_{0}}\left(\cdot\giventh{\theta}\right)$, weights $w^{F,\theta}_{0,j}= 1$. \newline
		\textbf{For} $n=1,...,N$: \newline
            \hspace*{4mm} Accumulate discounted weights, $w_{n,j}^{P,\theta} = \big(w_{n-1,j}^{F,\theta}\big)^\alpha$.\newline
            \hspace*{4mm} Simulate process model,
            ${X}_{n,j}^{P,\theta}\sim \process_n\big(\cdot|{X}_{n-1,j}^{F, \theta};{\theta}\big)$. \newline
            \hspace*{4mm} Measurement density,
            $g^{\theta}_{n,j}={f}_{{Y}_{n}|{X}_{n}}(y_{n}^{*}|{X}_{n,j}^{P,\theta}\giventh{\theta})$. \newline
            \hspace*{4mm} Compute $L_n^{B,\theta,\alpha} ={\sum_{j=1}^Jg^\theta_{n,j} w^{P,\theta}_{n,j}}/{\sum_{j=1}^J  w^{P,\theta}_{n,j}}$. \newline
            \hspace*{4mm} Conditional likelihood under $\phi$,
            $L_n^{\phi} = \frac{1}{J}\sum_{m=1}^{J}g^{\phi}_{n,m}$.\newline
            \hspace*{4mm} Select resampling indices $k_{1:J}$ with $\prob\big(k_{j}=m\big) \propto g^{\phi}_{n,m}$. \newline
            \hspace*{4mm} Obtain resampled particles ${X}_{n,j}^{F,\theta}={X}_{n,k_{j}}^{P,\theta}$. \newline
            \hspace*{4mm} Calculate resampled corrected weights
            $w_{n,j}^{F,\theta}= w^{P,\theta}_{n,k_j}  { g^{\theta}_{n,k_j}}/{ g^{\phi}_{n,k_j}}$.\newline
            \hspace*{4mm} Compute $ L_n^{A,\theta,\alpha} = L_n^\phi\cdot {\sum_{j=1}^J w^{F,\theta}_{n,j}}/{\sum_{j=1}^J  w^{P,\theta}_{n,j}}$.\newline
		\textbf{Return:} likelihood estimate $\hat{\lik}(\theta) = \prod_{n=1}^N L_n^{A,\theta,\alpha}$ or $\hat{\lik}(\theta) = \prod_{n=1}^N L_n^{B,\theta,\alpha}$, filtering distributions $\{(X_{n,j}^{F, \theta}, w^{F,\theta}_{n,j})\}_{n,j=1}^{N,J}.$
\end{algorithm}


\subsection{Algorithm Outline} 
We obtain two coupled sets of particles, one under $\phi \in \Theta$, and another with the process model at $\theta \in \Theta$ but with the resampling indices constructed from the first pass, with a ``baseline parameter'' $\phi$.
As the process model follows $\theta$ but the measurement model follows the baseline parameter $\phi$, we therefore call this a ``measurement off-parameter'' particle filter.
The resampling indices, which we write as $k_j \sim \text{Categorical}(g^{\phi}_{n,1},...,g^{\phi}_{n,J})$, are a function of $\phi$ for any value of $\theta$.
For the categorical distribution, we use systematic resamplig \cite{arulampalam02,king16} which usually has superior performance to multinomial resampling.
If $\theta$ and $\phi$ coincide, one only needs one particle filter run at $\theta=\phi$, otherwise one needs two runs at the same seed $\omega \in \Omega$.

We then reweight the conditional likelihoods by a correction factor accumulated over time to account for the resampling under $\phi$. That is, writing $g^{\theta}_{n,j}={f}_{{Y}_{n}|{X}_{n}}(y_{n}^{*}|{X}_{n,j}^{P,\theta}\giventh{\theta})$ for the measurement density and $L_n^{\phi} = \frac{1}{J}\sum_{m=1}^{J}g^{\phi}_{n,m}$ for the conditional likelihood estimate under $\phi$,  we can estimate the conditional likelihood under $\theta$ by
\vspace*{-1.5mm}
\begin{equation}
     \label{eq:mop-conditional-likelihood}
     L_n^{B,\theta,\alpha} = \frac{\sum_{j=1}^Jg^\theta_{n,j} w^{P,\theta}_{n,j}}{\sum_{j=1}^J  w^{P,\theta}_{n,j}} \text{ or } L_n^{A,\theta,\alpha} = L_n^\phi\cdot \frac{\sum_{j=1}^J w^{F,\theta}_{n,j}}{\sum_{j=1}^J  w^{P,\theta}_{n,j}},
     \vspace*{-1.5mm}
\end{equation}
where the weights for each particle are updated by
\vspace*{-1.5mm}
\begin{equation}
    \label{eq:weighting-scheme}
    w_{n,j}^{P,\theta} = (w_{n-1,j}^{F,\theta})^\alpha,  w^{F,\theta}_{n,j} = w^{P,\theta}_{n,k_j} { g^{\theta}_{n,k_j}}/{ g^{\phi}_{n,k_j}}, w^{F,\theta}_{0,j}= 1.
    \vspace*{-1.5mm}
\end{equation}

The before-resampling conditional likelihood estimate $L_n^{B,\theta,\alpha}$ should be used in general, as it has slightly lower variance than the after-resampling estimate $L_n^{A,\theta,\alpha}$, but the latter is useful in deriving properties of the MOP-$\alpha$ gradient estimate such as that in Theorem \ref{thm:mop-functional-forms}.

This yields a suitably reweighted particle filter that targets the posterior under $\theta$ and produces a strongly consistent likelihood estimate for the likelihood at $\theta$. Taking the gradient with respect to $\theta$ of the estimate for the log-likelihood at $\theta$ then yields an estimate for the score at $\theta$. 




\subsection{Discounting Weights for a Bias-Variance Tradeoff}

The correction factors, $w^{P,\theta}_{n,j}$ and $w^{F,\theta}_{n,j}$, accumulate as $n$ increases, leading to numerical instability.
One can discount the correction factors from previous timesteps by some $\alpha \in [0,1]$ as in Equation \ref{eq:weighting-scheme} while still targeting the posterior and likelihood if $\theta=\phi$.
This lets us optimize a bias-variance tradeoff for the MOP-$\alpha$ score estimate.
When $\alpha=1$, MOP-$1$ maintains the memory of each particle's ancestral trajectory, and Theorem~\ref{thm:mop-functional-forms} shows that it recovers the consistent but high-variance gradient estimate from \cite{poyiadjis11, scibior21}.
When $\alpha=0$, MOP-$0$ considers only single-step transition dynamics, recovering the low-variance but asymptotically biased gradient estimator of \cite{naesseth18}. 


\subsection{MOP-$\alpha$ Encompasses the Estimators of \cite{poyiadjis11, scibior21, naesseth18}}

\cite{scibior21} show that the estimate of \cite{naesseth18} is the gradient of a vanilla particle filter when resampling terms are dropped, and also recover the estimate of \cite{poyiadjis11} through applying a ``stop-gradient trick" to the particle filter. It turns out that both of these, when applied on the bootstrap filter, correspond to special cases of MOP-$\alpha$.


$\alpha$ therefore controls an exponentially-weighted moving average, determining how much memory the resulting gradient estimate has over ancestral trajectories.
This is illustrated in Figure \ref{fig:biasvar}, in the context of the Dhaka cholera model of \cite{king08}. 

\begin{thm}[MOP-$0$ and MOP-$1$ Functional Forms]
    \label{thm:mop-functional-forms}
    Writing $\nabla_\theta \hat\ell^\alpha(\theta)$ for the gradient estimate yielded by MOP-$\alpha$ when $\theta=\phi$ and using the after-resampling conditional likelihood estimate so that $\hat\lik(\theta) = \prod_{n=1}^N L_n^{A, \theta, \alpha}$, when $\alpha=0$,
    \vspace*{-2.5mm}
    \begin{equation}
        \nabla_\theta \hat\ell^0(\theta) 
        = \frac{1}{J} \sum_{n=1}^N \sum_{j=1}^J \nabla_\theta \log\left(f_{Y_n|X_{n}}(y_n^*|x_{n,j}^{F, \theta}; \theta)\right),
        \vspace*{-2.5mm}
    \end{equation}
    yielding \cite{naesseth18} on the bootstrap filter. When $\alpha=1$,
    \vspace*{-2.5mm}
    \begin{equation}
        \nabla_\theta \hat{\ell}^1(\theta) 
        = \frac{1}{J}\sum_{j=1}^J \nabla_\theta \log f_{Y_{1:N}|X_{1:N}}\left(y_{1:N}^* | x_{1:n,j}^{A, F,\theta}\right),
    \vspace*{-2.5mm}
    \end{equation}
    yielding the estimator of \cite{poyiadjis11, scibior21} with the bootstrap filter.
\end{thm}

We defer the proof to the supplementary material. It relies on a useful decomposition of the after-resampling conditional likelihood estimate $L_n^{A,\theta,\alpha}$ that yields a telescoping product in the MOP-$1$ case. Repeated applications of the log-derivative identity that $\nabla_x \log(f(x)) = (\nabla_x f(x))/f(x)$, and noting $\theta=\phi$ implies that $w_{n,j}^{P,\theta}$ evaluates to $1$, yield the result. 

This further illustrates how $\alpha$ dictates the memory of the gradient estimate. As \cite{scibior21} remarks, the MOP-$0$ estimator of \cite{naesseth18} only considers single-step quantities, and is ``memoryless'' beyond a single step. This is in contrast to the case when $\alpha=1,$ as that estimate, studied by \cite{poyiadjis11}, only considers the surviving particles at time $N$ and so fully tracks dependencies over time. 


\begin{figure}[ht!]
  \centering
    \includegraphics[width=\arxiv{10cm}{\textwidth/3}]{../imgs/095/biasvar.png}
    \caption{Illustration of the bias-variance tradeoff induced by the discounting hyperparameter $\alpha$, on the Dhaka cholera model of \cite{king08}. We display the MSE of score estimates for the trend in transmission, evaluated at the MLE.}
    \label{fig:biasvar}
\end{figure}

\subsection{Summary of Theoretical Guarantees}

The construction of MOP-$\alpha$ bypasses the issue of differentiating through a Monte Carlo algorithm with discontinuous resampling by turning it into a problem of differentiating through a simulator and a series of measurement density ratios.
We defer the theoretical analysis of MOP-$\alpha$ to Section \ref{sec:thms}, but first we highlight a few key results.
MOP-$\alpha$ estimates the likelihood and conditional distributions of latent variables, as one expects of a particle filter (Theorem \ref{thm:mop-targeting}).
When differentiated, we obtain the estimators of \cite{poyiadjis11, scibior21, naesseth18} as special cases (Theorem \ref{thm:mop-functional-forms}).
In particular, MOP-$1$ is consistent for the score (Theorem \ref{thm:mop-grad-consistency}), has rates for its bias and variance under different choices of $\alpha$ (Theorem \ref{thm:mop-biasvar}), and enjoys a linear rate of convergence for gradient descent with the resulting gradient estimate (Theorem \ref{thm:mop-convergence}).  



\section{Practical Maximum-Likelihood Estimation}

%One can think of MOP-$\alpha$ as taking a likelihood estimate at $\phi$, reweighting it to instead target the likelihood at $\theta$, and then taking the derivative with respect to $\theta$ of the above estimate. 

If $\theta$ is evaluated at $\phi$, the particles at $\theta$ and $\phi$ coincide. One then only needs to run one particle filter at $\theta=\phi$, setting the particles at $\phi$ to be copies of the particles at $\theta$ where gradients don't propagate. This is done algorithmically through the \texttt{stop\_gradient()} function in \texttt{JAX}, providing a mathematical justification for the ``stop-gradient trick" of \cite{scibior21}.

\subsection{Optimization}

This still leaves us with the question of designing an effective procedure for likelihood maximization with MOP-$\alpha$. We propose a simple algorithm we call Iterated Filtering with Automatic Differentiation (IFAD) in Algorithm \ref{alg:ifad} that runs a few iterations of IF2 to warm-start an iterative (first or second-order) method that uses the MOP-$\alpha$ gradient estimate. This leverages IF2's quick empirical convergence to a neighborhood of the MLE, overcoming the tendency of gradient methods to get stuck in saddle points and local minima. Conversely, switching to gradient ascent with MOP-$\alpha$ score estimates lets one bypass the difficulty that IF2 has with optimizing the last few units of log-likelihood. Combining these two methods in this way lets us enjoy the best of both worlds.

The  convergence of IF2 (and so the first stage of IFAD) to a neighborhood of the MLE happens fairly quickly in practice. In the case of the Dhaka cholera model of \cite{king08}, when an aggressive geometric cooling multiplier of 0.95 and initial random walk standard deviation of 0.02 is used, initial convergence happens within 40 iterations. Finding the MLE itself with IF2, however, takes much longer, as one has to use a less aggressive cooling rate to do so. For example, \cite{ionides15} use 100 iterations with the Dhaka cholera model of \cite{king08}, while \cite{wheeler23} use 200 with Model 1 in \cite{lee20}. By simply requiring that the first stage of IFAD get to a neighborhood of the MLE and not the MLE itself, we are able to substantially reduce the number of IF2 iterations required. 

\subsection{Linear Convergence Rates}

On the other hand, the second stage of IFAD enjoys a linear convergence rate under the usual strongly-convex and smoothness assumptions in convex optimization, as we show below in Theorem \ref{thm:mop-convergence}.

\begin{thm}[Linear Convergence of IFAD]
    
Consider the second stage of IFAD (Algorithm \ref{alg:ifad}) where one stops if $||\nabla_\theta \hat\ell^\alpha(\theta_m)|| \leq (1+\sigma) \epsilon$, where $\sigma \geq \frac{4 \Gamma}{(1-\beta)}$, for some $\beta \in (0,1)$. Assume $-\ell$ is strongly convex and smooth, $\gamma I \preceq \nabla_\theta^2 (-\ell) \preceq \Gamma I$. Choose the learning rate $\eta$ such that $\eta \leq \frac{c(1-\beta)}{2\Gamma}$. Then, for sufficiently large $\alpha$ and $J$ to ensure the score estimate is a $1/2$-approximation and the minimum eigenvalue of $H$ is greater than some $c > 0$ for all $m$ with probability at least $1-\delta$, the second stage of IFAD converges linearly to the MLE:
\vspace*{-2mm}
$$
\ell(\theta^*) - \ell(\theta_{m+1}) \leq \left(1-\eta\beta\frac{8\gamma}{9c}\right)(\ell(\theta^*)-\ell(\theta_m)).
$$
    \label{thm:mop-convergence}
\end{thm}
\vspace*{-5mm}

We defer the proof, which is similar to that of Theorem 6 in \cite{mahoney16}, to the supplementary information. For simplicity, we only prove this for MOP-$1$ and note that the general case follows as the required $1/2$-approximation can still be obtained for sufficiently large $\alpha$. 

We therefore see that the second stage of IFAD converges linearly to the MLE if (1) the log-likelihood surface is $\gamma$-strongly convex in a neighborhood of the MLE and (2) the first stage of IFAD successfully reaches a (high-probability) basin of attraction of the MLE. This happens fairly often in practice, for example, when sufficient regularity conditions for local asymptotic normality of the MLE hold. We conjecture that this applies to the entirety of IFAD, as IF2 converges very quickly to a neighborhood of the MLE, and will explore this in future work.

%IF2 posesses some SGD-like qualities, including what looks like linear convergence to a ball around the MLE with width determined by the random walk standard deviation. We will explore this conjecture in future work.

%While this theoretical guarantee currently only holds for IFAD-1, as it forms a particle approximation of the score as in \cite{poyiadjis11}, one can also extend this for a similar guarantee if the bias for the gradient estimate given by MOP-$\alpha$ for $\alpha < 1$ is small enough. For general $\alpha$, including MOP-0, the analysis then becomes more complicated, as one needs to handle the case of biased gradient descent. However, if future measurements $y_{n+1:N}^*$ do not provide a large amount of information on the identification of the current state $x_n$ given past and current measurements $y_{0:n}^*$, as mentioned in \cite{corenflos21}, the bias may be small enough for it to not present too much of a problem.

%The warm start allows gradient methods to shine, as regularity conditions ensure that the likelihood surface is well-behaved enough in this neighborhood that the aforementioned issues with saddle points and local minima are alleviated. 

\begin{algorithm}[H]
	\caption{IFAD}
    \label{alg:ifad}
	    \textbf{Input:} Number of particles $J$, timesteps $N$, IF2 cooling schedule $\eta_m$, MOP-$\alpha$ discounting parameter $\alpha$, $\theta_0$, $m=0.$\newline
        Run IF2 until initial "convergence" under cooling schedule $\eta_m$, or for a fixed number or iterations, to obtain $\{\Theta_j, j=1,...,J\}$, set $\theta_m := \frac{1}{J}\sum_{j=1}^J \Theta_j.$\newline
		\textbf{While} procedure not converged: \newline
		\hspace*{4mm} Run Algorithm \ref{alg:mop} to obtain $\hat\loglik(\theta_m).$ \newline
		\hspace*{4mm} Obtain $g(\theta_m) = \nabla_{\theta_m} (-\hat\loglik(\theta_m))$, $H(\theta_m)$ s.t. $\lambda_{\min}(H(\theta_m)) \geq c$. \newline
		\hspace*{4mm} Update $\theta_{m+1} := \theta_m - \eta (H(\theta_m))^{-1} g(\theta_m)$, $m:=m+1.$ \newline
		\textbf{Return} $\hat{\theta} := \theta_m.$
\end{algorithm}


\section{Application to a Cholera Transmission Model}

The cholera transmission model that \cite{king08} developed for Dhaka, Bangladesh, has been used to benchmark the performance of various POMP inference methods \cite{ionides15, wood16, wycoff24}, and we employ it here for the same purpose.
This model categorizes individuals in a population as susceptible, $S(t)$, infected, $I(t)$, and recovered, $R(t)$ and so is called an SIR compartmental model.
In this case, the compartment $R(t)$ is further subdivided into 
%. where the population at time $t$, $H_t$, is divided into the susceptible compartment $S_t$, infected compartment $I_t$, and
three recovered compartments $R^1(t)$, $R^2(t)$, $R^3(t)$ denoting varying degrees of cholera immunity.
We write $P(t)$ for the total population, and $M_n$ for the cholera deaths in each month.
As in \cite{king08, ionides15}, the transition dynamics follow a series of stochastic differential equations:
\vspace*{-1mm}
\begin{align*}
    dS&=(k \epsilon R^k+\delta(S-P)-\lambda(t) S)\, dt+d P-({\sigma S I}/{H})\, dB, \\
    dI&=\left(\lambda(t) S-(m+\delta+\gamma) I\right)\, dt+({\sigma S I}/{H})\, dB, \\
    dR^1&=(\gamma I-(k \epsilon+\delta) R^1)\, dt, \hspace{2mm} \dots \\
    dR^k&=(k \epsilon R^{k-1}\hspace{-3mm}-(k \epsilon+\delta) R^k)\, dt,
    \vspace*{-2mm}
\end{align*}
with Brownian motion $B(t)$, cholera death rate $m$, recovery rate $\gamma$, mean immunity duration $1/\epsilon$, standard deviation of the force of infection $\sigma$, and population death rate $\delta=0.02$. The force of infection, $\lambda_t$, is modeled by splines $(s_j)_{j=1}^6$
\vspace*{-2mm}
\begin{equation*}
    \lambda_t=\exp\hspace{-1mm}\left(\hspace{-.5mm}\beta_{\text{trend}}(t-t_0)+\hspace{-1mm}\sum_{j=1}^{6} \beta_j s_j(t)\hspace{-1mm}\right)\hspace{-1mm}\frac{I}{P} + \exp\hspace{-1mm} \left(\sum_{j=1}^{6} \omega_j s_j(t)\hspace{-1mm}\right)\hspace{-1mm},
    \vspace*{-2mm}
\end{equation*}
where the coefficients $(\beta_j)_{j=1}^6$ model seasonality in the force of infection, $\beta_{\text{trend}}$ models the trend in the force of infection, and the $\omega_j$ represent seasonality of a non-human environmental reservoir of disease.
The measurement model for observed monthly cholera deaths is given by 
    $Y_n \sim \gN(M_n, \tau^2M_n^2)$,
where $M_n=\gamma\int_{t_{n-1}}^{t_n}I(s)\, ds$ is the modeled number of cholera deaths in that month.


\begin{figure}
    \centering
    \includegraphics[width=\arxiv{12cm}{\textwidth/2}]{../imgs/095/tikzcholera.png}
    \vspace*{-7mm}
    \caption{Illustration of Dhaka cholera model from \cite{king08}. \ed{CHANGE H TO P? ALSO, THERE ARE ADVANTAGES TO PUTTING THE TIKZ SOURCE CODE INTO THE MS, TO ALLOW FOR FINE-TUNING EDITS.}}
    \label{fig:tikz-cholera}
\end{figure}

\subsection{Results}

We tested IFAD against IF2 on a global search problem for the Dhaka cholera model.
We re-implemented IF2 in order to do so, but we compare our results with the results of \cite{ionides15} (labeled "IF2 2015").
Our re-implementation ourperforms that of \cite{ionides15}, likely due to a better choice of algorithmic parameters.
For each method, we performed 100 searches, initialized with 100 initial starting parameter vectors drawn uniformly from the same wide bounding box used in \cite{ionides15}. We summarize our findings below. 

% \kevin{Dynamically save to a tex file and load from there}

\ed{PERHAPS THE 2ND D.P. OF SIGNIFICANCE CAN BE AVOIDED? ALSO, IF WE'RE GOING TO ADVISE MOP-ALPHA FOR $0<\alpha<1$, THAT COULD BE SHOWN HERE.?}
  
\begin{table}[h!]
\centering
\input{../imgs/095/table}
\caption{Maximum log-likelihood found by IF2, IFAD, and MOP alone. IFAD performs the best among all methods. Our implementation of IF2 outperforms that of \cite{ionides15}, but still ultimately underperforms IFAD. IFAD manages to find the MLE, matching the highest log-likelihood previously found in the \texttt{dacca()} model implemented within the \texttt{pomp} package of \cite{king16}.}
\label{table:mle}
\end{table}

\paragraph{IFAD Successfully Finds the MLE:} Previously, an MLE at a log-likelihood of $-3748.5$ was reported by \cite{king16}.
This MLE was obtained with much computational effort, using many global and local IF2 searches, and with the assistance of likelihood profiling.
Meanwhile, \cite{ionides15} only achieve a maximum log-likelihood of $-3768.6$, while the best log-likelihood found by \cite{king08} was only $-3793.4$. Despite being initialized for a global search, IFAD manages to get much closer to the MLE over the 100 searches than \cite{ionides15} and finds it up to Monte Carlo error, as seen in Table \ref{table:mle}. On this problem, the sequence of local searches and refinement that was previously required for finding the MLE is not necessary with the IFAD algorithm. 

\paragraph{IFAD Outperforms Both IF2 and MOP Alone:} While IF2 quickly approaches a neighborhood of the MLE within only 40 iterations, performing IF2 alone ultimately fails to achieve the last few log-likelihood units, as no IF2 search comes within 7 log-likelihood units of the MLE (as seen in Figure \ref{fig:scatter}). Conversely, when we tried gradient descent with MOP alone, we encountered many failed searches. This is a difficult, nonconvex, and noisy problem, \ed{SAY HOW MANY PARAMETERS ARE ESTIMATED HERE} and the search gets stuck in local minima and saddle points, failing to approach the MLE. 

IFAD, in comparison, approaches the MLE quickly due to the IF2 warm-start (as seen in Figure \ref{fig:optim}) and also succeeds at refining the coarse solution found by the warm-start with MOP gradient steps to find the MLE (as seen in Figures~\ref{fig:scatter} and \ref{fig:boxplot}). IFAD therefore successfully combines the best qualities of IFAD and MOP, outperforming either of them alone.
Monte~Carlo replication is appropriate for IFAD, as for any Monte Carlo algorithm used to solve a challenging numerical problem, but Figure~\ref{fig:boxplot} shows that IFAD can find higher likelihood values, more quickly and more reliably than the previous state-of-the-art.
\ed{I ADDED THIS PARAGRAPH TO TRY TO BE EXPLICIT ABOUT WHY WE ARE EXCITED BY THIS FIGURE. AT FACE VALUE, IT MAY NOT SEEM LIKE A BIG DEAL. IT WOULD BE BETTER TO EXPLAIN MORE CLEARLY WHY THIS IS EVIDENCE THAT THE METHOD COULD HAVE A TRANSFORMATIVE EFFECT ON DATA ANALYSIS VIA POMP MODELS.}

We emphasize that we chose to not include the use of many common heuristics used in the machine learning and optimization literature in the gradient descent stage. We used constant learning rates of $0.01, 0.05$, and $0.2$ for IFAD-$1,0,$ and $0.97$ respectively, and a constant cooling rate of $0.95$ for our IF2 implementation. While techniques such as annealing learning rates, gradient normalization, and momentum made the outperformance of IFAD even more apparent in other simulations we performed, we chose to report the results of the simplest implementation in part to serve as a baseline for the method's performance, and in part because the results of the simplest implementation were already very promising. 

\begin{figure}[htbp!]
    \includegraphics[width=\arxiv{8cm}{\textwidth/\real{4.2}}]{../imgs/095/pairs.png}
    \includegraphics[width=\arxiv{8cm}{\textwidth/\real{4.2}}]{../imgs/095/qq.png}
    \caption{Scatterplots depicting the performance of IFAD against that of IF2. \textbf{Left:} Paired searches from the same starting point. Controlling for initial starting point, tuning $\alpha$ allows IFAD-$0.97$ to strictly improve on IF2, \cite{poyiadjis11}, and \cite{naesseth18}, on almost every iteration. \textbf{Right:} Q-Q plot of ranked IFAD searches against ranked IF2 searches. It is clear that on average, IFAD performs best, and manages to find the MLE while no IF2 search successfully gets within 7 log-likelihood units of it. We use a dotted red line to display the MLE in both. }
    \label{fig:scatter}
\end{figure}

%\kevin{Include plots of IFAD vs Poyiadjis and Naesseth only in the supplemental information}

\ed{Labeling A and B would be more conventional than Left and Right, and also easier to refer to in the text.}
  
\begin{figure}[ht]
    \includegraphics[width=\arxiv{8cm}{\textwidth/\real{4.2}}]{../imgs/095/boxplot.png}
    \includegraphics[width=\arxiv{8cm}{\textwidth/\real{4.2}}]{../imgs/095/boxplot_all.png}
    \caption{\textbf{Left:} Raincloud plot depicting the performance of IFAD and IF2 where we plot the results of the best run out of every ten runs, simulating the result of the common procedure of running a few searches and choosing the best one. \textbf{Right:} Raincloud plot of all searches. IFAD outperforms all other methods, and the gradient steps improve on the warm-start given by running 40 IF2 iterations.
    We use a dotted red line to display the MLE.}
    \label{fig:boxplot}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\arxiv{10cm}{\textwidth/3}]{../imgs/095/optim.png}
    \caption{Optimization progress of IFAD and IF2. The dashed orange line depicts the median warm-start given by running 40 IF2 iterations. While running 60 more iterations of IF2 improves upon the median warm-start, doing so ultimately underperforms IFAD. We see that IFAD has better tail control and successfully reaches the MLE. 
    We use a dotted red line to display the MLE.}
    \label{fig:optim}
\end{figure}

\ed{IS THE IFAD(0.95) MEDIAN LINE IN FIG 4 (RIGHT) HIDING UNDER IFAD(1)? IF NOT, WEHRE IS IT?}

\section{Theoretical Analysis of MOP-$\alpha$}
\label{sec:thms}

Here, we will show that MOP-$\alpha$ targets the posterior and likelihood and yields the estimators of \cite{poyiadjis11, scibior21, naesseth18} as special cases when differentiated, and characterize rates for its bias and variance under different choices of $\alpha$. To do so, we require the following assumptions:

\begin{enumerate}[label=(A\arabic*),itemsep=-1.2ex] 
    \item \textbf{Continuity of the Likelihood.} $\ell(\theta)$ has more than two continuous derivatives in a neighborhood $\left\{\theta: \ell(\theta)>\lambda_1\right\}$ for some $\lambda_1<\sup _{\varphi} \ell(\varphi)$. \label{assump:conti-lik}
    \item \textbf{Bounded Process Model.} There exist $\underbar{M}, \bar{M}$ such that $0 < \underbar{M} \leq f_{X_n|X_{n-1}}(x_n | x_{n-1};\theta) \leq \bar{M} < \infty$. \label{assump:bounded-process}
    \item \textbf{Bounded Measurement Model.} There exist $\underbar{G}, \bar{G}$ such that $0<\underbar{G} \leq f_{Y_n \mid X_n}\left(y_n^* \mid x_n; \theta\right) \leq \bar{G}<\infty$ and there exists $G'(\theta)$ with $||\nabla_\theta \log f_{Y_n \mid X_n}\left(y_n^* \mid x_n; \theta\right)||_\infty \leq G'(\theta)< \infty$. \label{assump:bounded-measurement}
    \item \textbf{Bounded Gradient Estimates.} There are functions $G(\theta), H(\theta): \Theta \to [0,\infty)$ uniformly bounded by $G^*, H^*<\infty$, so the MOP-$\alpha$ gradient and Hessian estimates at $\theta=\phi$ are almost surely bounded by $G(\theta)$ and $H(\theta)$ for all $\alpha$. \label{assump:local-bounded-derivative}
    \item \textbf{Differentiability of Density Ratios and Simulator.} The measurement density, \arxiv{\\}{}$f_{Y_n|X_n}(y_n^*|x_n; \theta)$, and simulator have more than two continuous derivatives in $\theta$. \label{assump:diff-meas-and-sim}
\end{enumerate}

The fact that the likelihood estimate yielded by MOP-$\alpha$ has more than two continuous derivatives in $\theta$ follows from the construction of the likelihood estimate in equations \ref{eq:mop-conditional-likelihood} and \ref{eq:weighting-scheme}, as well as Assumptions \ref{assump:bounded-measurement} and \ref{assump:diff-meas-and-sim}. 

\subsection{MOP-$\alpha$ Targets the Posterior}


We show here that MOP-$\alpha$ targets the posterior and is strongly consistent for the likelihood when $\alpha=1$ or $\theta=\phi$.
Employing $\alpha<1$ lead to inconsistency when $\theta$ deviates from $\phi$, but this bias disappears as $\theta$ approaches $\phi$.

While the result is presented here as specific to MOP-$\alpha$, we actually prove a more general result in the supplementary material.
That is, we show a strong law of large numbers for triangular arrays of particles with ``off-parameter'' resampling, where we resample the particles according to an arbitrary resampling rule not necessarily in proportion to the targeted distribution of interest and employ weights that encode the cumulative discrepancy between the resampling and the target distribution instead of equal weights.

%\kevin{Should we just show the general result? That is, for an arbitrary set of indices $k_{n,j}$ generated in proportion to some arbitrary weights $\pi_{n,j}$ that do not necessarily sum to $1$ as the estimate is self-normalized, reweighting the particles by $w_{n,a(j)}g_{n,a(j)}/\pi_{n,a(j)}$ targets the posterior. $\pi_{n,j}$ can depend on $\{(x_{n,1:J},g_{n,1:J})\}$ as long as the resampling is carried out independently of $\{(x_{n,1:J},g_{n,1:J})\}$, conditional on $\pi_{n,1:J}$}

\begin{thm}[MOP-$\alpha$ Targets the Posterior and Likelihood]
    \label{thm:mop-targeting}
    When $\alpha=1$ or $\theta=\phi$, MOP-$\alpha$ targets the posterior and is strongly consistent for the likelihood. That is, for $\pi_n(\theta)=f_{X_{1:n}|Y_{1:n} ; \theta}$ and any measurable and bounded functional $h$ and for $\hat\lik(\theta) = \prod_{n=1}^N L_n^{A,\theta,\alpha}$ or $\prod_{n=1}^N L_n^{B,\theta,\alpha}$, it holds that
    \vspace*{-2.5mm}
    \begin{equation}
        \frac{\sum_{j=1}^J h(x_{n,j}^{A,F, \theta}) w_{n,j}^{A,F,\theta}}{\sum_{j=1}^J w_{n,j}^{A,F,\theta}} \stackrel{a.s.}{\to} E_{\pi_n(\theta)} h, \;\; \hat\lik(\theta)  \stackrel{a.s.}{\to} \lik(\theta).
    \vspace*{-2.5mm}
    \end{equation}
\end{thm}

%\kevin{Write the supplement formally with Del Moral-lite language, going into full detail with the proofs and calculations.}

\begin{proof}
    We provide a proof sketch here, deferring most of the details, and discussion of the after-resampling conditional likelihood estimate $L_n^{A,\theta,\alpha}$, to the supplementary information. 
    When $\theta=\phi$, regardless of the value of $\alpha$, the ratio ${g_{n,j}^\theta}/{g_{n,j}^\phi}=1,$ and this reduces to the vanilla particle filter.
    When $\alpha=1$ and $\theta\neq\phi,$ suppose inductively that $\{(X^{F,\theta}_{n-1,j},w^{F,\theta}_{n-1,j})\}_{j=1}^J$ targets $f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}|y^*_{1:n-1};\theta)$.
    It can then be shown that $\{(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j})\}_{j=1}^J$ targets $f_{X_{n}|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)$, that $\{(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j} g^\theta_{n,j} )\}_{j=1}^J$ targets  $f_{X_{n}|Y_{1:n}}(x_{n}|y^*_{1:n};\theta)$, and that weighting the particles by $(X^{F,\theta}_{n,j},w^{F,\theta}_{n,j}) = (X^{P,\theta}_{n,a(j)}, w^{P,\theta}_{n,a(j)} g^\theta_{n,a(j)}/ g^\phi_{n,a(j)})$ where $a(\cdot)$ is the ancestor function,
    resampling with probabilities proportional to $g^\phi_{n,j}$, also targets $f_{X_{n}|Y_{1:n}}(x_{n}|y^*_{1:n};\theta)$.
    If the likelihood is estimated with the before-resampling conditional likelihoods $\hat\lik(\theta) = \prod_{n=1}^N L_n^{B,\theta,\alpha}$, the strong consistency is a direct consequence of our earlier result that $\{ \big(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j}\big) \}$ targets $f_{X_{n}|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)$. 
\end{proof}




\subsection{MOP-$1$ Is Consistent for the Score}

Despite showing that the MOP-$1$ gradient estimate yields the estimate of \cite{poyiadjis11, scibior21} when applied to the bootstrap filter, \cite{poyiadjis11, scibior21} use
$$\frac{1}{J}\sum_{j=1}^J \nabla_\theta \log f_{X_{0:N}, Y_{1:N}}\left(x_{0:n,j}^{A, F,\theta}, y_{1:N}^* ; \theta\right)$$ and not
$$\frac{1}{J}\sum_{j=1}^J \nabla_\theta \log f_{Y_{1:N}| X_{1:N}}\left(y_{1:N}^* | x_{1:n,j}^{A, F,\theta}; \theta\right).$$
It is therefore not immediately apparent that these two converge to the same thing. As such we directly show the consistency of the MOP-$1$ gradient estimate below. 

\begin{thm}[Consistency of MOP-$1$ Gradient Estimate]
    The gradient estimate of MOP-$\alpha$ when $\alpha=1$, $\theta=\phi$ is strongly consistent for the score: $\nabla_\theta \hat\ell_J^1(\theta) \stackrel{a.s.}{\to} \nabla_\theta \ell(\theta)$ as $J \to \infty$.
    \label{thm:mop-grad-consistency}
\end{thm}
\begin{proof}
    Fix $\omega \in \Omega$, and set $\phi = \theta$, where $\theta$ is the point at which we wish to evaluate the gradient. The sequence $(\nabla_\theta \hat\lik_J^1(\theta)(\omega))_{J \in \mathbb{N}}$ is uniformly bounded over all $J$ by Assumption \ref{assump:local-bounded-derivative}. Again by Assumption \ref{assump:local-bounded-derivative}, the second derivative of $\hat\lik_J^1(\theta)(\omega)|_{\theta=\theta'}$ is also bounded by $H^*$ for almost every $\omega\in \Omega$ and every $\theta'\in \Theta$. So $(\nabla_\theta \hat\lik_J^1(\theta)( \omega))_{J \in \mathbb{N}}$ is uniformly Lipschitz, and therefore uniformly equicontinuous for almost every $\omega \in \Omega$.

    By Arzela-Ascoli, there is a uniformly convergent subsequence. But there is only one subsequential limit, as we can treat the gradient estimate at $\theta$ as a bounded functional of the particles by Assumption \ref{assump:local-bounded-derivative}, allowing us to apply Theorem \ref{thm:mop-targeting} to see that the sequence $(\nabla_\theta \hat\lik_J^1(\theta, \omega))_{J \in \mathbb{N}}$ converges pointwise for $\theta=\phi$ and almost every $\omega \in \Omega$. So the whole sequence must converge uniformly to $\lim_{J \to \infty} \nabla_\theta \hat\lik_J^1(\theta)(\omega).$ 
    
    With uniform convergence for the derivatives established, we can swap the limit and derivative and obtain, in conjunction with the strong consistency $\hat{\lik}_J^1(\theta, \omega) \stackrel{a.s.}{\to} \lik(\theta)$ in Theorem \ref{thm:mop-targeting}, that for almost every $\omega \in \Omega$, 
    $\lim_{J \to \infty} \nabla_\theta \hat\lik_J^1(\theta)(\omega) = \nabla_\theta \lim_{J \to \infty} \hat\lik_J^1(\theta)(\omega) = \nabla_\theta \lik(\theta).$
    The result then follows by the continuous mapping theorem. 
\end{proof}


\subsection{MOP-$\alpha$ Bias and Variance}
\begin{thm}
    \label{thm:mop-biasvar}
    When $\alpha\in(0,1)$, $\theta=\phi$, define $\psi(\alpha)=(\alpha^k  + \alpha^{k+1} - \alpha)(1-\alpha)^{-1}$, there exists some $\epsilon>0$ depending on $\bar{M}, \underbar{M}, \bar{G}, \underbar{G}$ as in \cite{karjalainen23} such that the MSE and variance of MOP-$\alpha$ are:
    \vspace*{-1ex}
    \begin{align*}
        &\E||\nabla_\theta\ell(\theta) - \nabla_\theta \hat\ell^\alpha(\theta)||_2^2 
        \\
        &\lesssim \min_{k \leq N} NpG'(\theta)^2\left(k^2J^{-1}+(1-\epsilon)^{\floor{k/(c\log(J))}}+k+\psi(\alpha)\right),\\[-1ex]
        &\text{Var}(\nabla_\theta \hat\ell^{\alpha}(\theta)) \lesssim \min_{k\leq N} NpG'(\theta)^2\left(\frac{k^2}{(1-\alpha)^2J} + \frac{16\alpha^{2k}}{(1-\alpha)^2}N\right).
        \end{align*}
\end{thm}


We defer the proof to the supplementary material, but provide a brief outline here. The variance bound can be reduced to the approximation error between MOP-$\alpha$ and a variant called MOP-$(\alpha,k)$ where the weights are truncated at lag $k$ with discount factor $\alpha$ to ensure strong mixing, and the covariance of the latter which can be controlled with Davydov's inequality and a standard $L^p$ error bound for the particle filter. The MSE bound considers the error between the score and MOP-$(1,k)$, and the error between MOP-$(1,k)$ and MOP-$\alpha$. The latter is $\tilde{O}(NpG'(\theta)^2(k+\psi(\alpha)))$ by tedious algebra, and the former can be controlled with a result on the the forgetting of the particle filter from \cite{karjalainen23} and the very same $L^p$ error bound mentioned above. 

We also show in the supplementary material that the variance of MOP-$0$ is $\tilde{O}(NpG'(\theta)^2/J)$, and note that \cite{poyiadjis11} establish that the variance of MOP-$1$ is $\tilde{O}(N^4/J)$ (ignoring factors of $p, G'$, etc.). This and Theorem \ref{thm:mop-biasvar} show that MOP-$\alpha$ interpolates between MOP-$0$ and MOP-$1$, as $N \leq Nk^2 \leq N^4$, with a phase transition as soon as $\alpha<1$. 



\section{Computational Efficiency}

MOP-$\alpha$ and IFAD are fast algorithms, both in theory and practice. In line with the cheap gradient principle of \cite{kakade2019provably}, getting a gradient estimate from MOP-$\alpha$ takes no more than 6 times that of the runtime of the particle filter (in practice, the multiplier is 3.75). It therefore shares the same $O(NJ)$ time complexity as the particle filter, unlike the $O(NJ^2)$ complexity of \cite{corenflos21} and Algorithm 2 in \cite{poyiadjis11, scibior21}.

%% As IF2 amounts to one iteration of the particle filter, with $O(NJ)$ time complexity. 
%%In practice,


Our implementation of the particle filter, simulator, and MOP-$\alpha$ in \texttt{JAX} \cite{jax} enabled us to take advantage of just-in-time compilation and GPU acceleration, even with a simulator written in Python. This led to a 16x speedup (379ms vs 6.29s on a Intel i9-13900K CPU and NVIDIA RTX3090 GPU) over the CPU-only implementation of the particle filter (with a simulator written in C++) in the \texttt{pomp} package of \cite{king16}. 


%\kevin{Write about new open source library that brings pomp to python with AD. There are implementations in tfp under the jax substrate, but these are not plug and play -- they require you to supply the transition densities. }




\section{Discussion, Limitations, and Future Work}

We note that it is possible to work without a differentiable simulator, but one would then require access to the transition densities. We are working on an analogue that uses (differentiable) transition densities instead of a differentiable simulator, that may be better suited to large discrete state spaces than either MOP-$\alpha$ or the Baum-Welch algorithm.
Additionally, our gradient estimate can be used for variational inference to approximate the posterior distribution over latent states \cite{naesseth18} and parameters. 

Discounting the weights by some $\alpha \in [0,1]$ is not the only way to interpolate between the estimators of \cite{naesseth18} and \cite{poyiadjis11}. As the proof of Theorem \ref{thm:mop-biasvar} implies, we can also truncate the weights at a fixed lag, corresponding to the MOP-$(1,k)$ estimate mentioned. The analysis is similar, with slightly better rates but a less natural interpretation. 



%% \subsection*{Author Affiliations}

%% Include department, institution, and complete address, with the ZIP/postal code, for each author. Use lower case letters to match authors with institutions, as shown in the example. PNAS strongly encourages authors to supply an \href{https://orcid.org/}{ORCID identifier} for each author. Individual authors must link their ORCID account to their PNAS account at \href{http://www.pnascentral.org/}{www.pnascentral.org}. For proper authentication, authors must provide their ORCID at submission and are not permitted to add ORCIDs on proofs.

% \subsection*{Format}

% Many authors find it useful to organize their manuscripts with the following order of sections: title, author line and affiliations, keywords, abstract, significance statement, introduction, results, discussion, materials and methods, acknowledgments, and references. Other orders and headings are permitted.

%% \subsection*{Manuscript Length}

%% A standard 6-page article is approximately 4,000 words, 50 references, and 4 medium-size graphical elements (i.e., figures and tables). The preferred length of articles remains at 6 pages, but PNAS will allow articles up to a maximum of 12 pages.

%% \subsection*{References}

% %% References should be cited in numerical order as they appear in text; this will be done automatically via bibtex,

% \subsection*{Data Archival}

% PNAS must be able to archive the data essential to a published article. Where such archiving is not possible, deposition of data in public databases, such as GenBank, ArrayExpress, Protein Data Bank, Unidata, and others outlined in the \href{https://www.pnas.org/author-center/editorial-and-journal-policies#materials-and-data-availability}{Information for Authors}, is acceptable.





%% \subsection*{Digital Figures}

%% EPS, high-resolution PDF, and PowerPoint are preferred formats for figures that will be used in the main manuscript. Authors may submit PRC or U3D files for 3D images; these must be accompanied by 2D representations in TIFF, EPS, or high-resolution PDF format. Color images must be in RGB (red, green, blue) mode. Include the font files for any text.

%% Images must be provided at final size, preferably 1 column width (8.7cm). Figures wider than 1 column should be sized to 11.4cm or 17.8cm wide. Numbers, letters, and symbols should be no smaller than 6 points (2mm) and no larger than 12 points (6mm) after reduction and must be consistent.

%% Figures and tables should be labelled and referenced in the standard way using the \verb|\label{}| and \verb|\ref{}| commands.

%% Figure \ref{fig:dacca-fit} shows an example of how to insert a column-wide figure. To insert a figure wider than one column, please use the \verb|\begin{figure*}...\end{figure*}| environment. Figures wider than one column should be sized to 11.4 cm or 17.8 cm wide. Use \verb|\begin{SCfigure*}...\end{SCfigure*}| for a wide figure with side legends.


%\begin{SCfigure*}[\sidecaptionrelwidth][t!]
%\centering
%\includegraphics[width=11.4cm,height=11.4cm]{frog.pdf}
%\caption{This legend would be placed at the side of the figure, rather than below it.}\label{fig:side}
%\end{SCfigure*}


%% \subsection*{Tables}
%% Tables should be included in the main manuscript file and should not be uploaded separately.


%% \subsection*{Single column equations}

%% Authors may use 1- or 2-column equations in their article, according to their preference.

%% To allow an equation to span both columns, use the \verb|\begin{figure*}...\end{figure*}| environment mentioned above for figures.

%% Note that the use of the \verb|widetext| environment for equations is not recommended, and should not be used.

%% \begin{figure*}[bt!]
%% \begin{align*}
%% (x+y)^3&=(x+y)(x+y)^2\\
%%        &=(x+y)(x^2+2xy+y^2) \numberthis \label{eqn:example} \\
%%        &=x^3+3x^2y+3xy^3+x^3.
%% \end{align*}
%% \end{figure*}



%\subsection*{Supporting Information Appendix (SI)}

% Authors should submit SI as a single separate SI Appendix PDF file, combining all text, figures, tables, movie legends, and SI references. SI will be published as provided by the authors; it will not be edited or composed. Additional details can be found in the \href{https://www.pnas.org/authors/submitting-your-manuscript#manuscript-formatting-guidelines}{PNAS Author Center}. The PNAS Overleaf SI template can be found \href{https://www.overleaf.com/latex/templates/pnas-template-for-supplementary-information/wqfsfqwyjtsd}{here}. Refer to the SI Appendix in the manuscript at an appropriate point in the text. Number supporting figures and tables starting with S1, S2, etc.

% Authors who place detailed materials and methods in an SI Appendix must provide sufficient detail in the main text methods to enable a reader to follow the logic of the procedures and results and also must reference the SI methods. If a paper is fundamentally a study of a new method or technique, then the methods must be described completely in the main text.

% \subsubsection*{SI Datasets}

% Supply .xlsx, .csv, .txt, .rtf, or .pdf files. This file type will be published in raw format and will not be edited or composed.

\arxiv{}{
\showmatmethods{} % Display the Materials and Methods section
\acknow{Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.}
\showacknow{} % Display the acknowledgments section
}

%\bibsplit[2]
%Use \bibsplit to split the references from the body of the text. Value "[2]" represents the number of reference in the left column (Note: Please avoid single column figures & tables on this page.)

% \bibliographystyle{pnas-new}
% Bibliography
\bibliography{bib-ifad}

\arxiv{
\appendix

\section{MOP Functional Forms}
\input{appendix/functional}


\section{Optimization Convergence Analysis}
\input{appendix/convergence}

\section{Feynman-Kac Models and Monte Carlo Approximations}
\input{appendix/feynman}

\section{Weighted Particle Filters that Target the Posterior}
\input{appendix/targeting}


\section{Consistency of MOP-$1$}
\input{appendix/consistency}

\section{Bias-Variance Analysis}
\input{appendix/biasvar}
}{
}

\end{document}
