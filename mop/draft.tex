\documentclass{article}
\usepackage[margin=2cm]{geometry}
\usepackage{natbib}

\usepackage{enumitem}
\setlist{nosep} % to reduce space in lists

\setlength{\parskip}{0.5em}
\setlength{\parindent}{0in}

\input{macros}

\newcommand{\off}{\operatorname{off}}
\newcommand{\on}{\operatorname{on}}

\title{Automatic Differentiation to Accelerate Inference for Partially Observed Stochastic Processes}
\author{Kevin Tan, Giles J. Hooker, Edward L. Ionides}
\date{}

\comments=1

\begin{document}
\maketitle
\begin{abstract}
    
Automatic differentiation (AD) has driven recent advances in machine learning, including deep neural networks and Hamiltonian Markov Chain Monte Carlo methods. Partially observed nonlinear stochastic dynamic systems have proved resistant to AD techniques due to (1) the requirement for simulation-based inference that does not require access to the system's transition probabilities and (2) the issue that widely used particle filter algorithms yield a likelihood function that is discontinuous as a function of the model parameters. We propose a new representation that embeds existing methods in a theoretical framework that readily permits extension to a new class of algorithms, providing opportunities for optimizing a bias/variance tradeoff. Further, we develop optimization algorithms suited to the Monte Carlo properties of the derivative estimate, including a hybrid algorithm that requires only a differentiable simulator for maximum likelihood estimation. Promising numerical results indicate that a hybrid algorithm that uses AD to refine a coarse solution from iterated filtering can beat current state-of-the-art methods on a challenging scientific benchmark problem.
\end{abstract}

\section{Introduction}

Maximum-likelihood estimation for inference in partially observed stochastic processes, also known as, continuous-state continuous-time hidden Markov models (HMMs), partially-observed Markov processes (POMPs), or partially-observed stochastic nonlinear dynamical systems, is a challenging problem that faces several challenges: 
\begin{enumerate}
    \item Intractable likelihood functions, bypassed with the approaches of simulated likelihood and likelihood-free inference. 
    \item The desire for simulation-based inference without access to the transition density of the latent Markov process, with various methods created by \cite{welch2009abc, wood2010sl, doucet2010pmcmc, Ionides_infdynsys, Ionides_infpomp}. 
    \item Accurate parameter estimation in the presence of significant Monte-Carlo noise in the likelihood estimate.
\end{enumerate}

Many approaches to inference in these scenarios (such as the EM algorithm, the various Kalman filter variants, and MCMC) either struggle with intractable likelihood functions when the models are complex, or assume access to the probability density of next states given the current state. This is a problem in some critical applications, such as disease modeling, where the models are complex enough such that even obtaining the transition density is an intractable problem. However, the particle filter with the bootstrap proposal, a popular method for solving the filtering problem in partially-observed dynamical systems, provides an unbiased estimate of the likelihood (\cite{delmoral2004feynman}) without requiring evaluation of the transition density of the latent Markov process, enabling an arbitrary model simulator to be plugged into the algorithm.

This is the backbone behind the only simulation-based full-information maximum likelihood method available in the literature thus far, the improved iterated filtering algorithm of \citet{Ionides_infpomp}. \citet{Ionides_infpomp} utilize the bootstrap filter for perform maximum likelihood estimation with an iterated perturbed Bayes map, successfully tackling challenging problems in epidemiology that available Bayesian methodology fails to solve. 

Still, maximum likelihood parameter estimation can be challenging, especially when the Monte Carlo variance of the evaluation is high and the number of parameters is not small. For example, though in practice the algorithm of \citet{Ionides_infpomp} converges quickly to a neighborhood of the MLE, it often struggles to successfully optimize the last few-units of the log-likelihood. \citet{Ionides_mcap} attempt to address the third problem, albeit with a computationally-expensive approach. 

\subsection{Automatic Differentiation for Particle Filters}

Recent advances in automatic differentiation (AD) for particle filters (\cite{blei2018vsmc, jon2018diffpf, corenflos2021ot, scibior2021dpf, doucet2022particlebased}) have drawn attention to AD as an alternative tool for inference in partially observed stochastic processes with the particle filter. However, these existing approaches either are not yet compatible with the desire for simulation-based inference, or are computationally expensive. 

\citet{blei2018vsmc} and \citet{jon2018diffpf} drop high-variance resampling terms from the gradient, leading to bias that does not vanish asymptotically according to \citet{corenflos2021ot}. Though the approach of \citet{corenflos2021ot} is promising, it is computationally expensive. \citet{doucet2022particlebased} provide another promising fixed-lag smoothing-based approach that is similar in spirit to our method (MOP-$\alpha$), but requires either (1) access to the process model transition densities or (2) the special case where the transition density factors into a policy that selects actions (in the reinforcement learning sense) and a deterministic model that maps states and actions to next-timestep states. 

Finally, \citet{scibior2021dpf} demonstrate that the estimators derived in \citet{doucet2011sf} can be attained with AD, albeit with a simple tweak to the particle filter that does not modify its likelihood estimates. Though their method is not fully compatible with simulation-based inference, we show that another simple tweak enables this compatibility and corresponds to a special case of our MOP-$\alpha$ algorithm. 

\subsection{Our Contributions}

Previous research on AD for particle filters has struggled with various issues: 
\begin{enumerate}
    \item Bias, if the discontinuous nature of particle resampling is ignored.
    \item High Monte Carlo variance, if continuity corrections lead to numerical instability.
    \item High computational cost, for algorithms which involve pairwise interactions between particles, marginalization, smoothing or optimal transport.
    \item Reduced applicability for algorithms which lose or fail to take advantage of the simulation-based capabilities of the bootstrap filter.
\end{enumerate}

We develop a new approach to statistical inference via AD for particle filters which addresses these concerns. We develop a new theoretical framework that encompasses existing methods, and addresses the seemingly incompatible paradox of differentiating through a Monte-Carlo algorithm with discontinuous resampling.

This is done through the construction of a smooth extension to the particle filter, as well as a \textbf{Measurement Off-Policy-$\alpha$} (MOP-$\alpha$) family of algorithms that (informally) encompasses the special case of AD of a vanilla PF (recovering the gradient estimator of \citet{blei2018vsmc}) when $\alpha=0$, and the DPF algorithm from \citet{scibior2021dpf} (that recovers the gradient estimator from \citet{doucet2011sf}) when $\alpha=1$. When $\alpha \in (0,1)$, this provides opportunities for optimizing a bias/variance tradeoff. 

We show these off-policy particle filters are properly weighted, recover desirable gradient estimates previously encountered in the literature, and possess other desirable theoretical properties. To cap off the theoretical results, we also derive a linear convergence rate for gradient methods on the particle filter in the presence of strong convexity, providing support for numerical optimization with AD for the particle filter.

In practice, we propose a hybrid algorithm that we call \textbf{Iterated Filtering with Automatic Differentiation (IFAD)} that warm-starts gradient ascent (or a similar iterative first or second-order gradient method) with a coarse solution obtained from a few rounds of iterated filtering. While this requires a differentiable simulator compatible with the reparametrization trick (as previous literature such as \citet{corenflos2021ot} also require), future work will deal with workarounds inspired by our framework that use likelihood ratios of transition densities in place of this. Promising numerical results indicate that IFAD beats IF2 (and by the transitive property, IF1, the Liu-West filter, and particle MCMC) on a challenging scientific benchmark problem.

While the Monte-Carlo Adjusted Profile (MCAP) from \citet{Ionides_mcap} provides a profile likelihood confidence interval that takes into account both Monte Carlo profile error and statistical uncertainty in the likelihood, it is computationally inefficient. \kevin{TODO} We provide a method for incorporating the MCAP methodology (with the poor man's likelihood profile) in our IFAD algorithm that provides a more computationally efficient profile likelihood confidence interval. 
\kevin{Quantile regression for estimating the max/the envelope of the likelihood evaluations from AD, then fit MCAP/MQLE/MSLE?}

Finally, we make the first steps towards developing a Python counterpart to the popular \texttt{pomp} R package by \citet{king2016pomp, king2017pompmanual} that we call \texttt{pompy}. The software we provide includes welcome features for scientific computing such as GPU acceleration, parallel computing for single-program-multiple-data programs, just-in-time compilation, and automatic differentiation enabled by the \texttt{jax} Python package from \citet{jax2018github}. 



\subsection{Significance Statement}
Many scientific models involve highly nonlinear stochastic dynamical systems which can be observed only via noisy and incomplete measurements. Under the Markov assumption on system dynamics, previous work has provided methods of performing inference for these models. In particular, prior to this work, iterated filtering algorithms were the only class of algorithms for maximum likelihood estimation that did not require access to the system's transition probabilities, instead needing only a simulator of the system dynamics. We leverage recent advances in automatic differentiation to propose a hybrid algorithm that requires only a differentiable simulator for maximum likelihood estimation. Our new method outperforms previous approaches on a challenging problem in epidemiology. This is implemented in a precursor to a Python counterpart to the popular \texttt{pomp} package by \citet{king2016pomp, king2017pompmanual} that includes welcome features for scientific computing such as GPU acceleration, parallel computing for single-program-multiple-data programs, just-in-time compilation, and automatic differentiation.

\kevin{TODO: (1) Fix beta smoothness to d(n) at no more than O(n), (2) add story to say that no one spotted that it could have been plug-and-play because of e.g. reasons why they would stumble}

\section{Problem Setup}

Consider an unobserved Markov process $\{X_t, t \geq t_0\}$, and observations $Y_1,...,Y_N$ at timesteps $t_1,..., t_N$. The process is parameterized by an unknown parameter $\theta \in \Theta$, where $X \in \gX, Y \in \gY,$ and finally $\gX, \gY, \Theta \subseteq \R^n$. By a similar decomposition to that in \citet{doucet2009tutorial}, we find that the joint density of $X_{0:N}, Y_{1:N}$ can be factored as
$$f_{X_{0: N}, Y_{1: N}}\left(x_{0: N}, y_{1: N} ; \theta\right)=f_{X_0}\left(x_0 ; \theta\right) \prod_{n=1}^N f_{X_n \mid X_{n-1}}\left(x_n \mid x_{n-1} ; \theta\right) f_{Y_n \mid X_n}\left(y_n \mid x_n ; \theta\right).$$

We call $f_{X_n|X_{n-1}}\left(x_{n} \mid x_{n-1}; \theta\right)$ the process model, writing $\operatorname{process}\left(x_n, \theta\right)$ for the simulator corresponding to the above process model, $f_{Y_n|X_n}\left(y_n \mid x_n, \theta\right)$ the measurement model, and will write $y_n^*$ for the actual values of the observations that were observed.

The above are all defined on a single probability space $(\Omega, \Sigma, \prob),$ where $\omega \in \Omega$ denotes an element of the sample space that can be thought of as a random seed. For notational simplicity, we omit the dependence of the likelihood and other suitable calculations on $X_0.$

\section{A Smooth Extension of the Particle Filter}

\textbf{Intuition:} Our theoretical framework for automatic differentiation for the particle filter hinges on the following observation. Instead of seeking to differentiate $\ell(\theta)$ directly, we instead focus on differentiating through a suitable fixed-seed reweighting of a bootstrap filter. The intuition is as follows. Consider a particle filter likelihood estimate where the system (i.e. \textbf{both the state transitions and resampling}) evolves under a parameter $\phi \in \Theta$ and the seed, or sample space element, is fixed at $\omega \in \Omega.$ Conditional on $\omega$ and $\phi,$ one can reweight the likelihood estimate to instead evaluate the likelihood at another (sufficiently nearby) $\theta \in \Theta.$ 

Differentiating this reweighted likelihood estimate with respect to $\theta$ then amounts to differentiating through the reweighting scheme, which is differentiable if the densities used for the reweighting are differentiable. We call this simulation-and-reweighting scheme an \textbf{off-policy} likelihood estimate, as it is similar in spirit to the off-policy methods found in the reinforcement learning literature.

The above intuition is formalized below.

\subsection{Doubly Off-Policy Particle Filters}


\begin{defn}[Seed-Fixing]
\label{defn:seed-fixing}
Consider a probability space $(\Omega, \Sigma, \mu)$ governing the particles and observations. Noting that the particle filter is unbiased for the likelihood when computing the expectation of the normalizing factor under the posterior $f_{X_{0:N|Y_{1:N}}}\left(x_{0: N} \mid y_{1: N}; \theta\right)$, we can write the likelihood as
$$
\begin{aligned}
\mathcal{L}(\theta) & =\int_{\Omega} \mathcal{L}(\theta, \omega, J) d \mu(\omega) \\
& =\int_{\Omega}\left(\prod_{n=1}^N \frac{1}{J} \sum_{j=1}^J f_{Y_n|X_n}\left(y_n^* \mid x_{n, j}^P(\omega), \theta\right)\right) f_{X_{0:N}, Y_{1:N}}\left(x_{0: N}^P(\omega) \mid y_{1: N}^*, \theta\right) d \mu(\omega) \\
& =\int_{\Omega}\left(\prod_{n=1}^N \frac{1}{J} \sum_{j=1}^J f_{Y_n|X_n}\left(y_n^* \mid x_{n, j}^P(\omega), \theta\right) f_{X_{0:N}, Y_{1:N}}\left(x_{n, j}^P(\omega) \mid y_n^*, \theta\right)\right) d \mu(\omega)
\end{aligned}
$$
where $\mathcal{L}(\theta, \omega, J)$ is the Monte-Carlo estimate of the likelihood given the realization of $\mathrm{J}$ particles corresponding to $\omega \in \Omega$. For simplicity, we resample every timestep. We call $\omega$ a seed, and the deterministic quantity $\mathcal{L}(\theta, \omega, J)$ the seed-fixed estimate.
\end{defn}

Armed with this seed-fixed likelihood estimate, we modify a particle filter likelihood estimate where the system evolves under a parameter $\phi \in \Theta$ and seed $\omega \in \Omega$ to instead evaluate the likelihood at another (sufficiently nearby) $\theta \in \Theta.$ 

\begin{defn}[Doubly Off-Policy Particle Filter Likelihood]
\label{defn:doubly-off-policy}
Let $\lik(\theta, \phi, \omega, J)$ denote a Monte-Carlo estimate of the likelihood evaluated at $\theta \in \Theta$ with $J$ particles, but where the system evolves according to another $\phi \in \Theta$ and conditional on a fixed seed $\omega \in \Omega$. This can be computed with a suitable reweighting of the particles, which we provide the derivation for later. We then have
\begin{equation} \label{eq:loglik2}
\lik(\theta)=\E[\lik(\theta,\phi,\Omega, J)]=\int_\Omega \lik(\theta,\phi,\omega, J)\, d\mu(\omega),
\end{equation}
where we define
\begin{align}
    \lik(\theta,\phi,\omega, J) = \prod_{n=1}^N \frac{1}{J} \sum_{j=1}^J &\left(f_{Y_n|X_n}(y_n^* | x_{n,j}^{P,\phi}(\omega), \phi) f_{X_n|Y_n}(x_{n,j}^{P, \phi}(\omega) | y_n^*, \phi) \right.\\
    &\cdot
    \left.
    \frac{f_{Y_n|X_n}(y^*_n|x_{n,j}^{P,\phi}(\omega),\theta)}{f_{Y_n|X_n}(y^*_n|x_{n,j}^{P,\phi}(\omega),\phi)}
    \cdot \frac{f_{X_n|X_{n-1}}(x_{n,j}^{P,\phi}(\omega)|x_{n-1,j}^{F,\phi}(\omega),\theta)}{f_{X_n|X_{n-1}}(x_{n,j}^{P,\phi}(\omega)|x_{n-1,j}^{F,\phi}(\omega),\phi)}\right).
\end{align}
\end{defn}

We want $\phi\in\Theta$ to be a point in parameter space around which $\lik(\theta,\phi,\omega, J)$ is differentiable with respect to $\theta$ and does not have excessive Monte~Carlo variance. This leads us to our first assumption.

\begin{aspt}[Smooth Neighborhood]
\label{assump:smooth-nbhd}
There exists a neighborhood $\mathcal{N}(\phi)$ around any $\phi \in \Theta$ where for all $\theta \in \mathcal{N}(\phi)$ and almost every $\omega \in \Omega$, the Monte Carlo estimate of the likelihood at $\theta \in \mathcal{N}(\phi)$ with the system evolving according to $\phi$ conditional on $\omega$, $\lik(\theta,\phi,\omega, J)$, is twice differentiable in $\theta$. 
\end{aspt}

\paragraph{Intuition:} We can justify this assumption as follows. For suitably nearby $\theta$ and $\phi$, the resampling indices for $\lik(\theta,\phi,\omega, J)$ and $\lik(\phi,\phi,\omega, J)$ are the same, eliminating any discontinuities from resampling, and that for small enough $\mathcal{N}(\phi)$ the likelihood ratios are bounded. The likelihood only changes by a factor of the likelihood ratios, which are bounded and smooth in $\theta$ if the densities used in the calculation are. 

Lastly, we require the following regularity conditions.

\begin{aspt}[Continuity of the Likelihood] $\ell(\theta)$ proper is continuous in a neighborhood $\left\{\theta: \ell(\theta)>\lambda_1\right\}$ for some $\lambda_1<\sup _{\varphi} \ell(\varphi)$.
\end{aspt}

\begin{aspt}[Bounded Measurement Model] There is an $\epsilon>0$ with $\epsilon^{-1}>f_{Y_n \mid X_n}\left(y_n{ }^* \mid x_n; \theta\right)>\epsilon$ for all $1 \leq n \leq N, x_n \in \gX$ and $\theta \in \Theta.$
\end{aspt}

\begin{aspt}[Locally Bounded Derivative]
Let $\mathcal{M}$ be an open subset of $\Omega$. There exists some function $G(\theta)$ and a constant $G=\sup _{\theta \in \mathcal{N}} G(\theta)<\infty$ such that
    $$
    \|\nabla \ell(\theta, \phi, \omega, J)\|_2<G(\theta) \leq G<\infty
    $$
    for every $\phi \in \mathcal{M}, \theta$ in smooth neighborhood $\mathcal{N}(\phi)$, and almost every $\omega \in \Omega$.
\end{aspt}

\subsubsection{Doubly Off-Policy Particle Filter}

Having the system evolve according to $\phi$ but evaluating the likelihood at $\theta$ requires us to reweight the conditional likelihood at each timestep by likelihood ratios corresponding to the process and measurement models. This corresponds to making two corrections in the particle weights at each timestep, by multiplying by 
\begin{equation}
    s_{n,j}=\frac{g_{n,j}^\theta}{g_{n,j}^{\phi}}=\frac{f_{Y_n|X_n}(y_n^*|x_{n,j}^{P, \phi}; \theta)}{f_{Y_n|X_n}(y_n^*|x_{n,j}^{P,\phi}; \phi)}, \qquad r_{n,j}=\frac{f_{X_n|X_{n-1}}(x_{n,j}^{P, \phi}|x_{n-1,j}^{F, \phi}; \theta)}{f_{X_n|X_{n-1}}(x_{n,j}^{P, \phi}|x_{n-1,j}^{F, \phi}; \phi)},
\end{equation}
as we illustrate in Appendix \ref{app:dop}. We use the notation $x_{n,j}^{P, \phi}$ to emphasize that the particles evolve according to $\phi$. 

We claim that this yields a properly weighted particle filter that recovers the derivative estimate of \citet{doucet2011sf, scibior2021dpf} when $\theta$ is evaluated at $\phi,$ which is is Algorithm \ref{alg:dop} when $\alpha=1$. The proof can be found in Appendix A. 

\subsubsection{Discounting Weights}


Observe that at Algorithm \ref{alg:dop}, at each timestep the corrected filtering importance weights $w_{n,j}^{F,\theta}$ optionally propagate to the next timestep for us to maintain the correction. This is because we introduce a discounting parameter $\alpha$ that affects the weights DOP, so $w_{n,j}^{P,\theta} := (w_{n-1,j}^{F,\theta})^{\alpha}$. Note that no matter what value $\alpha$ takes on, the system evolves according to $\phi$ when we perform the correction to evaluate the likelihood at $\theta$. The filter fully targets the posterior when $\theta=\phi$, or $\alpha=1$, as we show in Proposition \ref{prop:dop-correctness}. The value of $\alpha$ simply informs a bias-variance tradeoff, as we elaborate below. 


\paragraph{Bias-Variance Tradeoff} This discounting serves the purpose of balancing a tradeoff between maintaining memory of each particle's ancestral trajectory (most extreme when $\alpha=1$), or considering only the single-step transition dynamics (when $\alpha=0$). This can be thought of as an \textbf{exponentially-weighted moving average}, where $\alpha$ controls the amount of discounting. We elaborate on two special cases below.

\kevin{This isn't a smoother, because we'd need future information.}

\paragraph{When $\alpha=1,$} the likelihood estimate becomes
\begin{equation}
    \hat{\lik}(\theta) := \prod_{n=1}^N L_n^{A, \theta, \alpha} = \prod_{n=1}^N L_n^\phi \cdot \frac{\sum_{j=1}^J w_{n,j}^{F,\theta}}{\sum_{j=1}^J w_{n,j}^{P,\phi}}= \prod_{n=1}^N L_n^{A, \theta, \alpha} = \prod_{n=1}^N L_n^\phi \cdot \frac{\sum_{j=1}^J w_{n,j}^{F,\theta}}{\sum_{j=1}^J w_{n-1,j}^{F,\phi}} = \left(\frac{1}{J}\sum_{j=1}^J w_{N,j}^{F,\theta}\right) \prod_{n=1}^N L_n^\phi,
\end{equation}
as we have a telescoping product. Its derivative is then
\begin{equation}
    \nabla_\theta \hat{\ell}(\theta) := \frac{1}{J}\sum_{j=1}^J \nabla_\theta w_{N,j}^{F,\theta},
\end{equation}
the average gradient of each surviving particle's cumulative importance weights. 

\paragraph{When $\alpha=0,$} the likelihood estimate becomes
\begin{equation}
    \hat{\lik}(\theta) := \prod_{n=1}^N L_n^{A, \theta, \alpha} = \prod_{n=1}^N L_n^\phi \cdot \frac{\sum_{j=1}^J w_{n,j}^{F,\theta}}{\sum_{j=1}^J w_{n,j}^{P,\phi}} = \prod_{n=1}^N L_n^\phi \cdot \frac{1}{J}\sum_{j=1}^J r_{n,j}s_{n,j},
\end{equation}
and though the telescoping product disappears, this reduces to a series of one-step corrections that leave the filter still properly weighted.

\kevin{Is DOP properly weighted when alpha is not 1 and theta is not phi?}


\subsubsection{Correctness of DOP-$\alpha$}

\begin{prop}[Correctness of DOP-$\alpha$ (Algorithm \ref{alg:dop})]
    \label{prop:dop-correctness}
    Algorithm \ref{alg:dop} is a properly weighted particle filter that is unbiased for the likelihood, when either $\alpha=1$ or $\theta=\phi$. 
    
    In particular, when $\theta$ is evaluated at $\phi$, the likelihood estimate agrees with the bootstrap filter. In this case, its gradient when $\alpha=1$ is the same as the estimator of \citet{doucet2011sf},
    \begin{equation}
        \frac{1}{J}\sum_{j=1}^J \nabla_\theta \log f_{X_{0:N}, Y_{1:N}}(x_{1:N,j}^{A, F,\theta}, y_{1:N}^*),
    \end{equation}

    and when $\alpha=0,$ is is the same as differentiating through the fixed-seed likelihood estimate yielded by a vanilla particle filter.
\end{prop}

The proof for the above follows from Appendix A, as well as the technical lemmas for properly weighted particle filters. 


\subsubsection{Practical Considerations}

It is only ever practical to evaluate $\theta$ at $\phi$. This removes the issue of the likelihood ratios blowing up, and yields an estimate of the likelihood that agrees with the bootstrap filter estimate. Its gradient in this case is either the estimate of \citet{doucet2011sf} when $\alpha=1$, a single-timestep estimator that corresponds to naively taking the derivative of the particle filter when $\alpha=0$ as first proposed in \citet{blei2018vsmc}

\subsection{Measurement Off-Policy Particle Filters}

DOP has the entire system evolve according to $\phi$, and we differentiate through the likelihood ratios, which are smooth conditional on the resampling being the same (which in turn happens when $\theta \approx \phi$). However, the above algorithm requires access to likelihood ratios, and is therefore not compatible with simulation-based inference. 

There is a workaround. Consider the case where you have access to a differentiable simulator compatible with the reparametrization trick. If we have the state transitions evolve according to $\theta,$ but have the resampling according to $\phi,$ we can offload the derivative with respect to the process model to the reparametrization trick. 

We therefore make the following assumption in this section only:
\begin{aspt}[Differentiable Simulator]
    $\text{process}(\cdot | X_{t-1}; \theta)$ is differentiable with respect to $\theta,$ and is compatible with the reparametrization trick. 
\end{aspt}


\kevin{Develop theory for mixing-time swapping? Detect when IF2 reaches a neighborhood of the MLE, by detecting whether it has reached a local sort of stationary distribution when the perturbations are lower bounded, e.g. the autocorrelation between the mean vectors to estimate the mixing time or Gelman-Rubin. In practice, just swap when IF2 converges according to the diagnostics. Theory doesn't care, you just need to in a basin of attraction that is strongly convex.}





\section{Numerical Experiments}

\kevin{IFAD vs IF2, AD vs IF2, IFAD0 vs IFAD1, AD0 vs AD1 in Dacca, local and global plots, convergence line plot of IFAD vs IF2, Dacca data plots, toy model from IF2 paper, AD0 vs AD1 gradient histograms, MSE of AD0 and AD1 gradient estimates as J increases on analytically tractable toy model from IF2 paper or OU2 in pomp}

\kevin{Main plots are the local and global 2-way comparison plots of IFAD1 vs IF2, AD1 vs IF2. Visualize the Q-Q plots in the appendix, and use alternate representations as boxplots (or histograms) with dashed line for the maximum in the main paper.}

\citet{liu2023bridging} have established that the straight-through estimator can be thought of as a first-order approximation of the gradient.
\kevin{Benchmark DOP vs straight-through estimator/reinmax in MOP}



\bibliography{bib-ifad,bib-ref}
\bibliographystyle{apalike}

\appendix
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}

\section{Properly Weighted Particle Filters}


\begin{defn}[Targeting, \cite{liu1998smc}]
    A random variable $X$ drawn from a distribution $g$ is properly weighted by a weighting function $w(X)$ with respect to the distribution $\pi$ if for any integrable function $h$,
    $$
    E_g\{h(X) w(X)\}=E_\pi\{h(X)\}
    $$
    
    A set of particles $(X_j, w_j), j=1,2, \ldots$, targets $\pi$ if
    $$
    \frac{\sum_{j=1}^J h(X_j) w_j}{\sum_{j=1}^J w_j} \stackrel{a.s.}{\to} E_\pi(h(X))
    $$
    for any integrable function $h$ as $J \to \infty$, in the sense of \cite{chopin2004clt}. 
\end{defn}

It is known that the bootstrap and appropriately weighted particle filters target the posterior $f_{X_{1:N}|Y_{1:N}}$ in this sense, as long as they adhere to the procedure in \cite{chopin2004clt}. We now present three helper lemmas. The first follows from standard importance sampling arguments, the second from integrating out the marginal, and the third from Bayes' theorem. 


\begin{lem}[Change of Particle Measure]
    \label{lem:change-measure-proper-weights}
    Suppose that $\{(X_j,u_j),j=1,\dots,J\}$ targets $f_X$. Now, let $\{(Y_j,v_j),j=1,\dots,J\}$ be a sample drawn from $\{(X_j,u_j)\}$ where $(X_j,u_j)$ is represented, on average, $\pi_j J$ times. This could amount to multinomial resampling with probability $\pi_j$, or systematic resampling. Write
    \[
    (Y_j,v_j) = \big(X_{a(j)},u_{a(j)}/\pi_{a(j)}\big),
    \]
    where $a$ is called the ancestor function. Then, $\{(Y_j,v_j),j=1,\dots,J\}$ targets $f_X$.
\end{lem}

\begin{proof}
    Let $h$ be an integrable function. 
    \begin{align*}
        \E\left[\sum_{j=1}^J h(Y_j) v_j\right] 
        &= \sum_{j=1}^J \E\left[h(X_{a(j)}) \frac{u_{a(j)}}{\pi_{a(j)}}\right] \\
        &= \sum_{j=1}^J \E\left[h(X_{j}) u_{j}\right],
    \end{align*}
    and similarly for the denominator. Now, the numerator and denominator of the reweighted quantity have the same expectation as the numerator and denominator of the original quantity, so they must converge to $ E_\pi(h(X))$ almost surely as well. 
\end{proof}


\textbf{Remark:} Note that Lemma \ref{lem:change-measure-proper-weights} permits $\pi_{1:J}$ to depend on $\{(X_j,u_j)\}$ as long as the resampling is carried out independently of $\{(X_j,u_j)\}$, conditional on $\pi_{1:J}$.

\begin{lem}[Particle Marginals]
    \label{lem:marginal-proper-weights}
    Suppose that $\{(X_j,u_j),j=1,\dots,J\}$ targets $f_X$. Also suppose that $Z_j \sim f_{Z|X}(\cdot | X_j)$ where $f_{Z|X}$ is a conditional probability density function corresponding to a joint density $f_{X,Z}$ with marginal densities $f_X$ and $f_Z$. Then, $\{(Z_j,u_j)\}$ targets $f_Z$.
\end{lem}
\begin{proof}
    Let $h$ be an integrable function. 

    \begin{align*}
        \E\left[J^{-1}\sum_{j=1}^J h(Z_j) u_j\right] &= \E\left[\E\left[J^{-1}\sum_{j=1}^J h(x_j) u_j f(Z_j|x_j)\Bigg| X_j=x_j\right]\right] \\
        &= \E\left[J^{-1}\sum_{j=1}^J h(X_j) u_j f(Z_j|X_j)\right] \\
        &\stackrel{a.s.}{\to} C_Z \E_{f_X}[h(X) f_{Z|X}(Z|X)] \\
        &= C_Z \E_{f_Z}[h(Z)],
    \end{align*}

    where $C_Z$ is the normalizing factor, and similarly the denominator converges to $C_Z$. The result then follows from Slutsky's theorem. 
    
\end{proof}

\begin{lem}[Particle Posteriors]
    \label{lem:posterior-proper-weights}
    Suppose that $\{(X_j,u_j),j=1,\dots,J\}$ targets $f_X$. Also suppose that $(X^\prime_j,u^\prime_j) = \big(X_j,u_j\, f_{Z|X}(z^*|X_j)\big)$. Then, $\{(X^\prime_j,u^\prime_j)\}$ targets $f_{X|Z}(\cdot | z^*)$.
\end{lem}

\begin{proof}
    The proof is similar to that of Lemma \ref{lem:marginal-proper-weights}, except with Bayes' theorem. 
\end{proof}


\begin{prop}[MOP-1 Targets the Posterior]
    When $\alpha=1$ or $\phi=\theta$, MOP-$\alpha$ targets the posterior. 
\end{prop}
\begin{proof}
    When $\theta=\phi$, the ratio $\frac{g_{n,j}^\theta}{g_{n,j}^\phi}=1,$ and this reduces to the standard case.

    When $\alpha=1$, and $\theta\neq\phi,$ the proof is as follows. Recursively applying Lemmas \ref{lem:change-measure-proper-weights} \ref{lem:marginal-proper-weights}, and \ref{lem:posterior-proper-weights}, we obtain that 
    %to step~\ref{mop:step1}, Lemma~2 step~ {mop:weight:update} and Lemma~3 to step~\ref{mop:step2} we obtain that
    the MOP-1 filter targets the posterior.
    Specifically, suppose inductively that $\big\{\big(X^{F,\theta}_{n-1,j},w^{F,\theta}_{n-1,j}\big)\big\}$ is properly weighted for $f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}|y^*_{1:n-1};\theta)$.
    Then, Lemma \ref{lem:marginal-proper-weights} tells us that $\big\{\big(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j}\big)\big\}$ targets $f_{X_{n}|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)$.
    Lemma \ref{lem:posterior-proper-weights} tells us that $\big\{\big(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j} g^\theta_{n,j} \big)\big\}$ therefore targets  $f_{X_{n}|Y_{1:n}}(x_{n}|y^*_{1:n};\theta)$.
    Lemma \ref{lem:change-measure-proper-weights} guarantees that the resampling rule, given by 
    \[
    \big(X^{F,\theta}_{n,j},w^{F,\theta}_{n,j}\big) = \big(X^{P,\theta}_{n,a(j)}, w^{P,\theta}_{n,a(j)} g^\theta_{n,a(j)}\big/ g^\phi_{n,a(j)}\big),
    \]
    with resampling weights proportional to $g^\phi_{n,j}$, therefore also targets $f_{X_{n}|Y_{1:n}}(x_{n}|y^*_{1:n};\theta)$.
\end{proof}


This has addressed filtering, but not quite yet the likelihood evaluation. For this we use the following lemma.

\begin{lem}[Likelihood Proper Weighting]
    \label{lem:lik-proper-weight}
  $f_{Y_n|Y_{1:n-1}}(y_n^*|y_{1_n-1}^*;\theta)$ is properly estimated by either the before-resampling estimate,
\begin{equation}\label{L1}
L_n^{B,\theta} =  \frac{\sum_{j=1}^Jg^\theta_{n,j} w^{P,\theta}_{n,j}}{\sum_{j=1}^J  w^{P,\theta}_{n,j}},
\end{equation}
or by the after-resampling estimate,
\begin{equation}\label{L2}
L_n^{A,\theta} = L^\phi_n \frac{\sum_{j=1}^Jw^{F,\theta}_{n,j}}{\sum_{j=1}^J  w^{P,\theta}_{n,j}}.
\end{equation}
where $L^\phi_n$ is as defined in the various algorithms.
\end{lem}

Here, (\ref{L1}) is a direct consequence of our earlier result that $\{ \big(X^{P,\theta}_{n,j},w^{P,\theta}_{n,j}\big) \}$ is properly weighted for $f_{X_{n}|Y_{1:n-1}}(x_{n}|y^*_{1:n-1};\theta)$.
To see  (\ref{L2}),
we write the numerator of (\ref{lem:change-measure-proper-weights}) as
\[
L^\phi_n \sum_{j=1}^J \left[ \frac{g^\theta_{n,j}}{g^\phi_{n,j}} w^{P,\theta}_{n,j}\right] \frac{g^\phi_{n,j}}{L_n^\phi}
= L^\phi_n \sum_{j=1}^J w_{n,j}^{FC,\theta} \frac{g^\phi_{n,j}}{L_n^\phi}
\]
Using Lemma \ref{lem:change-measure-proper-weights}, we resample according to probabilities $\frac{g^\phi_{n,j}}{L_n^\phi}$ to see this is properly estimated by
\[
L^\phi_n \sum_{j=1}^J w^{F,\theta}_{n,j},
\]
from which we obtain (\ref{L2}).

Using Lemma \ref{lem:lik-proper-weight}, we obtain a likelihood estimate,
\[
L^{A,\theta} = \prod_{n=1}^N \left( L^\phi_n \, \frac{\sum_{j=1}^J w^{F,\theta}_{n,j}}{\sum_{j=1}^J w^{P,\theta}_{n,j}}\right).
\]
Since $w^{F,\theta}_{n,j}=w^{P,\theta}_{n+1,j}$, this is a telescoping product. The remaining terms are
$\sum_{j=1}^J w^{P,\theta}_{0,j} = J$ on the denominator and $\sum_{j=1}^J w^{F,\theta}_{N,j}$ on the numerator.
This derives the MOP/DOP estimate.

$L^{B,\theta}$ should generally be preferred, since there is no reason to include the extra variability from resampling when calculating the conditional log likelihood, but it lacks the nice telescoping product.



\section{Doubly Off-Policy Particle Filters}
\label{app:dop}


Evaluating the off-policy likelihood is possible with a weighted particle filter by using resampling weight $p(y^*_n|x_n;\phi)$ and making two corrections in the particle weight, through multiplying $w_{t,j}$ by the two likelihood ratios below,

\begin{equation}
    \label{eq:dmeas-ratio}
    s=p(y^*_n|x_n;\theta)/p(y^*_n|x_n;\phi)
\end{equation}
and
\begin{equation}
    \label{eq:rproc-ratio}
    r=p(x_n|x_{n-1};\theta)/p(x_n|x_{n-1};\phi).
\end{equation}


The following result showcases the correctness of corrections \ref{eq:dmeas-ratio} and \ref{eq:rproc-ratio}.

\begin{prop}
    For $\theta$, $\phi$ that satisfy Assumption \ref{assump:smooth-nbhd}, the following correction in the particle filter weights,
    \begin{equation}
        w_{t,j} := \tilde{w}_{t,j} \cdot p(y^*|x_{t,j}^P,\phi)\cdot r \cdot s
    \end{equation}
    successfully recovers $\lik(\theta)$. Therefore, DOP-$\alpha$ targets the posterior and the likelihood when $\alpha=1$ or $\theta=\phi$. 
\end{prop}

\begin{proof}
    Recall $\lik(\theta) = \E[\lik(\theta,\phi,\Omega, J)]=\int_\Omega \lik(\theta,\phi,\omega, J)\, d\mu(\omega)$ and $\lik(\theta) = \int_\Omega \lik(\theta, \omega, J) d\mu(\omega)$. We want to evaluate
    \begin{align*}
        \lik(\theta,\omega, J)
        &= \E\left[ \prod_{t=1}^T \lik_t(\theta) \middle| \omega\right]
        = \E\left[ \prod_{t=1}^T p(y_t^*|y_{1:t-1}^*,\theta) \middle| \omega\right] \\
        &= \E\left[ \prod_{t=1}^T \frac{1}{J} \sum_{j=1}^J  p(y_t^*|x_{t,j}^P,\theta) p(x_{t,j}^P|y_{1:t-1}^*,\theta) \middle| \omega\right] \\
        &= \E\left[ \prod_{t=1}^T\frac{1}{J}  \sum_{j=1}^J  p(y_t^*|x_{t,j}^P,\theta) p(x_{t,j}^P|x_{t-1}^F,\theta) \middle| \omega\right] 
    \end{align*}
    where $p(x_t^P|y_{1:t-1}^*,\theta)$ is the prediction distribution of particles, and the filtering distribution of particles is proportional to $p(y_t^*|x_{t,j}^P,\theta)$, the likelihood of particle $j$ at time $t$.
    
    Consider first the case with no resampling. Let $\lik^j(\theta)$, $\lik^j(\phi)$ be the likelihood of trajectory $j$ under $\theta$ and $\phi$ respectively. As the prediction and filtering particles are then the same, we use $x_{t,j}$ to refer to both. As such, conditional on $\omega$,
    \begin{align*}
        \lik^j(\theta) 
        &= \prod_{t=1}^T p(y_t^*|x_{t,j}, \theta) p(x_{t,j}|x_{t-1,j}, \theta) \\
        &= \prod_{t=1}^T p(y_t^*|x_{t,j}, \phi) \frac{p(y_t^*|x_{t,j}, \theta)}{p(y_t^*|x_{t,j}, \phi)} p(x_{t,j}|x_{t-1,j}, \theta) \frac{p(x_{t,j}|x_{t-1,j}, \theta) }{p(x_{t,j}|x_{t-1,j}, \phi) } \\
        &= \prod_{t=1}^T p(y_t^*|x_{t,j}, \phi) \cdot s \cdot p(x_{t,j}|x_{t-1,j}, \phi)\cdot r
    \end{align*}
    
    and therefore, if trajectories were proposed with equal probability with particles conditional on $\omega$,
    $$ \lik(\theta, \omega, J) = \frac{1}{J} \sum_{j=1}^J \prod_{t=1}^T p(y_t^*|x_{t,j}, \phi) \cdot s \cdot p(x_{t,j}|x_{t-1,j}, \phi)\cdot r $$
    The correction is therefore sufficient for recovering $\lik(\theta)$, with trajectories collected with the process model at $\phi$ and measurements evaluated with the measurement model at $\phi$. 
    
    Consider now the case with resampling. Without loss of generality we can resample every timestep. Here, $x_{t,j}^F(\phi) \sim p(y^*|x_{t,j}^P, \phi)$ and $x_{t,j}^P(\phi) \sim p(x_t|x_{t-1}^F,\phi)$, and similarly for $\theta$. We use this notation to emphasize that \textbf{both} the measurement model and the previously drawn particles depend on $\phi$ or $\theta$.
    
    \begin{align*}
        \lik_t(\theta) &= \E_{x_{t,j}^P \sim p(x_t|x_{t-1}^F, \theta)} \left[\frac{1}{J}\sum_{j=1}^J p(y_t^*|x_{t,j}^P,\theta) \right] \\
        &= \E_{x_{t,j}^P \sim p(x_t|x_{t-1}^F, \theta)} \left[\frac{1}{J}\sum_{j=1}^J p(y_t^*|x_{t,j}^P,\phi) \frac{p(y_t^*|x_{t,j}^P,\theta)}{p(y_t^*|x_{t,j}^P,\phi)} \right] \\
        &= \E_{x_{t,j}^P \sim p(x_t|x_{t-1}^F, \phi)} \left[\frac{1}{J}\sum_{j=1}^J p(y_t^*|x_{t,j}^P,\phi) \frac{p(y_t^*|x_{t,j}^P,\theta)}{p(y_t^*|x_{t,j}^P,\phi)} \frac{p(x_{t,j}|x_{t-1,j}, \theta) }{p(x_{t,j}|x_{t-1,j}, \phi) }\right] \\
        &= \E_{x_{t,j}^P \sim p(x_t|x_{t-1}^F, \phi)} \left[\frac{1}{J}\sum_{j=1}^J p(y_t^*|x_{t,j}^P,\phi) \cdot s \cdot r \right] \\
        &= \E_{\omega}\E_{x_{t,j}^P \sim \process(x_{t-1}^F, \omega, \phi)} \left[\frac{1}{J}\sum_{j=1}^J p(y_t^*|x_{t,j}^P,\phi) \cdot s \cdot r \middle| \omega \right]
    \end{align*}
    
    and again the correction is sufficient.

    Therefore,

    
    \begin{align}
         \mathcal{L}(\theta) 
         &= \int_\Omega \left(
         \prod_{n=1}^N \frac{1}{J} \sum_{j=1}^J f(y_n^* | x_{n,j}^P(\omega), \theta) p(x_{n,j}^P(\omega) | x_{n-1,j}^P(\omega), \theta) \right) \, d\mu(\omega) \\
         &= \int_\Omega \left(
         \prod_{n=1}^N \frac{1}{J} \sum_{j=1}^J f(y_n^* | x_{n,j}^{P,\theta}(\omega), \theta) p(x_{n,j}^{P,\theta}(\omega) | x_{n,j}^P(\omega), \theta) \right) \, d\mu(\omega) \\
         &= \int_\Omega \left(
         \prod_{n=1}^N \frac{1}{J} \sum_{j=1}^J f(y_n^* | x_{n,j}^{P,\theta}(\omega), \phi) p(x_{n,j}^{P,\theta}(\omega) | y_n^*, \phi) \cdot s  \right) \, d\mu(\omega) \\
         &= \int_\Omega \left(
         \prod_{n=1}^N \frac{1}{J} \sum_{j=1}^J f(y_n^* | x_{n,j}^{P,\phi}(\omega), \phi) p(x_{n,j}^{P,\phi}(\omega) | y_n^*, \phi) \cdot r \cdot s \right) \, d\mu(\omega) \\
         &= \int_\Omega \left(
         \prod_{n=1}^N \frac{1}{J} \sum_{j=1}^J f(y_n^* | x_{n,j}^{P,\phi}(\omega), \phi) p(x_{n,j}^{P, \phi}(\omega) | y_n^*, \phi) \cdot r \cdot s \right) \, d\mu(\omega) \\
         &= \int_\Omega \left(
         \prod_{n=1}^N \frac{1}{J} \sum_{j=1}^J f(y_n^* | x_{n,j}^{P,\phi}(\omega), \phi) \cdot r \cdot s \right)  p(x_{1:N} | y_{1:N}^*, \phi) \,  d\mu(\omega) \\
         &=: \int_\Omega \lik(\theta,\phi,\omega, J)\, d\mu(\omega).
     \end{align}
\end{proof}


\subsection{Off-Policy Weighted Differentiable Particle Filter}

We are finally ready to introduce the off-policy weighted differentiable particle filter. In the event where a seed is fixed, all expressions in Algorithm \ref{alg:dop} can be considered to be conditional on $\omega \in \Omega$.  

\begin{algorithm}[H]
\centering
	\caption{Doubly Off-Policy-$\alpha$}
    \label{alg:dop}
	\begin{algorithmic}[1]
	     \STATE \textbf{Input:} Number of particles $J$, timesteps $N$, measurement model $f_{Y_n|X_n}(y_n^*|x_n, \theta)$, simulator $\process(x_{n+1}|x_n, \theta)$, evaluation parameter $\theta$, behavior parameter $\phi$, seed $\omega$.
		\STATE Initialize filter particles ${X}_{0,j}^{F,\phi}\sim {f}_{{X}_{0}}\left(\cdot\giventh{\phi}\right)$, relative weights $w^{F,\theta}_{0,j}= 1$ for $j$ in $\seq{1}{J}$. Fix $\omega.$
		\FOR{$n=1,...,N$}
            \STATE Prediction weights with discounting: $w_{n,j}^{P,\theta} = \big(w_{n-1,j}^{F,\theta}\big)^\alpha$ for $j\ \mathrm{in}\ \seq{1}{J}$
            \label{dop-alpha:discount}
            \STATE Simulate for prediction:
            ${X}_{n,j}^{P,\phi}\sim {f}_{{X}_{n}|{X}_{n-1}}\big(\cdot|{X}_{n-1,j}^{F};{\phi}\big)$ for $j\ \mathrm{in}\ \seq{1}{J}$ \label{dop-alpha:step1}
            \STATE Adjust weights: $\displaystyle w_{n,j}^{P,\theta} = w_{n,j}^{P,\theta} \times
            \frac{{f}_{{X}_{n}|{X}_{n-1}}\big({X}_{n,j}^{P,\phi}|{X}_{n-1,j}^{F};{\theta}\big)}{{f}_{{X}_{n}|{X}_{n-1}}\big({X}_{n,j}^{P,\phi}|{X}_{n-1,j}^{F};{\phi}\big)}$ for $j$ in $1:J$
            \label{dop-alpha:dproc}
            \STATE Evaluate measurement density:
            $g^{\theta}_{n,j}={f}_{{Y}_{n}|{X}_{n}}(y_{n}^{*}|{X}_{n,j}^{P,\phi}\giventh{\theta})$ for $j$ in $\seq{1}{J}$
            \STATE Before-resampling conditional likelihood: $\displaystyle L_n^{B,\theta,\alpha} = \frac{\sum_{j=1}^Jg^\theta_{n,j} w^{P,\theta}_{n,j}}{\sum_{j=1}^J  w^{P,\theta}_{n,j}}$
            \STATE Conditional likelihood under $\phi$: 
            $L_n^{\phi} = \frac{1}{J}\sum_{m=1}^{J}g^{\phi}_{n,m}$
            \label{dop-alpha:Lphi}
            \STATE Normalize weights:
            $\displaystyle \tilde{g}^{\phi}_{n,j}= \frac{g^{\phi}_{n,j}}{JL_n^{\phi}}$
            for $j\ \mathrm{in}\ \seq{1}{J}$
            \STATE Apply systematic resampling to select indices $k_{1:J}$ with $\prob\big(k_{j}=m\big) =\tilde{g}^{\phi}_{n,m}$ \label{dop-alpha:systematic}
            \STATE Resample particles: ${X}_{n,j}^{F,\phi}={X}_{n,k_{j}}^{P,\phi}$
            \STATE Filter weights corrected for resampling:
            $\displaystyle w^{FC,\theta}_{n,j}= w^{P,\theta}_{n,j} \times \frac{ g^{\theta}_{n,j}}{ g^{\phi}_{n,j}}$ for $j\ \mathrm{in}\ \seq{1}{J}$ \label{dop-alpha:weight:update}
            \STATE Resample filter weights:
            $w_{n,j}^{F,\theta}= {w}_{n,k_{j}}^{FC,\theta}$
            for $j$ in $\seq{1}{J}$ \label{dop-alpha:step2}
            \STATE After-resampling conditional likelihood: $\displaystyle L_n^{A,\theta,\alpha} = L_n^\phi \, \frac{\sum_{j=1}^J w^{F,\theta}_{n,j}}{\sum_{j=1}^J  w^{P,\theta}_{n,j}}$
            \ENDFOR
		\RETURN likelihood estimate $\hat{\lik}(\theta) = \lik(\theta, \phi, \omega, J) := \prod_{n=1}^N L_n^{A,\theta,\alpha}$, filtering distribution $\{(x_{N,j}^P, w^{F,\theta}_{N,j})\}.$
	\end{algorithmic}
\end{algorithm}


Here, we consider a doubly off policy particle filter, meaning that both \code{rprocess} and \code{rmeasure} are computed at $\phi$ and particles are reweighted to correspond to $\theta$.

\section{Gradients}

\begin{prop}
    Consider the case of MOP-$\alpha$ when $\alpha=1$ and $\theta=\phi$. The gradient estimate is then

    \begin{equation}
        \nabla_\theta \hat{\ell}(\theta) := \frac{1}{J}\sum_{j=1}^J \nabla_\theta \log f_{Y_{1:N}|X_{1:N}}\left(y_{1:N}^* | x_{1:n,j}^{A, F,\theta}\right),
    \end{equation}

    yielding the gradient estimators of \cite{poyiadjis11, scibior2021dpf} when applied to the bootstrap filter. 
\end{prop}

\begin{proof}
    
Consider the case of MOP-$\alpha$ when $\alpha=1$ and $\theta=\phi$. The likelihood estimate becomes
\begin{equation*}
    \hat{\lik}(\theta) := \prod_{n=1}^N L_n^{A, \theta, \alpha} = \prod_{n=1}^N L_n^\phi \cdot \frac{\sum_{j=1}^J w_{n,j}^{F,\theta}}{\sum_{j=1}^J w_{n,j}^{P,\phi}}= \prod_{n=1}^N L_n^{A, \theta, \alpha} = \prod_{n=1}^N L_n^\phi \cdot \frac{\sum_{j=1}^J w_{n,j}^{F,\theta}}{\sum_{j=1}^J w_{n-1,j}^{F,\phi}} = \left(\frac{1}{J}\sum_{j=1}^J w_{N,j}^{F,\theta}\right) \prod_{n=1}^N L_n^\phi,
\end{equation*}
as we have a telescoping product. Then, as

$$\nabla_\theta \hat\ell(\theta) = \frac{\nabla_\theta \hat\lik(\theta)}{\hat\lik(\theta)} = \frac{\nabla_\theta\left(\frac{1}{J}\sum_{j=1}^J w_{N,j}^{F,\theta}\right) \prod_{n=1}^N L_n^\phi}{\prod_{n=1}^N L_n^\phi} =  \frac{1}{J}\sum_{j=1}^J \nabla_\theta w_{N,j}^{F,\theta},$$

The derivative of the log-likelihood estimate is then
\begin{equation*}
    \nabla_\theta \hat{\ell}(\theta) := \frac{1}{J}\sum_{j=1}^J \nabla_\theta w_{N,j}^{F,\theta},
\end{equation*}
which we decompose as follows.

Observe that as $\alpha=1$,

$$w_{n,j}^{P,\theta} = w_{n-1,\theta}^{F,\theta}\frac{g_{n,j}^\theta}{g_{n,j}^\phi} = \prod_{i=1}^n \frac{g_{i,k_j}^\theta}{g_{i,k_j}^\phi}.$$

Using the log-derivative identity again,
$$\frac{\nabla_\theta w_{n,j}^{P,\theta}}{w_{n,j}^{P,\theta}} = \nabla_\theta \log w_{n,j}^{P,\theta} = \nabla_\theta \log \left(\prod_{i=1}^n \frac{g_{i,k_j}^\theta}{g_{i,k_j}^\phi}\right) = \nabla_\theta \sum_{i=1}^n \left(\log g_{i,k_j}^\theta - \log g_{i,k_j}^\phi\right) = \sum_{i=1}^n \nabla_\theta \log g_{i,k_j}^\theta.$$

So 
$$  \nabla_\theta \sum_{n=1}^N \log g_{n,k_j}^\theta = \nabla_\theta \log\left(\prod_{n=1}^N g_{n,k_j}^\theta\right) =  \nabla_\theta \log\left(\prod_{n=1}^N f_{Y_n|X_n}\left(y_n^* | x_{n,k_j}^{P,\theta}\right)\right) = \nabla_\theta \log f_{Y_{1:N}|X_{1:N}}\left(y_{1:N}^* | x_{1:n,j}^{A, P,\theta}\right),$$

and 
$$\nabla_\theta w_{N,j}^{P,\theta} = w_{N,j}^{P,\theta} \sum_{i=1}^N \nabla_\theta \log g_{n,k_j}^\theta = w_{N,j}^{P,\theta} \nabla_\theta \log f_{Y_{1:N}|X_{1:N}}\left(y_{1:N}^* | x_{1:n,j}^{A, P,\theta}\right).$$

Substituting, we have that
\begin{equation*}
    \nabla_\theta \hat{\ell}(\theta) := \frac{1}{J}\sum_{j=1}^J \nabla_\theta w_{N,j}^{F,\theta} =\frac{1}{J}\sum_{j=1}^J \nabla_\theta w_{N,k_j}^{P,\theta} = \frac{1}{J}\sum_{j=1}^J w_{N,k_j}^{P,\theta} \nabla_\theta \log f_{Y_{1:N}|X_{1:N}}\left(y_{1:N}^* | x_{1:n,k_j}^{A, P,\theta}\right),
\end{equation*}
and finally, observing that when $\theta=\phi$ we have that $w_{N,j}^{F,\theta}=1$, that 
\begin{equation*}
    \nabla_\theta \hat{\ell}(\theta) := \frac{1}{J}\sum_{j=1}^J \nabla_\theta \log f_{Y_{1:N}|X_{1:N}}\left(y_{1:N}^* | x_{1:n,j}^{A, F,\theta}\right),
\end{equation*}

yielding the gradient estimators of \cite{poyiadjis11, scibior2021dpf} when applied to the bootstrap filter. 
\end{proof}

\end{document}
